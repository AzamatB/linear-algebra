%\documentstyle[11pt]{mybook}
%\input{latexmac}
%
%\setcounter{chapter}{0}
%\setcounter{section}{0}
%\setcounter{subsection}{0}
%%\markboth{\hfill\mbox{\sl APPENDIX}}{}  %was: {\mbox{\sl APPENDIX}\hfill}

%begin{document}
%%\newpage
%\pagestyle{myheadings}
%\cleardoublepage

\newcommand{\appendsection}[1]{\subsection*{#1}
   \addcontentsline{toc}{subsection}{#1}}
\markboth{}{}
\renewcommand{\thepage}{A-\arabic{page}}
\setcounter{page}{1}
%\thispagestyle{plain}
%\noindent
\chapter*{Appendix}
\addcontentsline{toc}{chapter}{Appendix}
%\bigskip
%\bigskip

%\noindent
%\appendsection{Introduction}
%\addcontentsline{toc}{subsection}{Introduction}
%\bigskip
%\par\noindent
Mathematics is made of arguments (reasoned discourse that is,
not crockery-throwing).
This section sketches the background material and 
argument techniques that we use in the book.

For more than a sketch, these are classics:
\cite{MathematicsPlausReason}, 
\cite{Quine},
and
\cite{NaiveSetThy}.
\cite{Beck} is
a recent book that is available online.




%\vskip .75in
\appendsection{Propositions}
%\addcontentsline{toc}{subsection}{Propositions}
%\bigskip
%\par\noindent
The point at issue in an argument is the 
\definend{proposition}.\index{proposition}
Mathematicians usually write the point in full before the proof
and label it either
\definend{Theorem}\index{theorem} 
for major points,
\definend{Corollary}\index{corollary}  
for points that follow immediately from
a prior one, or 
\definend{Lemma}\index{lemma} 
for when it is chiefly used to prove other results.

The statements expressing propositions can be complex, with many subparts.
The truth or falsity of the entire proposition depends both on the
truth value of the parts and on how the statement is
put together.

\startword{Not}
Where \( P \) is a proposition,
`it is not the case that \( P \)' is true provided that \( P \) is
false.
Thus, `\( n \) is not prime' is true only when \( n \) is the
product of smaller integers.

So `not' operates on statements, inverting their truth value.
We can picture it with a 
\definend{Venn diagram}.\index{Venn diagram}
\begin{center}
  \includegraphics{appen.1}
\end{center}
Where the box encloses all natural numbers, and inside the circle are
the primes, the shaded area holds numbers satisfying `not \( P \)'.

To prove that a `not \( P \)' statement holds, show that \( P \) is false.



\startword{And}
Consider the statement form `\( P \) and \( Q \)'.
For it to be true both halves must hold:
`\( 7 \) is prime and so is \( 3 \)' is true, while
`\( 7 \) is prime and \( 3 \) is not' is false.

Here is the Venn diagram for `\( P \) and \( Q \)'.
\begin{center}
  \includegraphics{appen.2}
\end{center}
To prove `\( P \) and \( Q \)', prove that each half holds.




\startword{Or}
A `\( P \) or \( Q \)' statement is true when either half holds:
`\( 7 \) is prime or \( 4 \) is prime' is true, while `\( 7 \) is not prime
or \( 4 \) is prime' is false.
We take `or' to mean that if both halves are true
`\( 7 \) is prime or \( 4 \) is not' then the statement as a whole is true.
(This is inclusive or.
Occasionally in everyday speech people use `or' in an exclusive way\Dash ``Live
free or die'' does not intend both halves to hold\Dash but
we will not use `or' in that way.)

The Venn diagram includes all of both circles.
\begin{center}
  \includegraphics{appen.3}
\end{center}
To prove `\( P \) or \( Q \)', show that in all cases at least one
half holds (perhaps sometimes one half and sometimes the other,
but always at least one).


\startword{If-then}
\index{if-then statement}
An `if \( P \) then \( Q \)' statement (sometimes stated as
`\( P \) implies \( Q \)' or `\( P\implies Q\)' or 
`\( P \) is sufficient to give \( Q \)' or `$Q$ if $P$') 
is true unless \( P \) is true while \( Q \) is false.
% Thus `if \( 7 \) is prime then \( 4 \) is not' is true 
% while `if \( 7 \) is prime then \( 4 \) is also prime' is false.
% (Contrary to its
% use in casual speech, in mathematics `if \( P \) then \( Q \)' 
% does not connote that
% \( P \) precedes \( Q \) or causes \( Q \).)

There is a fine point here\Dash `if \( P \) then \( Q \)' is
true when \( P \) is false, no matter what value $Q$ has:
`if \( 4 \) is prime then \( 7 \) is prime' and
`if \( 4 \) is prime then \( 7 \) is not' are both true statements.
(They are \definend{vacuously true}.\index{vacuously true})
Further, `if \( P \) then \( Q \)' is true when $Q$ is true, no matter 
what value $P$ has: `if $4$ is prime then $7$ is prime' and 
`if $4$ is not prime then $7$ is prime' are both true.

We adopt this definition of implication
because we want statements 
such as `if $n$ is a perfect square then $n$ is not prime'
to be true no matter which 
number $n$ appears in that statement.
For instance, we want `if $5$ is a perfect square then $5$ is not prime'
to be true so we want that if both $P$ and~$Q$ are false then
$P\implies Q$ is true.

The diagram
\begin{center}
  \includegraphics{appen.4}
\end{center}
shows that \( Q \) holds whenever \( P \) does.
Notice again that if \( P \) does not hold then \( Q \) may or may not
be in force.

There are two main ways to establish an implication.
The first way is direct:~assume that \( P \) is true and use that
assumption to prove \( Q \).
For instance,
to show `if a number is divisible by 5 then twice that
number is divisible by 10', assume that the number is \( 5n \) and
deduce that \( 2(5n)=10n \).
The second way is indirect:~prove the 
\definend{contrapositive}\index{contrapositive}
statement: `if \( Q \) is false then \( P \) is false'
(rephrased, `\( Q \) can only be false when \( P \) is also false').
Thus to show `if a number is prime then it
is not a perfect square', we can 
argue that if it were a square \( p=n^2 \) then it could be
factored \( p=n\cdot n \) where \( n<p \) and so wouldn't be prime
(\( p=0 \) or \( p=1 \) don't give \( n<p \) but they
are nonprime).

Note two things about this statement form.

First, an `if \( P \) then \( Q \)' result can sometimes be improved
by weakening \( P \) or strengthening \( Q \).
Thus, `if a number is divisible by \( p^2 \) then its square is also
divisible by \( p^2 \)' could be upgraded either by relaxing its
hypothesis:~`if a number is divisible by \( p \) then its square
is divisible by \( p^2 \)', or by tightening its conclusion:~`if
a number is divisible by \( p^2 \) then its square is divisible by
\( p^4 \)'.

Second,
after showing `if \( P \) then \( Q \)' then a good next step is to look into
whether there are cases where \( Q \) holds but \( P \) does not.
The idea is to better understand the relationship between \( P \) and
\( Q \) with an eye toward strengthening the proposition.



\startword{Equivalence}
\index{equivalent statements}\index{propositions!equivalent}
An if-then statement
cannot be improved when not only does \( P \) imply \( Q \) but
also \( Q \) implies \( P \). 
Some ways to say this are:
`\( P \) if and only if
\( Q \)', `\( P \) iff \( Q \)', `\( P \) and \( Q \) are logically
equivalent', `\( P \) is necessary and sufficient to give \( Q \)',
`\( P\iff Q \)'.
An example is `a number is divisible by a prime if and only if that number
squared is divisible by the prime squared'.

The picture shows that \( P \) and \( Q \) hold in exactly the
same cases.
\begin{center}
  \includegraphics{appen.5}
\end{center}
Although in simple arguments a chain like 
``\( P \) if and only if $R$, which holds if and only if $S$ \ldots''
may be practical, typically we show equivalence by showing the two halves
`if \( P \) then \( Q \)' and `if \( Q \) then \( P \)' separately.








\appendsection{Quantifiers}
\index{quantifier}
%\vskip .75in
%\noindent {\Large\bf Quantifiers}
%\bigskip
%\par\noindent
Compare these statements about natural numbers:
`there is an \( x \) such that \( x \) is
divisible by \( x^2 \)' is true, while
`for all numbers \( x \), that \( x \) is divisible by \( x^2 \)' is false.
The `there is' and `for all' 
prefixes are \definend{quantifiers}.\index{quantifiers}

\startword{For all}
The `for all' prefix is the 
\definend{universal quantifier},\index{quantifier!universal} 
symbolized \( \forall \).

In a sense the
box we draw to border the Venn diagram shows the universal quantifier since
it dilineates the universe of possible members.
\begin{center}
  \includegraphics{appen.6}
\end{center}

To prove that a statement holds in all cases, 
show that it holds in each case.
Thus to prove that `every number divisible by \( p \) has its
square divisible by \( p^2 \)', take a single number of the form
\( pn \) and square it \( (pn)^2=p^2n^2 \).
This is a ``typical element'' or ``generic element'' proof.

In this kind of argument we must be careful not to assume 
properties for that element
other than those in the hypothesis.
Here is an example of a common wrong argument:
``if \( n \) is divisible by a prime, say \( 2 \), so that \( n=2k \)
for some natural number~$k$,
then \( n^2=(2k)^2=4k^2 \) and the square of $n$ is divisible
by the square of the prime.''
That is an argument about the case \( p=2 \) but it isn't a proof for
general \( p \).
Contrast it with a correct one:
``if \( n \) is divisible by a prime
so that \( n=pk \) for some natural number~$k$,
then \( n^2=(pk)^2=p^2k^2 \) and so the square of $n$ is divisible
by the square of the prime.''




\startword{There exists}
The `there exists' prefix is the 
\definend{existential quantifier},\index{quantifier!existential}
symbolized 
\( \exists \).

%This quantifier is in some ways the opposite of `for all'.
%For instance, contrast these two definitions of primality
%of an integer \( p \): (i)~for all \( n \), if
%\( n \) is not \( 1 \) and \( n \) is not \( p \) then \( n \)
%does not divide \( p \), and
%(ii)~it is not the case that there exists an \( n \)
%(with \( n\neq 1 \) and \( n\neq p \)) such that \( n \) divides \( p \).

A Venn diagram 
of `there is a number such that \( P \)' shows both that there can be
more than one and also that not all numbers need satisfy \( P \).
\begin{center}
  \includegraphics{appen.7}
\end{center}

We can prove
an existence proposition by producing something satisfying
the property: once, to settle the question of primality of
\( 2^{2^5}+1 \), Euler produced the divisor \( 641 \) \cite{Sandifer}.
But there are proofs
showing that something exists without saying how to find it;
Euclid's argument given in the next subsection
shows there are infinitely many primes without giving a formula naming them.
In general, while demonstrating existence is better than nothing,
giving an example is better, and an
exhaustive list of all instances is ideal.

Finally,
along with ``Are there any?''\spacefactor=1000 % 
we often ask ``How many?''
So the question of uniqueness often arises in conjunction
with questions of existence.
Many times the two arguments are simpler if separated, so note that just as
proving something exists does not show it is unique,
neither does proving something is unique show that it exists.
(Obviously `the natural number halfway between three and four' 
would be unique, but no such number exists.)










\appendsection{Techniques of Proof}
%\vskip .75in
%\noindent
%{\Large\bf  Techniques of Proof}
%\bigskip
\startword{Induction}
\index{induction}
Many proofs are iterative,
``Here's why the statement is true for the number \( 1 \), 
it then follows for \( 2 \) and from there to \( 3 \) \ldots''.
These are proofs by \definend{induction}.\index{mathematical induction}
Such a proof has two steps.
In the \definend{base step}\index{base step!of induction} 
the proposition is established for some first
number, often \( 0 \) or~\( 1 \).
In the \definend{inductive step}\index{inductive step!of induction} 
we show that if the proposition
holds for numbers up to and including some \( k \) 
then holds for the next number $k+1$.

Here is an example proving that \( 1+2+3+\dots+n=n(n+1)/2 \). 
\begin{quote}\small
For the base step we show that the formula holds when \( n=1 \).
That's easy, the sum of the first \( 1 \) number does indeed equal 
\( 1(1+1)/2 \).

For the inductive step, assume that the formula holds
for the numbers \( 1,2,\ldots,k \) with $k\geq 1$.
That is, assume all of these instances of the formula.
\begin{align*}
  1
  &=1(1+1)/2  \\
  \text{and}\quad 1+2
  &=2(2+1)/2  \\
  \text{and}\quad  1+2+3
  &=3(3+1)/2  \\
  &\vdots    \\
  \text{and}\quad 1+\dots+k
  &=k(k+1)/2
\end{align*}
This is the \definend{induction hypothesis}.\index{induction!hypothesis}
With this assumption we will deduce that 
the formula also holds in the \( k+1 \)~next case.
\begin{equation*}
  1+2+\cdots+k+(k+1)
  =
  \frac{k(k+1)}{2}+(k+1)
  =
  \frac{(k+1)(k+2)}{2}
\end{equation*}
(The first equality follows from the induction hypothesis.)
\end{quote}
We've shown in the base case that the proposition holds for \( 1 \).
We've shown in the inductive step 
that if it holds for the case of \( 1 \) then it also holds for \( 2 \);
therefore it does hold for $2$.
We've also shown in the inductive step that 
if the statement holds for the cases of \( 1 \) and \( 2 \) 
then it also holds for the next case \( 3 \).
Continuing in this way, we get that the statement holds
for any natural number greater than or equal to \( 1 \).

Here is another example, proving
proof that every integer greater than \( 1 \) is a product
of primes.
\begin{quote}\small
The base step is easy: \( 2 \) is the product of a single prime.

For the inductive step assume that each of \( 2, 3,\ldots ,k \) is a
product of primes, aiming to show \( k+1 \) is also a product of
primes.
There are two possibilities.
First, if \( k+1 \) is not divisible by a number smaller than itself then it
is a prime and so is the product of primes.
The second possibility is that \( k+1 \) is divisible by a number
smaller than itself, and then its
factors can be written as a product of primes by the inductive hypothesis.
In either case
\( k+1 \) can be rewritten as a product of primes.
\end{quote}

There are two things to note about the `next number' in an induction
argument.
One thing is that while induction works on the integers, it's no good on the
reals since there is no `next' real.
The other thing is that we sometimes use induction to go down, say, from
\( 10 \) to \( 9 \) to \( 8 \), etc., down to \( 0 \).
So `next number' could mean `next lowest number'.
Of course, at the end we have not shown the fact for all natural numbers, only
for those less than or equal to \( 10 \).




\startword{Contradiction}
\index{contradiction}
Another technique of proof is
to show that something is true by showing that it cannot be false.

The classic example of proof by contradiction is Euclid's
argument that there are infinitely many primes.
\begin{quote}\small
Suppose there are only finitely many primes \( p_1,\dots,p_k \).
Consider \( p_1\cdot p_2\dots p_k +1 \).
None of the primes on this supposedly exhaustive list divides that number
evenly since each leaves a remainder of \( 1 \).
But every number is a product of primes so this can't be.
Therefore there cannot be only finitely many primes.
\end{quote}

Every proof by contradiction assumes that the proposition is
false and derives some contradiction to known facts.
Another example is this proof that
\( \sqrt{2} \) is not a rational number.
\begin{quote}\small
Suppose that  \( \sqrt{2}=m/n \).
\begin{equation*}
   2n^2=m^2
\end{equation*}
Factor out any \( 2 \)'s, giving
\( n=2^{k_n}\cdot \hat{n} \)
and
\( m=2^{k_m}\cdot \hat{m} \).
Rewrite.
\begin{equation*}
  2\cdot (2^{k_n}\cdot \hat{n})^2
  =
  (2^{k_m}\cdot \hat{m})^2
\end{equation*}
The Prime Factorization Theorem says that there must be the same number of
factors of \( 2 \) on both sides, but there are an odd number of them
\( 1+2k_n \) on the left and an even number of them \( 2k_m \) on the right.
That's a contradiction, so a rational with a square of
\( 2 \) is impossible.
\end{quote}

Both of these examples aimed to prove something doesn't exist.
A negative proposition often suggests a proof by contradiction.














\appendsection{Sets, Functions, and Relations}
\startword{Sets}
\index{sets}
Mathematicians work with collections, called \definend{sets}.\index{set} 
A set can be given as a listing between curly braces as in
\( \set{ 1,4,9,16 } \), or if that's
unwieldy, by using set-builder notation as in
\( \set{x\suchthat x^5-3x^3+2=0 } \) (read ``the set of all \( x \)
such that \ldots'').
We name sets with capital roman letters, as with the primes
\( P=\set{2,3,5,7,11,\ldots\,} \), except for a few special sets such as the
real numbers \( \Re \)
and the complex numbers \( \C \).
To denote that something is an 
\definend{element\/}\index{element}\index{set!element} 
(or \definend{member}\index{member}\index{set!member}) of a set we
use `\(\in \)',
so that \( 7\in\set{3,5,7} \) while \( 8\not\in\set{3,5,7} \).

Sets satisfy
the Principle of Extensionality, that two sets with the same elements
are equal.
Because of this, 
repeats collapse \( \set{7,7}=\set{7} \) and order doesn't
matter \( \set{2,\pi}=\set{\pi,2} \).

We say that \( A \) is a \definend{subset} of \( B \), written
$A\subseteq B$, if any element of $A$ is an element of $B$.
We use
`\( \subset \)' for the \definend{proper subset}\index{sets!proper subset}\index{proper!subset}\index{sets!subset} %
relationship that \( A \) is a subset of \( B \) but \( A\neq B \).
An example is 
\( \set{2,\pi}\subset\set{2,\pi,7} \).
These symbols may be flipped, for instance
\( \set{ 2,\pi,5}\supset\set{2,5} \).

Because of Extensionality, to prove that two sets are equal \( A=B \),
show that they have the same members.
Usually we show mutual inclusion,\index{mutual inclusion}%
\index{sets!mutual inclusion}
that both \( A\subseteq B \) and \( A\supseteq B \).

When a sets has no members then it is
the \definend{empty set}\index{empty set}\index{set!empty} \( \set{} \),
symbolized \( \emptyset \).
Any set has the empty set for a subset, by the `vacuously true'
property of the definition of implication.


\startword{Set operations}
Venn diagrams are handy here.
For instance, we can picture \( x\in P \) 
\begin{center}
  \includegraphics{appen.8}
\end{center}
and `\( P\subseteq Q \)'.
\begin{center}
  \includegraphics{appen.4}
\end{center}
\noindent
This is a repeat of the diagram for `if \ldots then \ldots' 
because `\( P\subseteq Q \)' means 
`if \( x\in P \) then \( x\in Q \)'.

For every propositional logic operator there is an associated set
operator.
The 
\definend{complement}\index{complement}\index{set!complement} 
of \( P \) is
\( P^{\text{comp}}=\set{x\suchthat \text{not$( x\in P)$}} \)
\begin{center}
  \includegraphics{appen.1}
\end{center}
\noindent
the \definend{union}\index{union}\index{set!union} is
\( P\union Q=\set{x\suchthat \text{$(x\in P)$ or $(x\in Q)$}} \)
\begin{center}
  \includegraphics{appen.3}
\end{center}
and the \definend{intersection}\index{intersection}\index{set!intersection} is
\( P\intersection Q=\set{x\suchthat \text{$(x\in P)$ and $(x\in Q)$}}. \)
\begin{center}
  \includegraphics{appen.2}
\end{center}



\startword{Sequences}
\index{sequence}
In addition to sets,
we also use collections where order matters and where repeats do
not collapse.
These are \definend{sequences},\index{sequence} denoted with angle brackets:
\( \sequence{ 2,3,7}\neq\sequence{2,7,3} \).
A sequence of length \( 2 \) is an 
\definend{ordered pair},\index{ordered pair}\index{pair!ordered}
and is often written with parentheses: \( (\pi,3) \).
We also sometimes say `ordered triple', `ordered \( 4 \)-tuple', etc.
The set of ordered \( n \)-tuples of elements of a set \( A \) is denoted
\( A^n \).
Thus \( \Re^2 \) is the set of pairs of reals.




\startword{Functions}
\index{function}
When we first learn about functions they are
presented as formulas such as \( f(x)=16x^2-100 \).
But progressing to more advanced mathematics reveals more general
functions\Dash trigonometric ones, exponential and
logarithmic ones, and even constructs like absolute value that involve
piecing together parts. 
And some functions take inputs that are not numbers:
the function that returns the $\Re^2$ distance from a point to the origin 
\( \sqrt{x^2+y^2} \)
takes the ordered pair \( (x,y) \) as its argument.
So we see that functions aren't
formulas, instead the key idea is that a function associates with each
input \( x \) a single output \( f(x) \).

Consequently, a \definend{function}\index{function} 
or \definend{map}\index{map} is defined
to be a set of ordered pairs \( (x,f(x)) \)
such that \( x \) suffices to determine \( f(x) \).
Restated, that is:
if \( x_1=x_2 \) then \( f(x_1)=f(x_2) \)
(this is the requirement that a
function must be \definend{well-defined}\index{function!well-defined}%
\index{well-defined}).\footnote{More on this is in the section
on isomorphisms}

Each input \( x \) is one of the function's 
\definend{arguments}.\index{argument}\index{function!argument} 
Each output \( f(x) \) is a \definend{value}\index{value}\index{function!value}
(often where $x$ is the input the output is denoted $y$).
The set of all arguments is \( f \)'s 
\definend{domain}\index{domain}\index{function!domain}
and the set of output values is its 
\definend{range}.\index{range}\index{function!range}
Usually we don't need to know what is and is not in the range and we instead
work with a convenient superset of the range, the
\definend{codomain}.\index{function!codomain}\index{codomain}
The notation for a function \( f \) with domain \( X \) and codomain \( Y \) is
\( \map{f}{X}{Y} \).
\begin{center}
  \includegraphics{appen.9}
\end{center}
We also use the notation \( x\mapsunder{f} 16x^2-100 \), read
`\( x \) maps under \( f \) to \( 16x^2-100 \)' or
`\( 16x^2-100 \) is the \definend{image}\index{image!under a function} 
of \( x \)'.

A map such as \( x\mapsto \sin(1/x) \) is a
combinations of simple maps, here
\( g(y)=\sin(y) \) applied to the image of \( f(x)=1/x \).
The \definend{composition}\index{composition}\index{function!composition} 
of \( \map{g}{Y}{Z} \) with \( \map{f}{X}{Y} \),
is the map sending
\( x\in X \) to \( g(\, f(x)\,)\in Z \).
It is denoted \( \map{\composed{g}{f}}{X}{Z} \).
This definition only makes sense if the range of \( f \) is a
subset of the domain of \( g \).

An 
\definend{identity map}\index{identity!function}\index{function!identity} 
\( \map{\identity}{Y}{Y} \) defined by
\( \identity(y)=y \) has the property that for any \( \map{f}{X}{Y} \),
the composition \( \composed{\identity}{f} \) is equal to \( f \).
So an identity map plays the same role with respect to function composition
that the number \( 0 \) plays in real number addition or that 
\( 1 \) plays in multiplication.

In line with that analogy, we define a
\definend{left inverse}\index{inverse!left} of a map 
\( \map{f}{X}{Y} \) to be a
function \( \map{g}{\text{range}(f)}{X} \) such that \( \composed{g}{f} \)
is the identity map on \( X \).
A \definend{right inverse}\index{inverse!right} of \( f \) is a
\( \map{h}{Y}{X} \) such that \( \composed{f}{h} \) is the identity.

A map that is both a left and right inverse of \( f \)
is called simply an 
\definend{inverse}.\index{inverse}\index{inverse!two-sided}\index{function!inverse}
An inverse, if one exists, is unique because if both \( g_1 \) and
\( g_2 \) are inverses of \( f \) then
\( g_1(x)=\composed{g_1}{ (\composed{f}{g_2}) }\,(x)
         =\composed{ (\composed{g_1}{f}) }{g_2}\,(x)
         =g_2(x) \)
(the middle equality comes from the associativity of function composition),
so we often call it ``the'' inverse, written \( f^{-1} \).
For instance, the inverse of the function \( \map{f}{\Re}{\Re} \)
given by \( f(x)=2x-3 \) is the function \( \map{f^{-1}}{\Re}{\Re} \)
given by \( f^{-1}(x)=(x+3)/2 \).

The superscript `\( f^{-1} \)' notation for function inverse can be 
confusing since it clashes with \( 1/f(x) \).
But it fits into a larger scheme.
Functions that have the same codomain as domain can be iterated,
so that where $\map{f}{X}{X}$, we can consider
the composition of $f$ with itself: \( \composed{f}{f} \), 
and \( \composed{f}{\composed{f}{f}} \), etc.
We 
write $\composed{f}{f}$ as \( f^2 \) and 
$\composed{\composed{f}{f}}{f}$ as \( f^3 \), etc.
Note that the familiar exponent rules for real numbers hold:
\( \composed{f^i}{f^j}=f^{i+j} \) and \( (f^i)^j=f^{i\cdot j} \).
Then where \( f \) is invertible,
writing \( f^{-1} \) for the inverse
and \( f^{-2} \) for the inverse of \( f^2 \), etc., gives that
these familiar exponent rules continue to hold, once we define
\( f^0 \) to be the identity map.

If the codomain \( Y \) equals the range of \( f \) then 
we say that the function is
\definend{onto}.\index{function!onto}\index{onto function}
A function has a right inverse if and only if it is onto 
(this is not hard to check).
If no two arguments share an image, if
\( x_1\neq x_2 \) implies  that \( f(x_1)\neq f(x_2) \), then the function is
\definend{one-to-one}.\index{function!one-to-one}\index{one-to-one function}
A function has a left inverse if and only if it is one-to-one (this is also 
not hard to check).

By the prior paragraph, a map has an inverse if and only if it is both
onto and one-to-one. 
Such a function is a 
\definend{correspondence}.\index{correspondence}\index{function!correspondence}
It associates one and only one element of the domain with each element of the
range.
Because a composition of one-to-one maps is one-to-one, and a composition
of onto maps is onto, a composition of correspondences is a
correspondence.

We sometimes want to shrink the domain of a function.
For instance, we may take the function \( \map{f}{\Re}{\Re} \) given by
\( f(x)=x^2 \) and, in order to have an inverse, limit input arguments to
nonnegative reals \( \map{\hat{f}}{\Re^+}{\Re} \).
Then \( \hat{f} \) is a different function than \( f \); we call it
the \definend{restriction}\index{function!restriction}\index{restriction} of
\( f \) to the smaller domain.








\startword{Relations}
\index{relation}
For some familiar operations we most naturally interpret them as functions:
addition maps \( (5,3) \) to \( 8 \).
But what of `\( < \)' or `\( = \)'?
We can take the approach of rephrasing `\( 3<5 \)' to `\( (3,5) \) is
in the relation \( < \)'.
That is, define a \definend{binary relation} on a set \( A \) to be
a set of ordered pairs of elements of \( A \).
For example, the \( < \) relation is the set
\(  \set{(a,b)\suchthat a<b} \); some elements of that set are
\( (3,5) \), \( (3,7) \), and \( (1,100) \).

Another binary relation on the natural numbers is equality; this relation is
the set
\( \set{\ldots,(-1,-1),(0,0),(1,1),\ldots} \).
Still another example is `closer than \( 10 \)', the set
\( \set{(x,y)\suchthat |x-y|<10 } \).
Some members of that relation are \( (1,10) \), \( (10,1) \),
and \( (42,44) \).
Neither \( (11,1) \) nor \( (1,11) \) is a member.

Those examples illustrate the generality of the definition.
All kinds of relationships (e.g., `both numbers
even' or `first number is the second with the digits reversed')
are covered.




\startword{Equivalence Relations}
\index{relation!equivalence}\index{equivalence relation}
We shall need to express that two objects are alike in some way.
They aren't identical, but they are related
(e.g., two integers that `give the same remainder when divided by \( 2 \)').

A binary relation \( \set{(a,b),\ldots } \)
is an 
\definend{equivalence relation}\index{equivalence!relation}\index{relation!equivalence} 
when it satisfies
\begin{enumerate}
  \item \definend{reflexivity}:\index{reflexivity}\index{relation!reflexive} 
     any object is related to itself;
  \item \definend{symmetry}:\index{symmetry}\index{relation!symmetric} 
     if \( a \) is related to \( b \) then
     \( b \) is related to \( a \);
 \item \definend{transitivity}:\index{transitivity}\index{relation!transitive}
     if \( a \) is related to \( b \) and \( b \) is
     related to \( c \) then \( a \) is related to \( c \).
\end{enumerate}
% (To see that these conditions formalize being the same, read them again,
% replacing `is related to' with `is like'.)
Some examples (on the integers): `\( = \)' is an equivalence relation,
`\( < \)' does not satisfy symmetry,
`same sign' is a equivalence, while `nearer than \( 10 \)' fails transitivity.






\startword{Partitions}
\index{partition|(}
In the `same sign' relation \( \set{ (1,3),(-5,-7),(-1,-1),\ldots} \)
there are two kinds of pairs, the ones with both numbers positive
and those with both negative.
So integers fall into exactly one of two classes, positive or negative.

A \definend{partition}\index{partition} 
of a set \( S \) is a collection of subsets
\( \set{S_1,S_2,\ldots} \) such that
every element of \( S \) is in one and only one \( S_i \)
\( S_1\union S_2\union \ldots{} = S \), and
no two overlap:~if \( i\neq j \) then
\( S_i\intersection S_j=\emptyset \).
Picture \( S \) as decomposed into distinct parts.
\begin{center}
  \includegraphics{appen.10}
\end{center}
Thus, the first paragraph says `same sign' partitions
the integers into the positives and the negatives.
Similarly, the equivalence relation `=' partitions the integers into
one-element sets.

An example is the fractions 
$S=\set{n/d\suchthat \text{$n,d\in\Z$ and $d\neq 0$}}$.
Of course, $2/3$ and $4/6$ are equivalent fractions.
That is,
we define two fractions $n_1/d_1$ and $n_2/d_2$ 
to be equivalent if $n_1d_2=n_2d_1$.
We can check that this is an equivalence relation, that
it satisfies the above three conditions.
With that, $S$ is divided up into parts.
\begin{center}
  \includegraphics{appen.11}
\end{center}

Before we show that equivalence relations always give rise to
partitions, we first illustrate the argument.
Consider the relationship between two integers of `same parity', 
the set \( \set{ (-1,3),(2,4),(0,0),\ldots} \)
(i.e., `give the same remainder when divided by \( 2 \)').
We want to say that the natural numbers split into two pieces,
the evens and the odds, and inside a piece each member has the same
parity as each other.
So for each \( x \) we define the set of numbers associated with
it: \( S_x=\set{y\suchthat (x,y)\in\text{`same\ parity'}} \).
Some examples are
\( S_1=\set{\ldots,-3,-1,1,3,\ldots} \), and
\( S_4=\set{\ldots,-2,0,2,4,\ldots} \), and
\( S_{-1}=\set{\ldots,-3,-1,1,3,\ldots} \).
These are the parts, e.g., \( S_1 \) is the odds.

\medskip
\par\noindent{\bf Theorem.}
\index{equivalence relation}
An equivalence relation induces a partition on the underlying set.

\begin{proof}
Call the set \( S \) and the relation \( R \).
In line with the illustration in the paragraph above, 
for each \( x\in S \) define \( S_x=\set{y\suchthat (x,y)\in R} \).

Observe that, as \( x \) is a member if \( S_x \),
the union of all these sets is \( S \).
So we will be done if we show that distinct parts are disjoint:
if \( S_x\neq S_y \) then \( S_x\intersection S_y=\emptyset \).
We will verify this through the contrapositive, that is, 
we wlll assume that \( S_x\intersection S_y\neq\emptyset \) in order to
deduce that \( S_x=S_y \).

Let \( p \) be an element of the intersection. 
Then by definition of $S_x$ and $S_y$, the two 
\( (x,p) \) and 
\( (y,p) \) are members of $R$, and by symmetry of this relation
\( (p,x) \) and
\( (p,y) \) are also members of \( R \).
To show that \( S_x=S_y \) we will show each is a subset of the other.

Assume that \( q\in S_x \) so that \( (q,x)\in R \).
Use transitivity along with
\( (x,p)\in R \) to conclude that \( (q,p) \) is also an element of \( R \).
But \( (p,y)\in R \) so another use of transitivity gives that
\( (q,y)\in  R \).
Thus \( q\in S_y \).
Therefore \( q\in S_x \) implies \( q\in S_y \), 
and so \( S_x\subseteq S_y \).

The same argument in the other direction gives the other inclusion, and
so the two sets are equal, completing the contrapositive argument. 
\end{proof}

We call each part of a partition an \definend{equivalence class}%
\index{equivalence!class}\index{class!equivalence}
(or informally, `part').

%Partitioning a set into equivalence classes may at first seem abstract,
%but it is the natural way to classify cases.
%Everyone knows even times even is even,
%this just describes a case under the `same parity' relation.

%A last remark about classification.
We somtimes pick a single element of each equivalence class to be the 
\definend{class representative}.%
\index{equivalence!representative}\index{representative}
\begin{center}
  \includegraphics{appen.13}
%   \setlength{\unitlength}{4pt}      % equivalence classes
%   \begin{picture}(58,18)(0,0)
%      \put(-10,10){\shortstack[r]{\small One representative \\ from each class:} }
%
%
%      \put(20,0){\begin{picture}(25,15)(0,0)
%                   \thicklines
%                   \put(0,0){\framebox(25,15)[bl]{}}
%
%                   \thinlines
%                   \put(0,15){\oval(12,11)[br] }
%                   \put(0,0){\oval(15,10)[tr] }
%                   \put(6,8){\oval(12,9)[r] }
%                   \put(11,0){\oval(8,10)[tr] }
%                   \put(11.5,15){\oval(8,10)[br] }
%                   \put(15,7.5){\makebox(0,0)[l]{\ldots} }
%                \end{picture}  }
%
%      \put(22,12){\makebox(0,0){\scriptsize \( \star \)} }
%      \put(23,3){\makebox(0,0){\scriptsize \( \star \)} }
%      \put(28,9){\makebox(0,0){\scriptsize \( \star \)} }
%      \put(32,3){\makebox(0,0){\scriptsize \( \star \)} }
%      \put(31,13){\makebox(0,0){\scriptsize \( \star \)} }
%   \end{picture}
\end{center}
Usually when we pick representatives we have some natural scheme in mind.
In that case we call them the
\definend{canonical} representatives.%
\index{natural representative}\index{canonical representative}%
\index{equivalence!class!canonical representative}%
\index{representative!canonical}
 
An example is the simplest form of a fraction.
We've defined \( 3/5 \) and \( 9/15 \) to be equivalent fractions.
In everyday work we often use the `simplest form' or `reduced form'
fraction as the class representatives.
\begin{center}
  \includegraphics{appen.12}
%   \setlength{\unitlength}{4pt}      % equivalence classes
%   \begin{picture}(58,18)(0,0)
%      \put(-10,10){\shortstack[r]{\small One representative \\ from each class:} }
%
%
%      \put(20,0){\begin{picture}(25,15)(0,0)
%                   \thicklines
%                   \put(0,0){\framebox(25,15)[bl]{}}
%
%                   \thinlines
%                   \put(0,15){\oval(12,11)[br] }
%                   \put(0,0){\oval(15,10)[tr] }
%                   \put(6,8){\oval(12,9)[r] }
%                   \put(11,0){\oval(8,10)[tr] }
%                   \put(11.5,15){\oval(8,10)[br] }
%                   \put(15,7.5){\makebox(0,0)[l]{\ldots} }
%                \end{picture}  }
%
%      \put(22,12){\makebox(0,0){\scriptsize \( \star\frac{1}{2} \)} }
%      \put(23,3){\makebox(0,0){\scriptsize \( \star\frac{2}{3} \)} }
%      \put(28,9){\makebox(0,0){\scriptsize \( \star\frac{3}{5} \)} }
%      \put(32,3){\makebox(0,0){\scriptsize \( \star\frac{2}{1} \)} }
%      \put(31,13){\makebox(0,0){\scriptsize \( \star\frac{-8}{7} \)} }
%   \end{picture}
\end{center}
\index{partition|)}
%\end{document}
