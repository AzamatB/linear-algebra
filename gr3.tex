% Chapter 1, Section 3 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linalg.html
%  2001-Jun-09
\section{Reduced Echelon Form}
After developing the mechanics of Gauss' method, 
we observed that it can be done in more than one way.
One example is that from this matrix 
\begin{equation*}
    \begin{mat}[r]
       2  &2  \\
       4  &3
    \end{mat}
\end{equation*}
we could derive any of these three echelon form matrices.
\begin{equation*}
    \begin{mat}[r]
       2  &2  \\
       0  &-1
    \end{mat}
    \qquad
    \begin{mat}[r]
       1  &1  \\
       0  &-1
    \end{mat}
    \qquad
    \begin{mat}[r]
       2  &0  \\
       0  &-1
    \end{mat}
\end{equation*}
The first results from $-2\rho_1+\rho_2$.
The second comes from following $(1/2)\rho_1$ with $-4\rho_1+\rho_2$.
The third comes
from $-2\rho_1+\rho_2$ followed by $2\rho_2+\rho_1$
(after the first row combination the matrix is already in
echelon form so the second one is extra work 
but it is nonetheless a legal row operation).

The fact that echelon form 
is not unique raises questions.
Will any two echelon form versions of a linear system have the same number of
free variables?
If yes, 
will the two have exactly the same free variables?
In this section we will 
give a way to decide if one linear system 
can be derived from another by row operations.
The answers to both questions, both ``yes,''
will follow from this.








\subsection{Gauss-Jordan Reduction}%
Gaussian elimination coupled with back-substitution
solves linear systems but it is not the only method possible.
Here is an extension of Gauss' method that has some advantages.

\begin{example} \label{exm:GJRedReadOffSol}
To solve
\begin{equation*}
  \begin{linsys}{3}
    x  &+  &y  &-  &2z  &=  &-2  \\
       &   &y  &+  &3z  &=  &7   \\
    x  &   &   &-  &z   &=  &-1  
  \end{linsys}
\end{equation*}
we can start as usual by going to echelon form.
\begin{equation*}
  \grstep{-\rho_1+\rho_3}
    \begin{amat}[r]{3}
       1  &1  &-2 &-2  \\
       0  &1  &3  &7   \\
       0  &-1 &1  &1
    \end{amat}
  \grstep{\rho_2+\rho_3}
    \begin{amat}[r]{3}
       1  &1  &-2 &-2  \\
       0  &1  &3  &7   \\
       0  &0  &4  &8
    \end{amat}
\end{equation*}
We can keep going to a second stage
by making the leading entries into \( 1 \)'s
\begin{equation*}
    \grstep{(1/4)\rho_3}
    \begin{amat}[r]{3}
       1  &1  &-2 &-2  \\
       0  &1  &3  &7   \\
       0  &0  &1  &2
    \end{amat}
\end{equation*}
and then to a third stage that uses the leading entries 
to eliminate all of the other entries in each column 
by combining upwards.
\begin{equation*}
  \grstep[2\rho_3+\rho_1]{-3\rho_3+\rho_2}
    \begin{amat}[r]{3}
       1  &1  &0  &2   \\
       0  &1  &0  &1   \\
       0  &0  &1  &2
    \end{amat}
  \grstep{-\rho_2+\rho_1}
    \begin{amat}[r]{3}
       1  &0  &0  &1   \\
       0  &1  &0  &1   \\
       0  &0  &1  &2
    \end{amat}
\end{equation*}
The answer is \( x=1 \), \( y=1 \), and \( z=2 \).
\end{example}
Using one entry to clear out the rest of a column is
called \definend{pivoting}\index{pivoting} on that entry.

Note that the row combination operations in the first stage move left to right,
from column one to column three, 
while the combination operations in the third stage move right to left.

\begin{example}  \label{exm:GJRedReadOffSolTwo}
The middle stage operations that 
turn the leading entries into \( 1 \)'s
don't interact, so we can combine multiple ones into a single step.
\begin{eqnarray*}
    \begin{amat}[r]{2}
       2   &1   &7   \\
       4   &-2  &6
    \end{amat}
  &\grstep{-2\rho_1+\rho_2}
  &\begin{amat}[r]{2}
       2   &1   &7   \\
       0   &-4  &-8
    \end{amat}                                   \\
  &\grstep[(-1/4)\rho_2]{(1/2)\rho_1}
  &\begin{amat}[r]{2}
       1   &1/2   &7/2   \\
       0   &1     &2
    \end{amat}                                    \\
  &\grstep{-(1/2)\rho_2+\rho_1}
  &\begin{amat}[r]{2}
       1   &0   &5/2   \\
       0   &1   &2
    \end{amat}
\end{eqnarray*}
The answer is $x=5/2$ and $y=2$.
\end{example}

This extension of Gauss' method is 
\definend{Gauss-Jordan reduction}.\index{linear equation!solution of!Gauss-Jordan}\index{Gauss-Jordan}\index{Gauss' method!Gauss-Jordan}
% It goes past echelon form to a more refined, more specialized,
% matrix form.

\begin{definition}\label{def:RedEchForm}
A matrix or linear system is in
\definend{reduced echelon form\/}\index{echelon form!reduced}\index{reduced echelon form}
if, in addition to being in echelon form, each leading entry is a
one and is the only nonzero entry in its column.
\end{definition}

\noindent
The cost of using Gauss-Jordan reduction to solve a system 
is the additional arithmetic.
The benefit is that we can just read off the solution set
from the reduced echelon form.

In any echelon form, reduced or not, we can read off 
when the system has an empty
solution set because there is a contradictory equation.
We can read off 
when the system has a one-element solution set because there is no
contradiction and every
variable is the leading variable in some row.
And, we can read off when the system has an infinite solution set because 
there is no contradiction and at least one variable is free.

If the echelon form is reduced then we can read off not just the size of the  
solution set but also its description.
We have no trouble describing the solution set when it is empty, of course.
\nearbyexample{exm:GJRedReadOffSol} and~\ref{exm:GJRedReadOffSolTwo} 
show how in a single element solution set case the single element is
in the column of constants.
The next example shows how to read the parametrization
of an infinite solution set from reduced echelon form.

\begin{example}
\begin{multline*}
  \begin{amat}[r]{4}
     2  &6  &1  &2  &5  \\
     0  &3  &1  &4  &1  \\
     0  &3  &1  &2  &5
  \end{amat}
  \grstep{-\rho_2+\rho_3}
  \begin{amat}[r]{4}
     2  &6  &1  &2  &5  \\
     0  &3  &1  &4  &1  \\
     0  &0  &0  &-2 &4
  \end{amat}                                        \\
  \grstep[(1/3)\rho_2 \\ -(1/2)\rho_3]{(1/2)\rho_1}
  \;\;\grstep[-\rho_3+\rho_1]{(4/3)\rho_3+\rho_2}
  \;\;\grstep{-3\rho_2+\rho_1}
  \begin{amat}[r]{4}
     1  &0  &-1/2  &0  &-9/2  \\
     0  &1  &1/3   &0  &3  \\
     0  &0  &0     &1  &-2
  \end{amat}
\end{multline*}
As a linear system this is 
\begin{equation*}
  \begin{linsys}{4}
     x_1  &&     &-&1/2x_3  &&   &= &-9/2  \\
          &&x_2  &+&1/3x_3  &&   &= &3  \\
          &&     &&        &{}\hspace{.5em}{}&x_4 &= &-2    
  \end{linsys}
\end{equation*}
so a solution set description is this.
\begin{equation*}  
  S=\set{\colvec{x_1 \\ x_2 \\ x_3 \\ x_4}
                        =\colvec[r]{-9/2 \\ 3 \\ 0 \\ -2}
                         +\colvec[r]{1/2 \\ -1/3 \\ 1 \\ 0}x_3
                        \suchthat x_3\in\Re}
\end{equation*}
\end{example}

Thus echelon form isn't some kind of one best form for systems.
Other forms, such as reduced echelon form,  have advantages and
disadvantages.
Instead of picturing linear systems (and the associated matrices) 
as things we operate on, 
always directed toward the goal of echelon form, we can think of 
them as interrelated when
we can get from one to another by row operations.
The rest of this subsection develops this relationship.

\begin{lemma} \label{le:RowOpsRev}
Elementary row operations are reversible.
\end{lemma}

\begin{proof}
For any matrix \( A \),
the effect of swapping rows is reversed by swapping them back,
multiplying a row by a nonzero \( k \) is undone by multiplying by
$1/k$,
and adding a multiple of row \( i \) to row \( j \) (with $i\neq j$)
is undone by subtracting the same multiple of row \( i \) from row \( j \).
\begin{equation*}
      A
     \grstep{\rho_i\leftrightarrow\rho_j}
     \;\grstep{\rho_j\leftrightarrow\rho_i}
      A
  \qquad
        A
     \grstep{k\rho_i}
     \;\grstep{(1/k)\rho_i}
      A
  \qquad
        A
     \grstep{k\rho_i+\rho_j}
     \;\grstep{-k\rho_i+\rho_j}
      A                          
\end{equation*}
(The $i\neq j$ conditions is needed.
See \nearbyexercise{exer:INotJMakesRowOpsRev}.)
\end{proof}

Again, the point of view that we are developing, buttressed now by this lemma,
is that the term `reduces to' is misleading:~where
\( A\longrightarrow B \), we shouldn't think of \( B \) as
``after'' \( A \) or ``simpler than'' $A$.
Instead we should think of them as interreducible or interrelated.
Below is a picture of the idea.
The matrices from the start of this section and their
reduced echelon form version are shown in a cluster.
They are all interreducible. 
\begin{center}  
  \includegraphics{ch1.28}
\end{center}

We say 
that matrices that reduce to each other are `equivalent with respect
to the relationship of row reducibility'.
The next result verifies this statement using the definition of 
an equivalence.\appendrefs{equivalence relations}

\begin{lemma}
Between matrices, `reduces to' is an equivalence re\-la\-tion.
\end{lemma}

\begin{proof}
We must check the conditions
(i)~reflexivity, that any matrix reduces to itself,
(ii)~symmetry, that if \( A \) reduces to \( B \) then
   \( B \) reduces to \( A \),
and (iii)~transitivity, that if \( A \) reduces to \( B \) and
      \( B \) reduces to \( C \) then \( A \) reduces to
      \( C \).

Reflexivity is easy; any  matrix reduces to itself in zero row operations.

That the relationship is symmetric is \nearbylemma{le:RowOpsRev}\Dash if
\( A \) reduces to \( B \) by some row operations
then also \( B \) reduces to \( A \) by reversing those operations.

For transitivity, suppose that \( A \) reduces to \( B \) and
that \( B \) reduces to \( C \).
Following the reduction steps from $A \rightarrow\cdots\rightarrow B$
with those from  $B \rightarrow\cdots\rightarrow C$ 
gives a reduction from \( A \) to \( C \).
\end{proof}

\begin{definition}
Two matrices that are interreducible by the elementary row operations
are \definend{row equivalent}.\index{matrix!row equivalence}%
\index{row equivalence}\index{equivalence relation!row equivalence}
\end{definition}

The diagram below shows the collection of all matrices as a box.
Inside that box, each matrix lies in some class.
Matrices are in the same class if and only if they are interreducible.
The classes are disjoint\Dash no matrix is in two distinct classes.
The collection of matrices has been partitioned into 
\definend{row equivalence classes}.\appendrefs{partitions and class representatives}\index{partition!row equivalence classes}

\begin{center}
  \includegraphics{ch1.27}
\end{center}
\noindent One of the classes in this partition is the
cluster of matrices from the start of this section that is shown above,
expanded to include all of the nonsingular $\nbyn{2}$ matrices. 

The next subsection proves that the reduced echelon form of a matrix is 
unique.
Rephrased in terms of the row-equivalence relationship, 
we shall prove that every matrix is 
row equivalent to one and only one reduced echelon form matrix.
In terms of the partition what we shall prove is:~every
equivalence class contains one and only one reduced echelon form matrix.
So each reduced echelon form matrix serves as a representative of its 
class.

\begin{exercises}
   \recommended \item 
     Use Gauss-Jordan reduction to solve each system.
     \begin{exparts*}
        \partsitem \(
          \begin{linsys}[t]{2}
               x  &+  &y  &=  &2  \\
               x  &-  &y  &=  &0  
          \end{linsys}   \)
        \partsitem \(
          \begin{linsys}[t]{3}
               x  &   &   &-  &z  &=  &4  \\
              2x  &+  &2y &   &   &=  &1  
          \end{linsys}  \)
        \partsitem  \(
           \begin{linsys}[t]{2}
               3x  &-  &2y  &=  &1  \\
               6x  &+  &y   &=  &1/2 
           \end{linsys}  \)
        \partsitem \(
           \begin{linsys}[t]{3}
              2x  &-  &y  &  &  &= &-1  \\
               x  &+  &3y &- &z &= &5   \\
                  &   &y  &+ &2z&= &5   
           \end{linsys} \)
     \end{exparts*}
     \begin{answer}
       These answers show only the Gauss-Jordan reduction.
       With it, describing the solution set is easy. 
       \begin{exparts}
         \partsitem $\begin{amat}[r]{2}
               1  &1  &2  \\
               1  &-1 &0
             \end{amat}
             \grstep{-\rho_1+\rho_2}
             \begin{amat}[r]{2}
               1  &1  &2  \\
               0  &-2 &-2
             \end{amat}                         
             \grstep{-(1/2)\rho_2}
             \begin{amat}[r]{2}
               1  &1  &2  \\
               0  &1  &1
             \end{amat}                         
             \grstep{-\rho_2+\rho_1}
             \begin{amat}[r]{2}
               1  &0  &1  \\
               0  &1  &1
             \end{amat}$
         \partsitem $
             \begin{amat}[r]{3}
               1  &0  &-1  &4  \\
               2  &2  &0   &1
             \end{amat}
             \grstep{-2\rho_1+\rho_2}
             \begin{amat}[r]{3}
               1  &0  &-1  &4  \\
               0  &2  &2   &-7
             \end{amat}                    
             \grstep{(1/2)\rho_2}
             \begin{amat}[r]{3}
               1  &0  &-1  &4  \\
               0  &1  &1   &-7/2
             \end{amat}$
         \partsitem 
           \begin{equation*}
             \begin{amat}[r]{2}
               3  &-2  &1  \\
               6  &1   &1/2
             \end{amat}
             \grstep{-2\rho_1+\rho_2}
             \begin{amat}[r]{2}
               3  &-2  &1  \\
               0  &5   &-3/2
             \end{amat}                       
             \grstep[(1/5)\rho_2]{(1/3)\rho_1}
             \begin{amat}[r]{2}
               1  &-2/3&1/3 \\
               0  &1   &-3/10
             \end{amat}                       
             \grstep{(2/3)\rho_2+\rho_1}
             \begin{amat}[r]{2}
               1  &0   &2/15 \\
               0  &1   &-3/10
             \end{amat}
          \end{equation*}
        \partsitem A row swap here makes the arithmetic easier.
         \begin{multline*}
          \begin{amat}[r]{3}
            2  &-1  &0  &-1  \\
            1  &3   &-1 &5   \\
            0  &1   &2  &5
          \end{amat}
          \grstep{-(1/2)\rho_1+\rho_2}
          \begin{amat}[r]{3}
            2  &-1  &0  &-1   \\
            0  &7/2 &-1 &11/2 \\
            0  &1   &2  &5
          \end{amat}                     
          \grstep{\rho_2\leftrightarrow\rho_3}
          \begin{amat}[r]{3}
            2  &-1  &0  &-1   \\
            0  &1   &2  &5    \\
            0  &7/2 &-1 &11/2
          \end{amat}                   \\                 
          \begin{aligned}
            &\grstep{-(7/2)\rho_2+\rho_3}
            \begin{amat}[r]{3}
              2  &-1  &0  &-1   \\
              0  &1   &2  &5    \\
              0  &0   &-8 &-12
            \end{amat}                     
            \grstep[-(1/8)\rho_2]{(1/2)\rho_1}
            \begin{amat}[r]{3}
              1  &-1/2&0  &-1/2 \\
              0  &1   &2  &5    \\
              0  &0   &1  &3/2
            \end{amat}                     \\                     
            &\grstep{-2\rho_3+\rho_2}
            \begin{amat}[r]{3}
              1  &-1/2&0  &-1/2 \\
              0  &1   &0  &2    \\
              0  &0   &1  &3/2
            \end{amat}                     
            \grstep{(1/2)\rho_2+\rho_1}
            \begin{amat}[r]{3}
              1  &0   &0  &1/2  \\
              0  &1   &0  &2    \\
              0  &0   &1  &3/2
            \end{amat}
          \end{aligned}
        \end{multline*}
       \end{exparts}  
     \end{answer}
  \recommended \item 
    Find the reduced echelon form of each matrix.
    \begin{exparts*}
      \partsitem \( \begin{mat}[r]
          2  &1  \\
          1  &3
        \end{mat}  \)
      \partsitem \( \begin{mat}[r]
          1  &3  &1  \\
          2  &0  &4  \\
         -1  &-3 &-3
        \end{mat}  \)
      \partsitem \( \begin{mat}[r]
          1  &0  &3  &1  &2  \\
          1  &4  &2  &1  &5  \\
          3  &4  &8  &1  &2
        \end{mat}  \)
      \partsitem \( \begin{mat}[r]
          0  &1  &3  &2  \\
          0  &0  &5  &6  \\
          1  &5  &1  &5
        \end{mat}  \)
    \end{exparts*}
    \begin{answer}
      Use Gauss-Jordan reduction.
      \begin{exparts}
        \partsitem $
            \grstep{-(1/2)\rho_1+\rho_2}
            \begin{mat}[r]
              2  &1  \\
              0  &5/2
            \end{mat}
            \grstep[(2/5)\rho_2]{(1/2)\rho_1}
            \begin{mat}[r]
              1  &1/2\\
              0  &1
            \end{mat}                      
            \grstep{-(1/2)\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  \\
              0  &1
            \end{mat}$
        \partsitem $
            \grstep[\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \begin{mat}[r]
              1  &3  &1  \\
              0  &-6 &2  \\
              0  &0  &-2
            \end{mat}
            \grstep[-(1/2)\rho_3]{-(1/6)\rho_2}
            \begin{mat}[r]
              1  &3  &1     \\
              0  &1  &-1/3  \\
              0  &0  &1
            \end{mat}                  
            \grstep[-\rho_3+\rho_1]{(1/3)\rho_3+\rho_2}
            \begin{mat}[r]
              1  &3  &0     \\
              0  &1  &0     \\
              0  &0  &1
            \end{mat}                  
            \grstep{-3\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  &0     \\
              0  &1  &0     \\
              0  &0  &1
            \end{mat}$
        \partsitem \ \begin{multline*}
            \grstep[-3\rho_1+\rho_3]{-\rho_1+\rho_2}
            \begin{mat}[r]
              1  &0  &3  &1  &2  \\
              0  &4  &-1 &0  &3  \\
              0  &4  &-1 &-2 &-4
            \end{mat}
            \grstep{-\rho_2+\rho_3}
            \begin{mat}[r]
              1  &0  &3  &1  &2  \\
              0  &4  &-1 &0  &3  \\
              0  &0  &0  &-2 &-7
            \end{mat}                           \\
            \grstep[-(1/2)\rho_3]{(1/4)\rho_2}
            \begin{mat}[r]
              1  &0  &3    &1  &2  \\
              0  &1  &-1/4 &0  &3/4  \\
              0  &0  &0    &1  &7/2
            \end{mat}                           
            \grstep{-\rho_3+\rho_1}
            \begin{mat}[r]
              1  &0  &3    &0  &-3/2  \\
              0  &1  &-1/4 &0  &3/4     \\
              0  &0  &0    &1  &7/2
            \end{mat}
          \end{multline*}
        \partsitem \ \begin{multline*}
            \grstep{\rho_1\leftrightarrow\rho_3}
            \begin{mat}[r]
              1  &5  &1  &5  \\
              0  &0  &5  &6  \\
              0  &1  &3  &2
            \end{mat}
            \grstep{\rho_2\leftrightarrow\rho_3}
            \begin{mat}[r]
              1  &5  &1  &5  \\
              0  &1  &3  &2  \\
              0  &0  &5  &6
            \end{mat}                  
            \grstep{(1/5)\rho_3}
            \begin{mat}[r]
              1  &5  &1  &5  \\
              0  &1  &3  &2  \\
              0  &0  &1  &6/5
            \end{mat}                  \\
            \grstep[-\rho_3+\rho_1]{-3\rho_3+\rho_2}
            \begin{mat}[r]
              1  &5  &0  &19/5  \\
              0  &1  &0  &-8/5  \\
              0  &0  &1  &6/5
            \end{mat}                  
            \grstep{-5\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  &0  &59/5  \\
              0  &1  &0  &-8/5  \\
              0  &0  &1  &6/5
            \end{mat}
          \end{multline*}
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Find each solution set by using Gauss-Jordan reduction and
    then reading off the parametrization.
    \begin{exparts*}
      \partsitem \( \begin{linsys}[t]{3}
                  2x  &+  &y  &-  &z  &=  &1  \\
                  4x  &-  &y  &   &   &=  &3  
                  \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                   x  &   &   &-  &z  &   &   &=  &1  \\
                      &   &y  &+  &2z &-  &w  &=  &3  \\
                   x  &+  &2y &+  &3z &-  &w  &=  &7  
                    \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                   x  &-  &y  &+  &z  &   &   &=  &0  \\
                      &   &y  &   &   &+  &w  &=  &0  \\
                  3x  &-  &2y &+  &3z &+  &w  &=  &0  \\
                      &   &-y &   &   &-  &w  &=  &0  
                  \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{5}
                   a  &+  &2b &+  &3c &+  &d  &-  &e  &=  &1  \\
                  3a  &-  &b  &+  &c  &+  &d  &+  &e  &=  &3  
                  \end{linsys}  \)
    \end{exparts*}
    \begin{answer}
      For the ``Gauss'' halves, see the answers to Chapter One's
      section~I.2 question
      \nearbyexercise{exer:SlvMatNot}.
      \begin{exparts}
      \partsitem The ``Jordan'' half goes this way.
        \begin{equation*}
          \grstep[-(1/3)\rho_2]{(1/2)\rho_1}
          \begin{amat}[r]{3}
            1  &1/2 &-1/2 &1/2  \\
            0  &1   &-2/3 &-1/3
          \end{amat}
          \grstep{-(1/2)\rho_2+\rho_1}
          \begin{amat}[r]{3}
            1  &0   &-1/6 &2/3  \\
            0  &1   &-2/3 &-1/3
          \end{amat}
        \end{equation*}
        The solution set is this
        \begin{equation*}
          \set{\colvec[r]{2/3 \\ -1/3 \\ 0}
               +\colvec[r]{1/6 \\ 2/3 \\ 1}z
              \suchthat z\in\Re}
        \end{equation*}
      \partsitem The second half is
        \begin{equation*}
          \grstep{\rho_3+\rho_2}
          \begin{amat}[r]{4}
            1  &0  &-1  &0  &1 \\
            0  &1  &2   &0  &3 \\
            0  &0  &0   &1  &0
          \end{amat}
        \end{equation*}
        so the solution is this.
        \begin{equation*}
          \set{\colvec[r]{1 \\ 3 \\ 0 \\ 0}
               +\colvec[r]{1 \\ -2 \\ 1 \\ 0}z
              \suchthat z\in\Re}
        \end{equation*}
      \partsitem This Jordan half
        \begin{equation*}
          \grstep{\rho_2+\rho_1}
          \begin{amat}[r]{4}
            1  &0  &1   &1  &0 \\
            0  &1  &0   &1  &0 \\
            0  &0  &0   &0  &0 \\
            0  &0  &0   &0  &0
          \end{amat}
        \end{equation*}
        gives 
        \begin{equation*}
          \set{\colvec[r]{0 \\ 0 \\ 0 \\ 0}
               +\colvec[r]{-1 \\ 0 \\ 1 \\ 0}z
               +\colvec[r]{-1 \\ -1 \\ 0 \\ 1}w
              \suchthat z,w\in\Re}
        \end{equation*}
        (of course, the zero vector could be omitted from the description).
      \partsitem The ``Jordan'' half
        \begin{equation*}
          \grstep{-(1/7)\rho_2}
          \begin{amat}[r]{5}
            1  &2  &3   &1   &-1   &1  \\
            0  &1  &8/7 &2/7 &-4/7 &0
          \end{amat}
          \grstep{-2\rho_2+\rho_1}
          \begin{amat}[r]{5}
            1  &0  &5/7 &3/7 &1/7  &1  \\
            0  &1  &8/7 &2/7 &-4/7 &0
          \end{amat}
        \end{equation*}
        ends with this solution set.
        \begin{equation*}
          \set{\colvec[r]{1 \\ 0 \\ 0 \\ 0 \\ 0}
               +\colvec[r]{-5/7 \\ -8/7 \\ 1 \\ 0 \\ 0}c
               +\colvec[r]{-3/7 \\ -2/7 \\ 0 \\ 1 \\ 0}d
               +\colvec[r]{-1/7 \\ 4/7 \\ 0 \\ 0 \\ 1}e
              \suchthat c,d,e\in\Re}
        \end{equation*}
    \end{exparts}
   \end{answer}
  \item 
    Give two distinct echelon form versions of this matrix.
    \begin{equation*}
      \begin{mat}[r]
        2  &1  &1  &3  \\
        6  &4  &1  &2  \\
        1  &5  &1  &5
      \end{mat}
    \end{equation*}
    \begin{answer}
      Routine Gauss' method gives one:
      \begin{equation*}
        \grstep[-(1/2)\rho_1+\rho_3]{-3\rho_1+\rho_2}
        \begin{mat}[r]
          2  &1  &1  &3  \\
          0  &1  &-2 &-7 \\
          0  &9/2&1/2&7/2
        \end{mat}
        \grstep{-(9/2)\rho_2+\rho_3}
        \begin{mat}[r]
          2  &1  &1    &3  \\
          0  &1  &-2   &-7 \\
          0  &0  &19/2 &35
        \end{mat}
      \end{equation*}
      and any cosmetic change, like multiplying the bottom row by \( 2 \),
      \begin{equation*}
        \begin{mat}[r]
          2  &1  &1    &3  \\
          0  &1  &-2   &-7 \\
          0  &0  &19   &70
        \end{mat}
      \end{equation*}
      gives another.  
    \end{answer}
  \recommended \item \label{exer:PossRedEchFrms} 
    List the reduced echelon forms possible for each size.
    \begin{exparts*}
      \partsitem \( \nbyn{2} \)
      \partsitem \( \nbym{2}{3} \)
      \partsitem \( \nbym{3}{2} \)
      \partsitem \( \nbyn{3} \)
    \end{exparts*}
    \begin{answer}
      In the cases listed below, we take $a,b\in\Re$.
      Thus, some canonical forms 
      listed below actually include infinitely many cases.
      In particular, they includes the cases $a=0$ and $b=0$.
      \begin{exparts}
        \partsitem 
          $\begin{mat}[r]
            0  &0  \\
            0  &0
          \end{mat}$,
          $\begin{mat}[r]
            1  &a  \\
            0  &0
          \end{mat}$, 
          $\begin{mat}[r]
            0  &1  \\
            0  &0
          \end{mat}$, 
          $\begin{mat}[r]
            1  &0  \\
            0  &1
          \end{mat}$
        \partsitem
          $\begin{mat}[r]
               0  &0  &0  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &a  &b  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               0  &1  &a  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               0  &0  &1  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &0  &a  \\
               0  &1  &b
             \end{mat}$,
          $\begin{mat}[r]
               1  &a  &0  \\
               0  &0  &1
             \end{mat}$,
          $\begin{mat}[r]
               0  &1  &0  \\
               0  &0  &1
             \end{mat}$
        \partsitem
          $\begin{mat}[r]
               0  &0  \\
               0  &0  \\
               0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &a  \\
               0  &0  \\
               0  &0
             \end{mat}$,
          $\begin{mat}[r]
               0  &1  \\
               0  &0  \\
               0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &0  \\
               0  &1  \\
               0  &0
             \end{mat}$
        \partsitem
          $\begin{mat}[r]
               0  &0  &0  \\
               0  &0  &0  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &a  &b  \\
               0  &0  &0  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               0  &1  &a  \\
               0  &0  &0  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               0  &0  &1  \\
               0  &0  &0  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &0  &a  \\
               0  &1  &b  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &a  &0  \\
               0  &0  &1  \\
               0  &0  &0
             \end{mat}$,
          $\begin{mat}[r]
               1  &0  &0  \\
               0  &1  &0  \\
               0  &0  &1
             \end{mat}$
      \end{exparts}  
    \end{answer}
  \recommended \item  
    What results from applying Gauss-Jordan reduction to a
    nonsingular matrix?
    \begin{answer}
      A nonsingular homogeneous linear system has a unique solution.
      So a nonsingular matrix must reduce to a (square) 
      matrix that is all \( 0 \)'s
      except for \( 1 \)'s down the upper-left to lower-right diagonal, e.g.,
      \begin{equation*}
         \begin{mat}[r]
           1  &0  \\
           0  &1  \\
         \end{mat},
         \quad\text{or}\quad
         \begin{mat}[r]
           1  &0  &0  \\
           0  &1  &0  \\
           0  &0  &1
         \end{mat},
         \quad\text{etc.}
      \end{equation*}  
    \end{answer}
 \item \cite{Cleary}
    Consider the following relationship on the set of $\nbyn{2}$ matrices:  
    we say that $A$ is \textit{sum-what like} $B$ if the sum of all of 
    the entries in $A$ is the same as the sum of all the entries in $B$.  
    For instance, the zero matrix would be sum-what like the matrix 
    whose first row had two sevens, and whose second row had two 
    negative sevens.
    Prove or disprove that this is an equivalence relation on the set 
    of $\nbyn{2}$ matrices.
    \begin{answer}
      It is an equivalence relation.
      To prove that we must check that the relation 
      is reflexive, symmetric, and transitive.

      Assume that all matrices are $\nbyn{2}$.
      For reflexive, we note that a matrix has the same sum of entries as
      itself.
      For symmetric, we assume $A$ has the same sum of entries as~$B$ 
      and obviously then $B$ has the same sum of entries as~$A$.
      Transitivity is no harder\Dash if $A$ has the same sum of entries
      as $B$ and $B$ has the same sum of entries as $C$ then clearly
      $A$ has the same as $C$.
    \end{answer}
 \item \cite{Cleary}
  Consider the set of students in a class.  
  Which of the following relationships are equivalence relations?  
  Explain each answer in at least a sentence.
  \begin{exparts}
    \item  Two students $x$ and $y$ are related 
      if $x$ has taken at least as many 
      math classes as $y$.
    \item Students $x$ and $y$ are related if $x$ and $y$ have names 
      that start with the same letter.
  \end{exparts}
  \begin{answer}
    To be an equivalence, each relation must be reflexive, symmetric, and
    trasitive.
    \begin{exparts}
      \item This relation 
        is not symmetric because if $x$ has taken $4$~classes and $y$
        has taken $3$ then $x$ is related to $y$ but $y$ is not related
        to $x$.
      \item This is reflexive because $x$'s name starts with the same
        letter as does $x$'s.
        It is symmetric because if $x$'s name starts with the same letter 
        as $y$'s then $y$'s starts with the same letter as does~$x$'s.
        And it is transitive because if $x$'s name starts with the same letter
        as does~$y$'s and $y$'s name starts with the same letter as 
        does $z$'s then $x$'s starts with the same letter as does $z$'s.
        So it is an equivalence.
    \end{exparts}
  \end{answer}
 \item \label{exer:INotJMakesRowOpsRev}
   The proof of \nearbylemma{le:RowOpsRev} contains a reference to the 
   $i\neq j$ condition on the row combination operation.
   \begin{exparts}
     \partsitem The definition of row operations has an $i\neq j$ condition on
        the swap operation $\rho_i\leftrightarrow\rho_j$. 
        Show that in 
        $A\grstep{\rho_i\leftrightarrow\rho_j}\;
          \grstep{\rho_i\leftrightarrow\rho_j}A$
        this condition is not needed.
     \partsitem Write down a $\nbyn{2}$ matrix with nonzero entries,
        and show that the $-1\cdot\rho_1+\rho_1$ operation is not
        reversed by $1\cdot\rho_1+\rho_1$.
     \partsitem Expand the proof of that lemma to make explicit exactly where 
        the $i\neq j$ condition on combining is used.
   \end{exparts}
   \begin{answer}
    \begin{exparts}
      \partsitem The $\rho_i\leftrightarrow\rho_i$ operation does not
        change $A$.
      \partsitem For instance,
        \begin{equation*}
          \begin{mat}[r]
            1  &2  \\
            3  &4  
          \end{mat}
          \grstep{-\rho_1+\rho_1}
          \begin{mat}[r]
            0  &0  \\
            3  &4  
          \end{mat}
          \grstep{\rho_1+\rho_1}
          \begin{mat}[r]
            0  &0  \\
            3  &4  
          \end{mat}
        \end{equation*}
        leaves the matrix changed.
      \partsitem If $i\neq j$ then
        \begin{eqnarray*}
          \begin{mat}
            \vdotswithin{a_{i,1}}                     \\
            a_{i,1}  &\cdots  &a_{i,n}  \\
            \vdotswithin{a_{i,1}}                     \\
            a_{j,1}  &\cdots  &a_{j,n}  \\
            \vdotswithin{a_{i,1}}                     
          \end{mat}
          &\grstep{k\rho_i+\rho_j}
          &\begin{mat}
            \vdotswithin{a_{i,1}}                                      \\
            a_{i,1}           &\cdots  &a_{i,n}          \\
            \vdotswithin{a_{i,1}}                                      \\
            ka_{i,1}+a_{j,1}  &\cdots  &ka_{i,n}+a_{j,n}  \\
            \vdotswithin{a_{i,1}}                     
          \end{mat}                                        \\
          &\grstep{-k\rho_i+\rho_j}
          &\begin{mat}
            \vdotswithin{a_{i,1}}                                      \\
            a_{i,1}           &\cdots  &a_{i,n}          \\
            \vdotswithin{a_{i,1}}                                      \\
            -ka_{i,1}+ka_{i,1}+a_{j,1}  &\cdots &-ka_{i,n}+ka_{i,n}+a_{j,n} \\
            \vdotswithin{a_{i,1}}                     
          \end{mat}
        \end{eqnarray*}
        does indeed give $A$ back.
        (Of course, if $i=j$ then the third matrix would have entries of the 
        form $-k(ka_{i,j}+a_{i,j})+ka_{i,j}+a_{i,j}$.)
    \end{exparts}
   \end{answer}
\end{exercises}




















\subsection{The Linear Combination Lemma}
We will close this section and this chapter by proving 
that every matrix is row equivalent to one
and only one reduced echelon form matrix.
The ideas that appear here will reappear, and be further developed, in the
next chapter.

The underlying theme here is that one way to understand a
mathematical situation is by being able to classify the cases that can happen.
We have met this theme several times already.
We have classified solution sets of linear systems into the no-elements, 
one-element, and infinitely-many elements cases.
We have also classified linear systems with the same number of equations 
as unknowns into the nonsingular and singular cases.
We adopted these classifications because they give us a way to understand
the situations that we were investigating.
Here, where we are investigating row equivalence, we know that the set of all
matrices breaks into the row equivalence classes.
When we finish the proof here, we will have a way to understand each of those
classes\Dash its matrices can be thought of as derived by row operations from the
unique reduced echelon form matrix in that class.

Put in more operational terms,
after that proof we will (as promised in this section's opening) have a
way to decide if one matrix can be derived from another by row reduction.
We apply the Gauss-Jordan procedure to both and see whether
or not they come to the same reduced echelon form.

% Here is an informal argument that the reduced 
% echelon form version of a matrix is unique.
% Consider again the example that started this section of a matrix that
% reduces to three different echelon form matrices.
% The first matrix of the three is the natural echelon form version.
% The second matrix is the same as 
% the first except that a row has been halved.
% The third matrix, too, is just a cosmetic variant of the first. 
% The definition of reduced echelon form outlaws this kind of fooling around.
% In reduced echelon form,
% halving a row is not possible because that would
% change the row's leading entry away from one, and
% neither is combining rows possible, because then a leading entry would no
% longer be alone in its column.

% This informal justification is not a proof;
% the argument shows that no two different reduced echelon form matrices
% are related by a single row operation step, but the argument does not
% ruled out the possibility that two different reduced echelon form
% matrices could be related by multiple steps.
% Before we go to the proof, we finish this subsection by 
% rephrasing our work in a terminology that will be enlightening.

To understand how row operations act to transform one matrix into another,
we consider the effect that they have on the parts of a matrix.
The crucial observation is that row operations combine the rows linearly.

% \begin{definition}
% A \definend{linear combination}\index{linear combination} of 
% \( x_1,\ldots,x_m \)
% is an expression of the form $c_1x_1+c_2x_2+\,\cdots\,+c_mx_m$
% where the \( c \)'s are scalars.
%\end{definition}

% \noindent (We have already used the phrase 
% `linear combination' in this book.
% The meaning is unchanged, but the next result's statement makes
% a more formal definition in order.)

\begin{lemma}[Linear Combination Lemma] \index{Linear Combination Lemma}
A linear combination of linear combinations is a linear combination.
\end{lemma}

\begin{proof}
Given the linear combinations 
$c_{1,1}x_1+\dots+c_{1,n}x_n$ through $c_{m,1}x_1+\dots+c_{m,n}x_n$,
consider a combination of those
\begin{equation*}
  d_1(c_{1,1}x_1+\dots+c_{1,n}x_n)\,+\dots+\,d_m(c_{m,1}x_1+\dots+c_{m,n}x_n)
\end{equation*}
where the $d$'s are scalars along with the $c$'s.
Distributing those $d$'s and regrouping gives
\begin{equation*}
  %&=d_1c_{1,1}x_1+\dots+d_1c_{1,n}x_n\,
  % +d_2c_{2,1}x_1+\dots+\,
  % d_mc_{1,1}x_1+\dots+d_mc_{1,n}x_n         \\
  =(d_1c_{1,1}+\dots+d_mc_{m,1})x_1\,+\dots+\,(d_1c_{1,n}+\dots+d_mc_{m,n})x_n
\end{equation*}
which is a linear combination of the $x$'s.
\end{proof}

In this subsection we will use the convention
that, where a matrix is named with an upper case roman letter,
the matching lower-case greek letter names the rows.
\begin{equation*}
  A=
    \begin{mat}
      \makebox[2.25cm]{$\cdots$\ $\alpha_1$\ $\cdots$}   \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\alpha_2$\ $\cdots$}   \\[.5ex]
      \vdotswithin{\makebox[2.25cm]{$\cdots$\ $\alpha_1$\ $\cdots$}}                  \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\alpha_m$\ $\cdots$}   
    \end{mat}
  \qquad
  B=
    \begin{mat}
      \makebox[2.25cm]{$\cdots$\ $\smash[b]{\beta}_1$\ $\cdots$}  \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\smash[b]{\beta}_2$\ $\cdots$}  \\[.5ex]
      \vdotswithin{\makebox[2.25cm]{$\cdots$\ $\beta_1$\ $\cdots$}}   \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\smash[b]{\beta}_m$\ $\cdots$}  
    \end{mat}
\end{equation*}

\begin{corollary} \label{cor:RowsOfEqMatsLinCombos}
Where one matrix reduces to another, each row of the second
is a linear combination of the rows of the first.
\end{corollary}

The proof 
uses induction.\appendrefs{mathematical induction}\spacefactor=1000 %
Before we proceed, here is an outline of the argument.
For the base step, we
will verify that the proposition is true when reduction 
can be done in zero row operations.
For the inductive step, we will 
argue that if being able to reduce the first matrix to the second in some
number $t\geq 0$ of operations implies that each row of the second is a linear
combination of the rows of the first, then being able to reduce the first to
the second in $t+1$ operations implies the same thing.
Together, these prove the result because  
the base step shows that it is true in the zero operations case,
and then the inductive step
implies that it is true in the one operation case, and then the inductive step
applied again gives that it is therefore true for two operations, etc.

\begin{proof}
We proceed by induction on the minimum number of row operations that take a
first matrix $A$ to a second one $B$.
In the base step, that
zero reduction operations suffice, the two matrices are equal and each 
row of $B$ is obviously a combination of
$A$'s rows: $\vec{\beta}_i
  =0\cdot\vec{\alpha}_1+\cdots+1\cdot\vec{\alpha}_i+\cdots+0\cdot\vec{\alpha}_m$.

For the inductive step, assume the inductive hypothesis:~with $t\geq 0$,
if \( B \) can be derived from \( A \) in \( t \) or fewer operations 
then \( B\)'s rows are linear combinations of $A$'s rows.
Suppose that reducing from \( A \) to \( B \) requires $t+1$ operations.
Because there are more than zero operations, 
there must be a next-to-last matrix $G$  
so that $A\longrightarrow\cdots\longrightarrow G\longrightarrow B$.
This \( G \) is only $t$ operations away from \( A \) and so the inductive
hypothesis applies to it. 
That is, each row of \( G \)
is a linear combination of the rows of \( A \).

If the operation taking \( G \) to \( B \) is a row swap then
the rows of $B$ are just the rows of $G$ reordered, and thus each row of $B$
is also a linear combination of the rows of $A$.
The other two possibilities for this operation, that it multiplies a 
row by a scalar and that it adds a multiple of one row to another, both result
in the rows of $B$ being linear combinations of the rows of $G$.
But therefore, by the Linear Combination Lemma, each row of $B$ is a linear
combination of the rows of $A$.

With both a base step and an inductive step,  
the proposition follows by the principle of mathematical induction.
\end{proof}

\begin{example}
In the reduction
\begin{equation*}
    \begin{mat}[r]
       0  &2  \\
       1  &1
     \end{mat}
    \grstep{\rho_1\leftrightarrow\rho_2}
    \begin{mat}[r]
       1  &1  \\
       0  &2
     \end{mat}                  
    \grstep{(1/2)\rho_2}
    \begin{mat}[r]
       1  &1  \\
       0  &1
     \end{mat}                   
    \grstep{-\rho_2+\rho_1}
    \begin{mat}[r]
       1  &0  \\
       0  &1
     \end{mat}
\end{equation*}
call the matrices \( A \), \( D \), \( G \), and \( B \).
The methods of the proof show that there are three sets of linear
relationships. 
\begin{equation*}
  \begin{aligned}
     \delta_1 &=0\cdot\alpha_1+1\cdot\alpha_2         \\
     \delta_2 &=1\cdot\alpha_1+0\cdot\alpha_2
  \end{aligned}
  \qquad
  \begin{aligned}
     \gamma_1 &=0\cdot\alpha_1+1\cdot\alpha_2         \\
     \gamma_2 &=(1/2)\alpha_1+0\cdot\alpha_2
  \end{aligned}
  \qquad
  \begin{aligned}
     \beta_1 &=(-1/2)\alpha_1+1\cdot\alpha_2        \\
     \beta_2 &=(1/2)\alpha_1+0\cdot\alpha_2
  \end{aligned}
\end{equation*}
\end{example}

The prior result gives us the insight that Gauss' method works by taking
linear combinations of the rows.
But to what end; why do we go to echelon form as a particularly simple, or
basic, version of a linear system?
The answer, of course, is that echelon form is suitable for back substitution,
because we have isolated the variables.
For instance, in this matrix
\begin{equation*}
  R=\begin{mat}[r]
    2  &3  &7  &8  &0  &0  \\
    0  &0  &1  &5  &1  &1  \\
    0  &0  &0  &3  &3  &0  \\
    0  &0  &0  &0  &2  &1
  \end{mat}
\end{equation*}
$x_1$ has been removed from $x_5$'s equation.
That is, Gauss' method has made $x_5$'s row independent of $x_1$'s row.

% Independence of a collection of row vectors, or of any kind of vectors, 
% will be precisely defined and explored in the next chapter.
% But a first take on it is that we can show that, say, the third row above
% is not comprised of the other rows, that
% $\rho_3\neq c_1\rho_1+c_2\rho_2+c_4\rho_4$.
% For, suppose that there are scalars $c_1$, $c_2$, and $c_4$ such that this
% relationship holds.
% \begin{align*}
%   \rowvec{0  &0  &0  &3  &3  &0}
%   &=c_1\rowvec{2 &3 &7 &8 &0 &0}             \\
%   &\quad\hbox{}+c_2\rowvec{0 &0 &1 &5 &1 &1} \\
%   &\quad\hbox{}+c_4\rowvec{0 &0 &0 &0 &2 &1}
% \end{align*}
% The first row's leading entry is in the first column and narrowing our
% consideration of the above relationship to consideration only of the entries
% from the first column $0=2c_1+0c_2+0c_4$ gives that $c_1=0$.
% The second row's leading entry is in the third column and the equation of
% entries in that column $0=7c_1+1c_2+0c_4$, along with the knowledge that
% $c_1=0$, gives that $c_2=0$.
% Now, to finish, the third row's leading entry is in the fourth column and the
% equation of entries in that column $3=8c_1+5c_2+0c_4$, along with $c_1=0$ and
% $c_2=0$, gives an impossibility.

The following result makes this precise.
What Gauss' linear elimination method eliminates is linear
relationships among the rows.

\begin{lemma}      \label{le:EchFormNoLinCombo}
In an echelon form matrix,
no nonzero row is a linear combination of the other nonzero rows.
\end{lemma}

\begin{proof}
Let $R$ be in echelon form.
If we have a non-\( \vec{0} \) row written as a combination
of the others
$\rho_i=c_1\rho_1+\cdots+c_{i-1}\rho_{i-1}+
               c_{i+1}\rho_{i+1}+\cdots+c_m\rho_m$
then we can rewrite the equation as
\begin{equation*}
   \vec{0}=c_1\rho_1+\cdots+c_{i-1}\rho_{i-1}+c_i\rho_i+
               c_{i+1}\rho_{i+1}+\cdots+c_m\rho_m
  \tag{$*$}
\end{equation*}
where $c_i=-1$.
We will use induction on the row index~$i$
to show that all of the coefficients~$c_i$ in that equation are~$0$.

The base case is the first row~$i=1$.
Equation~($*$) defines an equation among the column $\ell_1$ entries of those 
rows
(recall our notation that $\ell_i$ is the column number of 
the leading entry in row~$i$).
\begin{equation*}
  0=c_1r_{1,\ell_1}+c_2r_{2,\ell_1}+\cdots+c_mr_{m,\ell_1}
\end{equation*}
The matrix is in echelon form so
every row after the first has a zero in that column, and thus  
$r_{2,\ell_1}=\cdots=r_{m,\ell_1}=0$.
We conclude that $c_1=0$ because as the leading entry
in the row, $r_{1,\ell_1}\neq 0$.

The inductive step is to prove the implication:~if 
for each row index $k\in\set{1,\ldots,i}$ the coefficient $c_k$ is $0$
then $c_{i+1}$ is also $0$. 
Consider the entries from column~$\ell_{i+1}$ in equation~($*$). 
\begin{equation*}
  0=c_1r_{1,\ell_1}+\cdots+c_{i+1}r_{i+1,\ell_{i+1}}+\cdots+c_mr_{m,\ell_{i+1}}
\end{equation*}
By the inductive hypothesis the coefficients $c_1$, \ldots $c_i$ are
all $0$ so the equation reduces to 
$0=c_{i+1}r_{i+1,\ell_{i+1}}+\cdots+c_mr_{m,\ell_{i+1}}$.
As in the base case we next note that the matrix is in echelon form
so $r_{i+2,\ell_{i+1}}=\cdots=r_{m,\ell_{i+1}}=0$, and 
thus $c_{i+1}=0$ because $r_{i+1,\ell_{i+1}}\neq 0$ as it is the row's leading entry.
\end{proof}

\begin{theorem}
\label{th:ReducedEchelonFormIsUnique}
Each matrix is row equivalent to a unique reduced echelon form matrix.
\end{theorem}

\begin{proof} \cite{Yuster}
Fix a number of rows \( m \).
We will proceed by induction on the number of columns \( n \).

The base case is that the matrix has \( n=1 \) column.
If this is the zero matrix then its unique echelon form is the zero matrix. 
If instead it has any nonzero entries then when the matrix is brought to 
reduced echelon form, it must have at least one nonzero entry, so it has a
\( 1 \), in the first row. 
In either case, its reduced echelon form is unique.

For the inductive step we assume that \( n>1 \) and that all \( m \)~row
matrices with fewer than \( n \) columns have a unique reduced echelon form.
Consider a \( \nbym{m}{n} \) matrix \( A \) and suppose that 
\( B \) and \( C \) are two reduced echelon form matrices derived from \( A \).
We will show that these two must be equal.

Let \( \hat{A} \) be the matrix consisting of the first \( n-1 \) columns of
\( A \).
Observe that 
% if an $n$~column matrix is in reduced echelon form then any initial
% set of its columns is also in reduced echelon form, so \( \hat{A} \)
% is in reduced echelon form. 
any sequence of row operations that bring \( A \) to reduced 
echelon form will also bring \( \hat{A} \) to reduced echelon form.
By the inductive hypothesis this reduced echelon form of \( \hat{A} \)
is unique, so if \( B \) and \( C \) differ then the difference must 
occur in their \( n \)-th columns.

We finish the inductive step, and the argument,
by showing that the two cannot differ only in that column.
Consider a homogeneous system of equations for which \( A \) is the
matrix of coefficients.  
\begin{equation*}
  \begin{linsys}{4}
    a_{1,1}x_1  &+  &a_{1,2}x_2  &+  &\cdots  &+  &a_{1,n}x_n  &=  &0  \\
    a_{2,1}x_1  &+  &a_{2,2}x_2  &+  &\cdots  &+  &a_{2,n}x_n  &=  &0  \\
              &&&&&&&\vdotswithin{=}  \\
    a_{m,1}x_1  &+  &a_{m,2}x_2  &+  &\cdots  &+  &a_{m,n}x_n  &=  &0  
  \end{linsys}
  \tag{$*$}
\end{equation*}
By \nearbytheorem{th:GaussMethod}
%the first theorem of this chapter 
the set of solutions to that system
is the same as the set of solutions to this system
\begin{equation*}
  \begin{linsys}{4}
    b_{1,1}x_1  &+  &b_{1,2}x_2  &+  &\cdots  &+  &b_{1,n}x_n  &=  &0  \\
    b_{2,1}x_1  &+  &b_{2,2}x_2  &+  &\cdots  &+  &b_{2,n}x_n  &=  &0  \\
               &&&&&&&\vdotswithin{=}  \\
    b_{m,1}x_1  &+  &b_{m,2}x_2  &+  &\cdots  &+  &b_{m,n}x_n  &=  &0  
  \end{linsys}
  \tag{$**$}
\end{equation*}
and to this one.
\begin{equation*}
  \quad
  \begin{linsys}{4}
    c_{1,1}x_1  &+  &c_{1,2}x_2  &+  &\cdots  &+  &c_{1,n}x_n  &=  &0  \\
    c_{2,1}x_1  &+  &c_{2,2}x_2  &+  &\cdots  &+  &c_{2,n}x_n  &=  &0  \\
               &&&&&&&\vdotswithin{=}  \\
    c_{m,1}x_1  &+  &c_{m,2}x_2  &+  &\cdots  &+  &c_{m,n}x_n  &=  &0  
  \end{linsys}
  \tag{$\mathord{*}\mathord{*}\mathord{*}$}
\end{equation*}
Suppose that \( B  \) and \( C \) differ only in column~\( n \), in row~\( i \).
Subtract row~\( i \) of ($\mathord{*}\mathord{*}\mathord{*}$) from 
row~\( i \) of ($**$).
Since \( B \) and \( C \) agree on their first \( n-1\)~columns the 
result is the equation 
\( (b_{i,n}-c_{i,n})\cdot x_n=0 \).
Because \( b_{i,n}\neq c_{i,n} \) we know that $x_n=0$.
Thus in ($**$) and~($\mathord{*}\mathord{*}\mathord{*}$)
the \( n \)-th column has a leading \( 1 \), or else 
the variable \( x_n \) would be free.
That's a contradiction, because with the first \( n-1 \)~columns of
\( B \) and \( C \) equal, the leading \( 1 \)'s in the 
\( n \)-th column would have to be in the same row.
So \( B=C \).
\end{proof}

That result answers the two questions that we posed in the introduction
to this chapter: do any two echelon form versions of a linear system 
have the same number of free variables, and if so are they
exactly the same variables?
We get from any echelon form version to the reduced echelon form by pivoting up,
and so uniqueness of reduced echelon form implies that the same variables 
are free in all echelon form version of a system.
Thus both questions are answered ``yes.''
There is no linear system and no combination of row operations such
that, say, we could solve the system 
one way and get $y$ and $z$ free but solve it
another way and get $y$ and $w$ free.

We end this section with a recap.
In Gauss' method we start with a matrix and then
derive a sequence of other matrices.
We defined two matrices to be related if one can be derived from the other.
That relation is an equivalence relation, %\appendrefs{equivalence relation} 
called row equivalence, and
so partitions the set of all matrices into row equivalence classes.
\begin{center}
  \includegraphics{ch1.30}
\end{center}
(There are infinitely many matrices in the pictured class, but we've only
got room to show two.)
We have proved there is one and only one reduced echelon form matrix in
each row equivalence class.
So the reduced echelon form is a
{\em canonical form}\appendrefs{canonical representatives}%
\index{canonical form!for row equivalence}%
\index{representative!for row equivalence classes}%
for row equivalence:
the reduced echelon form matrices are
representatives of the classes.
\begin{center}
  \includegraphics{ch1.31}
\end{center}
We can answer questions about the classes by translating them
into questions about the representatives.

\begin{example}  \label{ex:MatsNotRowEq}
We can decide if matrices are interreducible
by seeing if Gauss-Jordan reduction produces the same
reduced echelon form result.
Thus, these are not row equivalent
\begin{equation*}
  \begin{mat}[r]
    1  &-3  \\
   -2  &6
  \end{mat}
  \qquad
  \begin{mat}[r]
    1  &-3  \\
   -2  &5
  \end{mat}
\end{equation*}
because their reduced echelon forms are not equal.
\begin{equation*}
  \begin{mat}[r]
    1  &-3  \\
    0  &0
  \end{mat}
  \qquad
  \begin{mat}[r]
    1  &0   \\
    0  &1
  \end{mat}
\end{equation*}
\end{example}

\begin{example}
Any nonsingular \( \nbyn{3} \) matrix Gauss-Jordan reduces to this.
\begin{equation*}
    \begin{mat}[r]
      1  &0  &0 \\
      0  &1  &0 \\
      0  &0  &1
    \end{mat}
\end{equation*}
\end{example}

\begin{example} \label{ex:RowEqClassTwoTwoMats}
We can describe the classes by listing all possible
reduced echelon form matrices.
Any $\nbyn{2}$ matrix lies in one of these:~the class of matrices
row equivalent to this,
\begin{equation*}
  \begin{mat}[r]
     0  &0  \\
     0  &0
  \end{mat}
\end{equation*}
the infinitely many classes of matrices row equivalent to one of this type
\begin{equation*}
  \begin{mat}
     1  &a  \\
     0  &0
  \end{mat}
\end{equation*}
where \( a\in\Re \) (including $a=0$),
the class of matrices row equivalent to this,
\begin{equation*}
  \begin{mat}[r]
     0  &1  \\
     0  &0
  \end{mat}
\end{equation*}
and the class of matrices row equivalent to this
\begin{equation*}
  \begin{mat}[r]
     1  &0  \\
     0  &1
  \end{mat}
\end{equation*}
(this is the class of nonsingular $\nbyn{2}$ matrices).
\end{example}



\begin{exercises}
  \recommended \item 
    Decide if the matrices are row equivalent.
    \begin{exparts*}
       \partsitem \(
           \begin{mat}[r]
             1  &2  \\
             4  &8
           \end{mat}, 
           \begin{mat}[r]
             0  &1  \\
             1  &2
           \end{mat} \)
       \partsitem \(
           \begin{mat}[r]
             1  &0  &2  \\
             3  &-1 &1  \\
             5  &-1 &5
           \end{mat},  
           \begin{mat}[r]
             1  &0  &2  \\
             0  &2  &10 \\
             2  &0  &4
           \end{mat} \)
       \partsitem \(
           \begin{mat}[r]
             2  &1  &-1 \\
             1  &1  &0  \\
             4  &3  &-1
           \end{mat},  
           \begin{mat}[r]
             1  &0  &2  \\
             0  &2  &10 \\
           \end{mat} \)
       \partsitem \(
           \begin{mat}[r]
             1  &1  &1  \\
            -1  &2  &2
           \end{mat},  
           \begin{mat}[r]
             0  &3  &-1 \\
             2  &2  &5
           \end{mat} \)
       \partsitem \(
           \begin{mat}[r]
             1  &1  &1  \\
             0  &0  &3
           \end{mat},  
           \begin{mat}[r]
             0  &1  &2  \\
             1  &-1 &1
           \end{mat} \)
    \end{exparts*}
    \begin{answer}
      Bring each to reduced echelon form and compare.
      \begin{exparts}
        \partsitem The first gives
          \begin{equation*}
            \grstep{-4\rho_1+\rho_2}
            \begin{mat}[r]
              1  &2  \\
              0  &0
            \end{mat}
          \end{equation*}
          while the second gives
          \begin{equation*}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \begin{mat}[r]
              1  &2  \\
              0  &1
            \end{mat}
            \grstep{-2\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  \\
              0  &1
            \end{mat}
          \end{equation*}
          The two reduced echelon form matrices are not identical, and so the
          original matrices are not row equivalent.
        \partsitem The first is this.
          \begin{equation*}
            \grstep[-5\rho_1+\rho_3]{-3\rho_1+\rho_2}
            \begin{mat}[r]
              1  &0  &2  \\
              0  &-1 &-5 \\
              0  &-1 &-5
            \end{mat}
            \grstep{-\rho_2+\rho_3}
            \begin{mat}[r]
              1  &0  &2  \\
              0  &-1 &-5 \\
              0  &0  &0
            \end{mat}
            \grstep{-\rho_2}
            \begin{mat}[r]
              1  &0  &2  \\
              0  &1  &5  \\
              0  &0  &0
            \end{mat}
          \end{equation*}
          The second is this.
          \begin{equation*}
            \grstep{-2\rho_1+\rho_3}
            \begin{mat}[r]
              1  &0  &2  \\
              0  &2  &10 \\
              0  &0  &0
            \end{mat}
            \grstep{(1/2)\rho_2}
            \begin{mat}[r]
              1  &0  &2  \\
              0  &1  &5  \\
              0  &0  &0
            \end{mat}
          \end{equation*}
          These two are row equivalent.
        \partsitem These two are not row equivalent because they have different
          sizes.
        \partsitem The first,
          \begin{equation*}
            \grstep{\rho_1+\rho_2}
            \begin{mat}[r]
              1  &1  &1  \\
              0  &3  &3
            \end{mat}
            \grstep{(1/3)\rho_2}
            \begin{mat}[r]
              1  &1  &1  \\
              0  &1  &1
            \end{mat}
            \grstep{-\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  &0  \\
              0  &1  &1
            \end{mat}
          \end{equation*}
          and the second.
          \begin{equation*}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \begin{mat}[r]
              2  &2  &5  \\
              0  &3  &-1
            \end{mat}
            \grstep[(1/3)\rho_2]{(1/2)\rho_1}
            \begin{mat}[r]
              1  &1  &5/2 \\
              0  &1  &-1/3
            \end{mat}
            \grstep{-\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  &17/6 \\
              0  &1  &-1/3
            \end{mat}
          \end{equation*}
          These are not row equivalent.
        \partsitem Here the first is
          \begin{equation*}
            \grstep{(1/3)\rho_2}
            \begin{mat}[r]
              1  &1  &1  \\
              0  &0  &1
            \end{mat}
            \grstep{-\rho_2+\rho_1}
            \begin{mat}[r]
              1  &1  &0  \\
              0  &0  &1
            \end{mat}
          \end{equation*}
          while this is the second.
          \begin{equation*}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \begin{mat}[r]
              1  &-1 &1  \\
              0  &1  &2
            \end{mat}
            \grstep{\rho_2+\rho_1}
            \begin{mat}[r]
              1  &0  &3  \\
              0  &1  &2
            \end{mat}
          \end{equation*}
          These are not row equivalent.
       \end{exparts}  
     \end{answer}
  \item 
     Describe the matrices in each of the classes represented in
     \nearbyexample{ex:RowEqClassTwoTwoMats}.
     \begin{answer}
       First, the only matrix row equivalent to the matrix of all
       \( 0 \)'s is itself (since row operations have no effect).

       Second, the matrices that reduce to 
       \begin{equation*}
         \begin{mat}
           1  &a  \\
           0  &0
         \end{mat}
       \end{equation*}
       have the form
       \begin{equation*}
         \begin{mat}
           b  &ba \\
           c  &ca
         \end{mat}
       \end{equation*}
       (where \( a,b,c\in\Re \), and \(b\) and \(c\) are not both zero).  

       Next, the matrices that reduce to 
       \begin{equation*}
         \begin{mat}[r]
           0  &1  \\
           0  &0
         \end{mat}
       \end{equation*}
       have the form
       \begin{equation*}
         \begin{mat}
           0  &a \\
           0  &b
         \end{mat}
       \end{equation*}
       (where \( a,b\in\Re \), and not both are zero).  

       Finally, the matrices that reduce to 
       \begin{equation*}
         \begin{mat}[r]
           1  &0  \\
           0  &1
         \end{mat}
       \end{equation*}
       are the nonsingular matrices.
       That's because a linear system for which this is the matrix of
       coefficients will have a unique solution, and that is the definition
       of nonsingular.
       (Another way to say the same thing is to say that they fall into none
       of the above classes.)
     \end{answer}
  \item 
    Describe all matrices in the row equivalence class of
    these.
    \begin{exparts*}
       \partsitem  \(
           \begin{mat}[r]
             1  &0  \\
             0  &0
           \end{mat}  \)
       \partsitem  \(
           \begin{mat}[r]
             1  &2      \\
             2  &4
           \end{mat} \)
       \partsitem \(
           \begin{mat}[r]
             1  &1      \\
             1  &3
           \end{mat} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem They have the form
          \begin{equation*}
            \begin{mat}
              a  &0  \\
              b  &0
            \end{mat}
          \end{equation*}
          where \( a,b\in\Re \).
        \partsitem They have this form (for \( a,b\in\Re \)).
          \begin{equation*}
            \begin{mat}
             1a  &2a \\
             1b  &2b
            \end{mat}
          \end{equation*}
        \partsitem They have the form
          \begin{equation*}
            \begin{mat}
              a  &b  \\
              c  &d
            \end{mat}
          \end{equation*}
          (for \( a,b,c,d\in\Re \)) where \( ad-bc\neq 0 \).
          (This is the formula that determines when a \( \nbyn{2} \) matrix
          is nonsingular.)
      \end{exparts}  
    \end{answer}
  \item 
    How many row equivalence classes are there?
    \begin{answer}
       Infinitely many.
       For instance, in 
       \begin{equation*}
         \begin{mat}
           1  &k  \\
           0  &0
         \end{mat}
       \end{equation*}
       each $k\in\Re$ gives a different class.  
    \end{answer}
  \item 
    Can row equivalence classes contain different-sized matrices?
    \begin{answer}
      No.
      Row operations do not change the size of a matrix.  
    \end{answer}
  \item 
    How big are the row equivalence classes?
    \begin{exparts} 
      \partsitem Show that for any matrix of all zeros, the class is finite.
      \partsitem Do any other classes contain only finitely many members?
    \end{exparts}
    \begin{answer}
     \begin{exparts}
      \partsitem A row operation on a matrix of zeros has no effect.
        Thus each such matrix is alone in its row equivalence class.  
      \partsitem No.
        Any nonzero entry can be rescaled.
     \end{exparts}
    \end{answer}
  \recommended \item 
    Give two reduced echelon form matrices that have their leading
    entries in the same columns,
    but that are not row equivalent.
    \begin{answer}
      Here are two.
      \begin{equation*}
        \begin{mat}[r]
          1  &1  &0  \\
          0  &0  &1
        \end{mat}
        \quad\text{and}\quad
        \begin{mat}[r]
          1  &0  &0  \\
          0  &0  &1
        \end{mat}
      \end{equation*}  
     \end{answer}
  \recommended \item 
    Show that any two \( \nbyn{n} \) nonsingular matrices are
    row equivalent.
    Are any two singular matrices row equivalent?
    \begin{answer}
      Any two \( \nbyn{n} \) nonsingular matrices have
      the same reduced echelon
      form, namely the matrix with all \( 0 \)'s except for \( 1 \)'s down
      the diagonal.
      \begin{equation*}
        \begin{mat}
          1  &0  &       &0  \\
          0  &1  &       &0  \\
             &   &\ddots &   \\
          0  &0  &       &1
        \end{mat}
      \end{equation*}

      Two same-sized singular matrices need not be row equivalent.
      For example, these two \( \nbyn{2} \) singular matrices
      are not row equivalent.
      \begin{equation*}
        \begin{mat}[r]
          1  &1  \\
          0  &0
        \end{mat}
        \quad\text{and}\quad
        \begin{mat}[r]
          1  &0  \\
          0  &0
        \end{mat}
      \end{equation*}  
    \end{answer}
  \recommended \item 
    Describe all of the row equivalence classes containing these.
    \begin{exparts*}
      \partsitem \( \nbyn{2} \)~matrices
      \partsitem \( \nbym{2}{3} \)~matrices
      \partsitem \( \nbym{3}{2} \)~matrices
      \partsitem \( \nbyn{3} \)~matrices
    \end{exparts*}
    \begin{answer}
      Since there is one and only one reduced echelon form matrix in each
      class, we can just list the possible reduced echelon form matrices.

      For that list, see the answer for \nearbyexercise{exer:PossRedEchFrms}. 
    \end{answer}
  \item  
     \begin{exparts}
          \partsitem Show that a vector $\vec{\beta}_0$ is a linear combination
            of members of the set $\set{\vec{\beta}_1,\ldots,\vec{\beta}_n}$
            if and only if there is a linear relationship 
            $\zero=c_0\vec{\beta}_0+\cdots+c_n\vec{\beta}_n$
            where $c_0$ is not zero.
            (\textit{Hint.}   Watch out for the $\vec{\beta}_0=\zero$ case.)
         \partsitem Use that to simplify the proof of 
            \nearbylemma{le:EchFormNoLinCombo}.   
       \end{exparts}
       \begin{answer}
          \begin{exparts}
           \partsitem If there is a linear relationship where $c_0$ is not zero
             then we can subtract $c_0\vec{\beta}_0$ from both sides and divide
             by $-c_0$ to get $\vec{\beta}_0$ as a linear
             combination of the others.
             (Remark:  
             if there are no other vectors in the set\Dash if the 
             relationship is, say, 
             $\zero=3\cdot\zero$\Dash then the statement is still true because
             the zero vector is by definition the sum of the empty set 
             of vectors.)

             Conversely, if $\vec{\beta}_0$ is a combination of the others 
             $\vec{\beta}_0=c_1\vec{\beta}_1+\dots+c_n\vec{\beta}_n$
             then subtracting
             $\vec{\beta}_0$ from both sides gives a relationship where 
             at least one
             of the coefficients is nonzero; namely,
             the $-1$ in front of $\vec{\beta}_0$.
           \partsitem The first row is not a linear combination of the
             others for
             the reason given in the proof:~in the equation of components from
             the column containing the leading entry of the first row, the
             only nonzero entry is the leading entry from the first row, so
             its coefficient must be zero.
             Thus, from the prior part of this exercise, the first row is in
             no linear relationship with the other rows.

             Thus, when considering whether the second row can be in a linear 
             relationship
             with the other rows, we can leave the first row out.
             But now the argument just applied to the first row will apply
             to the second row.
             (That is, we are arguing here by induction.)             
         \end{exparts}
      \end{answer}
  % \item 
  %   Why, in the proof of \nearbytheorem{th:ReducedEchelonFormIsUnique},
  %   do we bother to restrict to the nonzero rows?
  %   Why not just stick to the relationship that we began with,
  %   $\beta_i=c_{i,1}\delta_1+\dots+c_{i,m}\delta_m$, with $m$ instead of $r$,
  %   and argue using it that the only nonzero coefficient
  %   is \( c_{i,i}  \), which is \( 1 \)?
  %   \begin{answer}
  %      The zero rows could have nonzero coefficients, and
  %      so the statement would not be true.
  %   \end{answer}
  \recommended \item 
   \cite{Trono}
   Three truck drivers went into a roadside cafe.
   One truck driver purchased four sandwiches, a cup of coffee, and ten 
   doughnuts for \$$8.45$.
   Another driver purchased three sandwiches, a cup of coffee, and seven
   doughnuts for \$$6.30$.
   What did the third truck driver pay for a sandwich, a cup of coffee, and 
   a doughnut?
   \begin{answer}
     We know that $4s+c+10d=8.45$ and that $3s+c+7d=6.30$, and we'd like to
     know what $s+c+d$ is.
     Fortunately, $s+c+d$ is a linear combination of $4s+c+10d$ and $3s+c+7d$.
     Calling the unknown price $p$, we have this reduction.
     \begin{equation*}
       \begin{amat}{3}
         4  &1  &10  &8.45 \\
         3  &1  &7   &6.30 \\
         1  &1  &1   &p
       \end{amat}
       \grstep[-(1/4)\rho_1+\rho_3]{-(3/4)\rho_1+\rho_2}
       \begin{amat}{3}
         4  &1    &10     &8.45      \\
         0  &1/4  &-1/2   &-0.037\,5 \\
         0  &3/4  &-3/2   &p-2.112\,5
       \end{amat}
       \grstep{-3\rho_2+\rho_3}
       \begin{amat}{3}
         4  &1    &10     &8.45      \\
         0  &1/4  &-1/2   &-0.037\,5 \\
         0  &0    &0      &p-2.00
       \end{amat}
     \end{equation*}
     The price paid is \$$2.00$.
   \end{answer}
  % \item
  %  The fact that Gaussian reduction disallows multiplication of
  %  a row by zero is needed for the proof of uniqueness of reduced echelon form,
  %  or else every matrix would
  %  be row equivalent to a matrix of all zeros.
  %  Where is it used?
  %  \begin{answer}
  %    If multiplication of a row by zero were allowed then
  %    \nearbylemma{le:EquivMatsSameForm}
  %    would not hold.
  %    That is, where
  %    \begin{equation*}
  %      \begin{mat}[r]
  %        1  &3  \\
  %        2  &1
  %      \end{mat}
  %      \grstep{0\rho_2}
  %      \begin{mat}[r]
  %        1  &3  \\
  %        0  &0
  %      \end{mat}
  %    \end{equation*}
  %    all the rows of the second matrix can be expressed as linear combinations
  %    of the rows of the first, but the converse does not hold.
  %    The second row of the first matrix is not a linear combination of the
  %    rows of the second matrix.  
  %  \end{answer}
  \recommended \item 
   The Linear Combination Lemma says which equations can be gotten from
   Gaussian reduction from a given linear system.
   \begin{enumerate}
     \item Produce an equation not implied by this system.
       \begin{equation*}
         \begin{linsys}{2}
           3x  &+  &4y  &=  &8 \\
           2x  &+  & y  &=  &3 
         \end{linsys}
       \end{equation*}
     \item Can any equation be derived from an inconsistent system?
   \end{enumerate}
   \begin{answer}
     \begin{enumerate}
        \item An easy answer is this:
          \begin{equation*}
            0=3.
          \end{equation*}
          For a less wise-guy-ish answer, solve the system:
          \begin{equation*}
            \begin{amat}[r]{2}
              3  &-1  &8  \\
              2  &1   &3
            \end{amat}
            \grstep{-(2/3)\rho_1+\rho_2}
            \begin{amat}[r]{2}
              3  &-1  &8    \\
              0  &5/3 &-7/3
            \end{amat}
          \end{equation*}
          gives \( y=-7/5 \) and \( x=11/5 \).
          Now any equation not satisfied by \( (-7/5,11/5) \) will do,
          e.g., \( 5x+5y=3 \).
        \item Every equation can be derived from an inconsistent system.
          For instance, here is how to derive ``\( 3x+2y=4 \)'' from
          ``\( 0=5 \)''.
          First,
          \begin{equation*}
            0=5
            \grstep{(3/5)\rho_1}
            0=3
            \grstep{x\rho_1}
            0=3x
          \end{equation*}
          (validity of the \( x=0 \) case is separate but clear).
          Similarly, \( 0=2y \).
          Ditto for \( 0=4 \).
          But now, \( 0+0=0 \) gives \( 3x+2y=4 \).
     \end{enumerate}  
    \end{answer}
  \item 
    \cite{HoffmanKunze}
    Extend the definition of row equivalence to linear systems.
    Under your definition, do equivalent systems have the same solution set?
    \begin{answer}
      Define linear systems to be equivalent if their augmented
      matrices are row equivalent.
      The proof that equivalent systems have the same solution set is easy.  
    \end{answer}
  \recommended \item 
    In this matrix
    \begin{equation*}
      \begin{mat}[r]
        1  &2  &3  \\
        3  &0  &3  \\
        1  &4  &5
      \end{mat}
    \end{equation*}
    the first and second columns add to the third.
    \begin{exparts}
      \partsitem Show that remains true under any row operation.
      \partsitem Make a conjecture.
      \partsitem Prove that it holds.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The three possible row swaps are easy, 
          as are the three possible rescalings.
          One of the six possible row combinations is \( k\rho_1+\rho_2 \):
          \begin{equation*}
            \begin{mat}
              1           &2           &3  \\
              k\cdot 1+3  &k\cdot 2+0  &k\cdot 3+3  \\
              1           &4           &5
            \end{mat}
          \end{equation*}
          and again the first and second columns add to the third.
          The other five combinations are similar.
        \partsitem The obvious conjecture is that row operations do not change
          linear relationships among columns.
        \partsitem A case-by-case 
          proof follows the sketch given in the first item.
      \end{exparts}  
   \end{answer}
\end{exercises}
