% Chapter 1, Section 3 _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linalg.html
%  2001-Jun-09
\section{Reduced Echelon Form}
After developing the mechanics of Gauss' method, 
we observed that it can be done in more than one way.
One example is that from this matrix 
\begin{equation*}
    \begin{pmatrix}
       2  &2  \\
       4  &3
    \end{pmatrix}
\end{equation*}
we could derive any of these three echelon form matrices.
\begin{equation*}
    \begin{pmatrix}
       2  &2  \\
       0  &-1
    \end{pmatrix}
    \qquad
    \begin{pmatrix}
       1  &1  \\
       0  &-1
    \end{pmatrix}
    \qquad
    \begin{pmatrix}
       2  &0  \\
       0  &-1
    \end{pmatrix}
\end{equation*}
The first results from $-2\rho_1+\rho_2$.
The second comes from following $(1/2)\rho_1$ with $-4\rho_1+\rho_2$.
The third comes
from $-2\rho_1+\rho_2$ followed by $2\rho_2+\rho_1$
(after the first row combination the matrix is already in
echelon form so the second one is extra work 
but it is nonetheless a legal row operation).

The fact that echelon form 
is not unique leaves us with some questions.
Will any two echelon form versions of a linear system have the same number of
free variables?
If yes, 
will the two have exactly the same variables free?
In this section we will answer both questions ``yes''.
We will do more than answer those questions.
We will give a way to decide if one linear system 
can be derived from another by row operations.
The answers to the two questions will follow from this larger result.








\subsection{Gauss-Jordan Reduction}%
Gaussian elimination coupled with back-substitution
solves linear systems but it is not the only method possible.
Here is an extension of Gauss' method that has some advantages.

\begin{example}
To solve
\begin{equation*}
  \begin{linsys}{3}
    x  &+  &y  &-  &2z  &=  &-2  \\
       &   &y  &+  &3z  &=  &7   \\
    x  &   &   &-  &z   &=  &-1  
  \end{linsys}
\end{equation*}
we can start as usual by going to echelon form.
\begin{equation*}
  \grstep{-\rho_1+\rho_3}
    \begin{amatrix}{3}
       1  &1  &-2 &-2  \\
       0  &1  &3  &7   \\
       0  &-1 &1  &1
    \end{amatrix}
  \grstep{\rho_2+\rho_3}
    \begin{amatrix}{3}
       1  &1  &-2 &-2  \\
       0  &1  &3  &7   \\
       0  &0  &4  &8
    \end{amatrix}
\end{equation*}
We can keep going to a second stage
by making the leading entries into ones
\begin{equation*}
    \grstep{(1/4)\rho_3}
    \begin{amatrix}{3}
       1  &1  &-2 &-2  \\
       0  &1  &3  &7   \\
       0  &0  &1  &2
    \end{amatrix}
\end{equation*}
and then to a third stage that uses the leading entries 
to eliminate all of the other entries in each column 
by combining upwards.
\begin{equation*}
  \grstep[2\rho_3+\rho_1]{-3\rho_3+\rho_2}
    \begin{amatrix}{3}
       1  &1  &0  &2   \\
       0  &1  &0  &1   \\
       0  &0  &1  &2
    \end{amatrix}
  \grstep{-\rho_2+\rho_1}
    \begin{amatrix}{3}
       1  &0  &0  &1   \\
       0  &1  &0  &1   \\
       0  &0  &1  &2
    \end{amatrix}
\end{equation*}
The answer is \( x=1 \), \( y=1 \), and \( z=2 \).
\end{example}
Using one entry to clear out the rest of a column is
called \definend{pivoting}\index{pivoting} on that entry.

Note that the row combination operations in the first stage proceed from column
one to column three while the combination operations in the third stage proceed 
from column three to column one.

\begin{example}
The middle stage operations that 
turn the leading entries into \( 1 \)'s
don't interact, so we can combine them into a single step.
\begin{eqnarray*}
    \begin{amatrix}{2}
       2   &1   &7   \\
       4   &-2  &6
    \end{amatrix}
  &\grstep{-2\rho_1+\rho_2}
  &\begin{amatrix}{2}
       2   &1   &7   \\
       0   &-4  &-8
    \end{amatrix}                                   \\
  &\grstep[(-1/4)\rho_2]{(1/2)\rho_1}
  &\begin{amatrix}{2}
       1   &1/2   &7/2   \\
       0   &1     &2
    \end{amatrix}                                    \\
  &\grstep{-(1/2)\rho_2+\rho_1}
  &\begin{amatrix}{2}
       1   &0   &5/2   \\
       0   &1   &2
    \end{amatrix}
\end{eqnarray*}
The answer is $x=5/2$ and $y=2$.
\end{example}

This extension of Gauss' method is 
\definend{Gauss-Jordan reduction}.\index{linear equation!solution of!Gauss-Jordan}\index{Gauss-Jordan}\index{Gauss' method!Gauss-Jordan}
It goes past echelon form to a more refined, more specialized,
matrix form.

\begin{definition}\label{def:RedEchForm}
A matrix is in
\definend{reduced echelon form\/}\index{echelon form!reduced}\index{reduced echelon form}
if, in addition to being in echelon form, each leading entry is a
one and is the only nonzero entry in its column.
\end{definition}

\noindent
The cost of using Gauss-Jordan reduction to solve a system 
is the additional arithmetic.
The benefit is that we can just read off the solution set
from the reduced echelon form.

In any echelon form, plain or reduced, we can read off 
when a system has an empty
solution set because there is a contradictory equation.
We can also read off 
when a system has a one-element solution set because there is no
contradiction and every
variable is the leading variable in some row.
And, we can read off when a system has an infinite solution set because 
there is no contradiction and at least one variable is free.

In reduced echelon form we can read off not just what kind of 
solution set the system has, but also its description.
Whether or not the echelon form is reduced,
we have no trouble describing the solution set when it is empty, of course.
The two examples above show that when the system has a single solution then
the solution can be read off from the right-hand column.
In the case when the solution set is infinite, its
parametrization can also be read off of the reduced echelon form.
Consider, for example, this system that is shown brought to echelon form and
then to reduced echelon form.
\begin{multline*}
  \begin{amatrix}{4}
     2  &6  &1  &2  &5  \\
     0  &3  &1  &4  &1  \\
     0  &3  &1  &2  &5
  \end{amatrix}
  \grstep{-\rho_2+\rho_3}
  \begin{amatrix}{4}
     2  &6  &1  &2  &5  \\
     0  &3  &1  &4  &1  \\
     0  &0  &0  &-2 &4
  \end{amatrix}                                        \\
  \grstep[(1/3)\rho_2 \\ -(1/2)\rho_3]{(1/2)\rho_1}
  \;\grstep[-\rho_3+\rho_1]{(4/3)\rho_3+\rho_2}
  \;\grstep{-3\rho_2+\rho_1}
  \begin{amatrix}{4}
     1  &0  &-1/2  &0  &-9/2  \\
     0  &1  &1/3   &0  &3  \\
     0  &0  &0     &1  &-2
  \end{amatrix}
\end{multline*}
Starting with the middle matrix, the echelon form version, 
back substitution produces $-2x_4=4$ so that $x_4=-2$,
then another back substitution gives
$3x_2+x_3+4(-2)=1$ implying that $x_2=3-(1/3)x_3$, 
and then the final back substitution gives
$2x_1+6(3-(1/3)x_3)+x_3+2(-2)=5$ implying that $x_1=-(9/2)+(1/2)x_3$.
Thus the solution set is this.
\begin{equation*}  
  S=\set{\colvec{x_1 \\ x_2 \\ x_3 \\ x_4}
                        =\colvec{-9/2 \\ 3 \\ 0 \\ -2}
                         +\colvec{1/2 \\ -1/3 \\ 1 \\ 0}x_3
                        \suchthat x_3\in\Re}
\end{equation*}
Now, considering the final matrix, the reduced echelon form version, note that
adjusting the parametrization by moving the $x_3$ terms to the other side
does indeed give the description of this infinite solution set.

Part of the reason that this works is straightforward.
While a set can have many parametrizations that describe it, e.g.,
both of these also describe the above set $S$
(take $t$ to be $x_3/6$ and $s$ to be $x_3-1$) 
\begin{equation*}
  \set{\colvec{-9/2 \\ 3 \\ 0 \\ -2}
                         +\colvec{3 \\ -2 \\ 6 \\ 0}t
                        \suchthat t\in\Re}
  \qquad
  \set{\colvec{-4 \\ 8/3 \\ 1 \\ -2}
                         +\colvec{1/2 \\ -1/3 \\ 1 \\ 0}s
                        \suchthat s\in\Re}
\end{equation*} 
nonetheless we have in this book stuck to a convention of
parametrizing using the unmodified free variables (that is, 
$x_3=x_3$ instead of $x_3=6t$).
We can easily see that
a reduced echelon form version of a system is equivalent to
a parametrization in terms of unmodified free variables.
For instance,
\begin{equation*}
  \begin{aligned}
    x_1 &=4-2x_3 \\
    x_2 &=3-x_3
  \end{aligned}
  \quad\Longleftrightarrow\quad
  \begin{amatrix}{3}
    1  &0  &2  &4  \\
    0  &1  &1  &3  \\
    0  &0  &0  &0
  \end{amatrix}
\end{equation*}
(to move from left to right we also need to know how many equations are
in the system).
So, the convention of parametrizing with the free variables by solving
each equation for its leading variable and then eliminating that leading
variable from every other equation is exactly equivalent to the reduced
echelon form conditions that each leading entry must be a one and must be the
only nonzero entry in its column.

Not as straightforward is the other 
part of the reason that the reduced echelon form version allows us to
read off the parametrization that we would have gotten had we stopped at
echelon form and then done back substitution.
The prior paragraph shows that reduced echelon form corresponds to some
parametrization, but why the same parametrization?
A solution set can be parametrized in many ways, and Gauss' method
or the Gauss-Jordan method can be done in many ways, so a first guess
might be that we could derive many different reduced echelon form
versions of the same starting system and many different parametrizations.
But we never do.
Experience shows that starting with the same system and  proceeding with row
operations in many different ways always yields the same reduced echelon form
and the same parametrization (using the unmodified free variables).

In the rest 
of this section we will show that the reduced echelon form version
of a matrix is unique.
It follows that the parametrization of a linear system
in terms of its unmodified free variables is unique
because two different ones would give two different reduced echelon forms.

We shall use this result, and the ones that lead up to it, in the rest of
the book but perhaps a restatement in a way that makes it seem more 
immediately useful may be encouraging.
Imagine that we solve a linear system, parametrize, and check in the back of
the book for the answer.
But the parametrization there appears different.
Have we made a mistake, or could these be different-looking
descriptions of the same set, as with the three descriptions above of $S$?
The prior paragraph notes that
we will show here   
that different-looking parametrizations (using the unmodified free
variables) describe genuinely different sets. 

Here is an informal argument that the reduced 
echelon form version of a matrix is unique.
Consider again the example that started this section of a matrix that
reduces to three different echelon form matrices.
The first matrix of the three is the natural echelon form version.
The second matrix is the same as 
the first except that a row has been halved.
The third matrix, too, is just a cosmetic variant of the first. 
The definition of reduced echelon form outlaws this kind of fooling around.
In reduced echelon form,
halving a row is not possible because that would
change the row's leading entry away from one, and
neither is combining rows possible, because then a leading entry would no
longer be alone in its column.

This informal justification is not a proof;
the argument shows that no two different reduced echelon form matrices
are related by a single row operation step, but the argument does not
ruled out the possibility that two different reduced echelon form
matrices could be related by multiple steps.
Before we go to the proof, we finish this subsection by 
rephrasing our work in a terminology that will be enlightening.

Many different matrices yield the same reduced echelon
form matrix.
The three echelon form matrices from the start of this section, and
the matrix they were derived from, all give this reduced echelon form
matrix.
\begin{equation*}
  \begin{pmatrix}
    1  &0  \\
    0  &1
  \end{pmatrix}
\end{equation*}
We think of these matrices as related to each other.
The next result speaks to this relationship.

\begin{lemma} \label{le:RowOpsRev}
Elementary row operations are reversible.
\end{lemma}

\begin{proof}
For any matrix \( A \),
the effect of swapping rows is reversed by swapping them back,
multiplying a row by a nonzero \( k \) is undone by multiplying by
$1/k$,
and adding a multiple of row \( i \) to row \( j \) (with $i\neq j$)
is undone by subtracting the same multiple of row \( i \) from row \( j \).
\begin{equation*}
      A
     \grstep{\rho_i\leftrightarrow\rho_j}
     \;\grstep{\rho_j\leftrightarrow\rho_i}
      A
  \qquad
        A
     \grstep{k\rho_i}
     \;\grstep{(1/k)\rho_i}
      A
  \qquad
        A
     \grstep{k\rho_i+\rho_j}
     \;\grstep{-k\rho_i+\rho_j}
      A                          
\end{equation*}
(The $i\neq j$ conditions is needed.
See \nearbyexercise{exer:INotJMakesRowOpsRev}.)
\end{proof}

This lemma suggests that `reduces to' is misleading\Dash where
\( A\longrightarrow B \), we shouldn't think of \( B \) as
``after'' \( A \) or ``simpler than'' $A$.
Instead we should think of them as interreducible or interrelated.
Below is a picture of the idea.
The matrices from the start of this section and their
reduced echelon form version are shown in a cluster.
They are all interreducible;
these relationships are shown also. 
\begin{center}  
  \includegraphics{ch1.28}
\end{center}

We say 
that matrices that reduce to each other are `equivalent with respect
to the relationship of row reducibility'.
The next result verifies this statement using the definition of 
an equivalence.\appendrefs{equivalence relations}

\begin{lemma}
Between matrices, `reduces to' is an equivalence re\-la\-tion.
\end{lemma}

\begin{proof}
We must check the conditions
(i)~reflexivity, that any matrix reduces to itself,
(ii)~symmetry, that if \( A \) reduces to \( B \) then
   \( B \) reduces to \( A \),
and (iii)~transitivity, that if \( A \) reduces to \( B \) and
      \( B \) reduces to \( C \) then \( A \) reduces to
      \( C \).

Reflexivity is easy; any  matrix reduces to itself in zero row operations.

That the relationship is symmetric is \nearbylemma{le:RowOpsRev}\Dash if
\( A \) reduces to \( B \) by some row operations
then also \( B \) reduces to \( A \) by reversing those operations.

For transitivity, suppose that \( A \) reduces to \( B \) and
that \( B \) reduces to \( C \).
Linking the reduction steps from $A \rightarrow\cdots\rightarrow B$
with those from  $B \rightarrow\cdots\rightarrow C$ 
gives a reduction from \( A \) to \( C \).
\end{proof}

\begin{definition}
Two matrices that are interreducible by the elementary row operations
are \definend{row equivalent}.\index{matrix!row equivalence}%
\index{row equivalence}\index{equivalence relation!row equivalence}
\end{definition}

The diagram below shows the collection of all matrices as a box.
Inside that box, each matrix lies in some class.
Matrices are in the same class if and only if they are interreducible.
The classes are disjoint\Dash no matrix is in two distinct classes.
The collection of matrices has been partitioned into 
\definend{row equivalence classes}.\appendrefs{partitions and class representatives}\index{partition!row equivalence classes}

\begin{center}
  \includegraphics{ch1.27}
\end{center}
\noindent One of the classes in this partition is the
cluster of matrices shown above,
expanded to include all of the nonsingular $\nbyn{2}$ matrices. 

The next subsection proves that the reduced echelon form of a matrix is 
unique; that 
every matrix reduces to one and only one reduced echelon form matrix.
Rephrased in terms of the row-equivalence relationship, 
we shall prove that every matrix is 
row equivalent to one and only one reduced echelon form matrix.
In terms of the partition what we shall prove is:~every
equivalence class contains one and only one reduced echelon form matrix.
So each reduced echelon form matrix serves as a representative of its 
class.

After that proof we shall, 
as mentioned in the introduction to this section, have a
way to decide if one matrix can be derived from another by row reduction.
We just apply the Gauss-Jordan procedure to both and see whether
or not they come to the same reduced echelon form.

\begin{exercises}
   \recommended \item 
     Use Gauss-Jordan reduction to solve each system.
     \begin{exparts*}
        \partsitem \(
          \begin{linsys}[t]{2}
               x  &+  &y  &=  &2  \\
               x  &-  &y  &=  &0  
          \end{linsys}   \)
        \partsitem \(
          \begin{linsys}[t]{3}
               x  &   &   &-  &z  &=  &4  \\
              2x  &+  &2y &   &   &=  &1  
          \end{linsys}  \)
        \partsitem  \(
           \begin{linsys}[t]{2}
               3x  &-  &2y  &=  &1  \\
               6x  &+  &y   &=  &1/2 
           \end{linsys}  \)
        \partsitem \(
           \begin{linsys}[t]{3}
              2x  &-  &y  &  &  &= &-1  \\
               x  &+  &3y &- &z &= &5   \\
                  &   &y  &+ &2z&= &5   
           \end{linsys} \)
     \end{exparts*}
     \begin{answer}
       These answers show only the Gauss-Jordan reduction.
       With it, describing the solution set is easy. 
       \begin{exparts}
         \partsitem $\begin{amatrix}{2}
               1  &1  &2  \\
               1  &-1 &0
             \end{amatrix}
             \grstep{-\rho_1+\rho_2}
             \begin{amatrix}{2}
               1  &1  &2  \\
               0  &-2 &-2
             \end{amatrix}                         
             \grstep{-(1/2)\rho_2}
             \begin{amatrix}{2}
               1  &1  &2  \\
               0  &1  &1
             \end{amatrix}                         
             \grstep{-\rho_2+\rho_1}
             \begin{amatrix}{2}
               1  &0  &1  \\
               0  &1  &1
             \end{amatrix}$
         \partsitem $
             \begin{amatrix}{3}
               1  &0  &-1  &4  \\
               2  &2  &0   &1
             \end{amatrix}
             \grstep{-2\rho_1+\rho_2}
             \begin{amatrix}{3}
               1  &0  &-1  &4  \\
               0  &2  &2   &-7
             \end{amatrix}                    
             \grstep{(1/2)\rho_2}
             \begin{amatrix}{3}
               1  &0  &-1  &4  \\
               0  &1  &1   &-7/2
             \end{amatrix}$
         \partsitem 
           \begin{equation*}
             \begin{amatrix}{2}
               3  &-2  &1  \\
               6  &1   &1/2
             \end{amatrix}
             \grstep{-2\rho_1+\rho_2}
             \begin{amatrix}{2}
               3  &-2  &1  \\
               0  &5   &-3/2
             \end{amatrix}                       
             \grstep[(1/5)\rho_2]{(1/3)\rho_1}
             \begin{amatrix}{2}
               1  &-2/3&1/3 \\
               0  &1   &-3/10
             \end{amatrix}                       
             \grstep{(2/3)\rho_2+\rho_1}
             \begin{amatrix}{2}
               1  &0   &2/15 \\
               0  &1   &-3/10
             \end{amatrix}
          \end{equation*}
        \partsitem A row swap here makes the arithmetic easier.
         \begin{multline*}
          \begin{amatrix}{3}
            2  &-1  &0  &-1  \\
            1  &3   &-1 &5   \\
            0  &1   &2  &5
          \end{amatrix}
          \grstep{-(1/2)\rho_1+\rho_2}
          \begin{amatrix}{3}
            2  &-1  &0  &-1   \\
            0  &7/2 &-1 &11/2 \\
            0  &1   &2  &5
          \end{amatrix}                     
          \grstep{\rho_2\leftrightarrow\rho_3}
          \begin{amatrix}{3}
            2  &-1  &0  &-1   \\
            0  &1   &2  &5    \\
            0  &7/2 &-1 &11/2
          \end{amatrix}                   \\                 
          \begin{aligned}
            &\grstep{-(7/2)\rho_2+\rho_3}
            \begin{amatrix}{3}
              2  &-1  &0  &-1   \\
              0  &1   &2  &5    \\
              0  &0   &-8 &-12
            \end{amatrix}                     
            \grstep[-(1/8)\rho_2]{(1/2)\rho_1}
            \begin{amatrix}{3}
              1  &-1/2&0  &-1/2 \\
              0  &1   &2  &5    \\
              0  &0   &1  &3/2
            \end{amatrix}                     \\                     
            &\grstep{-2\rho_3+\rho_2}
            \begin{amatrix}{3}
              1  &-1/2&0  &-1/2 \\
              0  &1   &0  &2    \\
              0  &0   &1  &3/2
            \end{amatrix}                     
            \grstep{(1/2)\rho_2+\rho_1}
            \begin{amatrix}{3}
              1  &0   &0  &1/2  \\
              0  &1   &0  &2    \\
              0  &0   &1  &3/2
            \end{amatrix}
          \end{aligned}
        \end{multline*}
       \end{exparts}  
     \end{answer}
  \recommended \item 
    Find the reduced echelon form of each matrix.
    \begin{exparts*}
      \partsitem \( \begin{pmatrix}
          2  &1  \\
          1  &3
        \end{pmatrix}  \)
      \partsitem \( \begin{pmatrix}
          1  &3  &1  \\
          2  &0  &4  \\
         -1  &-3 &-3
        \end{pmatrix}  \)
      \partsitem \( \begin{pmatrix}
          1  &0  &3  &1  &2  \\
          1  &4  &2  &1  &5  \\
          3  &4  &8  &1  &2
        \end{pmatrix}  \)
      \partsitem \( \begin{pmatrix}
          0  &1  &3  &2  \\
          0  &0  &5  &6  \\
          1  &5  &1  &5
        \end{pmatrix}  \)
    \end{exparts*}
    \begin{answer}
      Use Gauss-Jordan reduction.
      \begin{exparts}
        \partsitem $
            \grstep{-(1/2)\rho_1+\rho_2}
            \begin{pmatrix}
              2  &1  \\
              0  &5/2
            \end{pmatrix}
            \grstep[(2/5)\rho_2]{(1/2)\rho_1}
            \begin{pmatrix}
              1  &1/2\\
              0  &1
            \end{pmatrix}                      
            \grstep{-(1/2)\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  \\
              0  &1
            \end{pmatrix}$
        \partsitem $
            \grstep[\rho_1+\rho_3]{-2\rho_1+\rho_2}
            \begin{pmatrix}
              1  &3  &1  \\
              0  &-6 &2  \\
              0  &0  &-2
            \end{pmatrix}
            \grstep[-(1/2)\rho_3]{-(1/6)\rho_2}
            \begin{pmatrix}
              1  &3  &1     \\
              0  &1  &-1/3  \\
              0  &0  &1
            \end{pmatrix}                  
            \grstep[-\rho_3+\rho_1]{(1/3)\rho_3+\rho_2}
            \begin{pmatrix}
              1  &3  &0     \\
              0  &1  &0     \\
              0  &0  &1
            \end{pmatrix}                  
            \grstep{-3\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  &0     \\
              0  &1  &0     \\
              0  &0  &1
            \end{pmatrix}$
        \partsitem \ \begin{multline*}
            \grstep[-3\rho_1+\rho_3]{-\rho_1+\rho_2}
            \begin{pmatrix}
              1  &0  &3  &1  &2  \\
              0  &4  &-1 &0  &3  \\
              0  &4  &-1 &-2 &-4
            \end{pmatrix}
            \grstep{-\rho_2+\rho_3}
            \begin{pmatrix}
              1  &0  &3  &1  &2  \\
              0  &4  &-1 &0  &3  \\
              0  &0  &0  &-2 &-7
            \end{pmatrix}                           \\
            \grstep[-(1/2)\rho_3]{(1/4)\rho_2}
            \begin{pmatrix}
              1  &0  &3    &1  &2  \\
              0  &1  &-1/4 &0  &3/4  \\
              0  &0  &0    &1  &7/2
            \end{pmatrix}                           
            \grstep{-\rho_3+\rho_1}
            \begin{pmatrix}
              1  &0  &3    &0  &-3/2  \\
              0  &1  &-1/4 &0  &3/4     \\
              0  &0  &0    &1  &7/2
            \end{pmatrix}
          \end{multline*}
        \partsitem \ \begin{multline*}
            \grstep{\rho_1\leftrightarrow\rho_3}
            \begin{pmatrix}
              1  &5  &1  &5  \\
              0  &0  &5  &6  \\
              0  &1  &3  &2
            \end{pmatrix}
            \grstep{\rho_2\leftrightarrow\rho_3}
            \begin{pmatrix}
              1  &5  &1  &5  \\
              0  &1  &3  &2  \\
              0  &0  &5  &6
            \end{pmatrix}                  
            \grstep{(1/5)\rho_3}
            \begin{pmatrix}
              1  &5  &1  &5  \\
              0  &1  &3  &2  \\
              0  &0  &1  &6/5
            \end{pmatrix}                  \\
            \grstep[-\rho_3+\rho_1]{-3\rho_3+\rho_2}
            \begin{pmatrix}
              1  &5  &0  &19/5  \\
              0  &1  &0  &-8/5  \\
              0  &0  &1  &6/5
            \end{pmatrix}                  
            \grstep{-5\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  &0  &59/5  \\
              0  &1  &0  &-8/5  \\
              0  &0  &1  &6/5
            \end{pmatrix}
          \end{multline*}
      \end{exparts}  
    \end{answer}
  \recommended \item 
    Find each solution set by using Gauss-Jordan reduction and
    then reading off the parametrization.
    \begin{exparts*}
      \partsitem \( \begin{linsys}[t]{3}
                  2x  &+  &y  &-  &z  &=  &1  \\
                  4x  &-  &y  &   &   &=  &3  
                  \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                   x  &   &   &-  &z  &   &   &=  &1  \\
                      &   &y  &+  &2z &-  &w  &=  &3  \\
                   x  &+  &2y &+  &3z &-  &w  &=  &7  
                    \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{4}
                   x  &-  &y  &+  &z  &   &   &=  &0  \\
                      &   &y  &   &   &+  &w  &=  &0  \\
                  3x  &-  &2y &+  &3z &+  &w  &=  &0  \\
                      &   &-y &   &   &-  &w  &=  &0  
                  \end{linsys}  \)
      \partsitem \( \begin{linsys}[t]{5}
                   a  &+  &2b &+  &3c &+  &d  &-  &e  &=  &1  \\
                  3a  &-  &b  &+  &c  &+  &d  &+  &e  &=  &3  
                  \end{linsys}  \)
    \end{exparts*}
    \begin{answer}
      For the ``Gauss'' halves, see the answers to Chapter One's
      section~I.2 question
      \nearbyexercise{exer:SlvMatNot}.
      \begin{exparts}
      \partsitem The ``Jordan'' half goes this way.
        \begin{equation*}
          \grstep[-(1/3)\rho_2]{(1/2)\rho_1}
          \begin{amatrix}{3}
            1  &1/2 &-1/2 &1/2  \\
            0  &1   &-2/3 &-1/3
          \end{amatrix}
          \grstep{-(1/2)\rho_2+\rho_1}
          \begin{amatrix}{3}
            1  &0   &-1/6 &2/3  \\
            0  &1   &-2/3 &-1/3
          \end{amatrix}
        \end{equation*}
        The solution set is this
        \begin{equation*}
          \set{\colvec{2/3 \\ -1/3 \\ 0}
               +\colvec{1/6 \\ 2/3 \\ 1}z
              \suchthat z\in\Re}
        \end{equation*}
      \partsitem The second half is
        \begin{equation*}
          \grstep{\rho_3+\rho_2}
          \begin{amatrix}{4}
            1  &0  &-1  &0  &1 \\
            0  &1  &2   &0  &3 \\
            0  &0  &0   &1  &0
          \end{amatrix}
        \end{equation*}
        so the solution is this.
        \begin{equation*}
          \set{\colvec{1 \\ 3 \\ 0 \\ 0}
               +\colvec{1 \\ -2 \\ 1 \\ 0}z
              \suchthat z\in\Re}
        \end{equation*}
      \partsitem This Jordan half
        \begin{equation*}
          \grstep{\rho_2+\rho_1}
          \begin{amatrix}{4}
            1  &0  &1   &1  &0 \\
            0  &1  &0   &1  &0 \\
            0  &0  &0   &0  &0 \\
            0  &0  &0   &0  &0
          \end{amatrix}
        \end{equation*}
        gives 
        \begin{equation*}
          \set{\colvec{0 \\ 0 \\ 0 \\ 0}
               +\colvec{-1 \\ 0 \\ 1 \\ 0}z
               +\colvec{-1 \\ -1 \\ 0 \\ 1}w
              \suchthat z,w\in\Re}
        \end{equation*}
        (of course, the zero vector could be omitted from the description).
      \partsitem The ``Jordan'' half
        \begin{equation*}
          \grstep{-(1/7)\rho_2}
          \begin{amatrix}{5}
            1  &2  &3   &1   &-1   &1  \\
            0  &1  &8/7 &2/7 &-4/7 &0
          \end{amatrix}
          \grstep{-2\rho_2+\rho_1}
          \begin{amatrix}{5}
            1  &0  &5/7 &3/7 &1/7  &1  \\
            0  &1  &8/7 &2/7 &-4/7 &0
          \end{amatrix}
        \end{equation*}
        ends with this solution set.
        \begin{equation*}
          \set{\colvec{1 \\ 0 \\ 0 \\ 0 \\ 0}
               +\colvec{-5/7 \\ -8/7 \\ 1 \\ 0 \\ 0}c
               +\colvec{-3/7 \\ -2/7 \\ 0 \\ 1 \\ 0}d
               +\colvec{-1/7 \\ 4/7 \\ 0 \\ 0 \\ 1}e
              \suchthat c,d,e\in\Re}
        \end{equation*}
    \end{exparts}
   \end{answer}
  \item 
    Give two distinct echelon form versions of this matrix.
    \begin{equation*}
      \begin{pmatrix}
        2  &1  &1  &3  \\
        6  &4  &1  &2  \\
        1  &5  &1  &5
      \end{pmatrix}
    \end{equation*}
    \begin{answer}
      Routine Gauss' method gives one:
      \begin{equation*}
        \grstep[-(1/2)\rho_1+\rho_3]{-3\rho_1+\rho_2}
        \begin{pmatrix}
          2  &1  &1  &3  \\
          0  &1  &-2 &-7 \\
          0  &9/2&1/2&7/2
        \end{pmatrix}
        \grstep{-(9/2)\rho_2+\rho_3}
        \begin{pmatrix}
          2  &1  &1    &3  \\
          0  &1  &-2   &-7 \\
          0  &0  &19/2 &35
        \end{pmatrix}
      \end{equation*}
      and any cosmetic change, like multiplying the bottom row by \( 2 \),
      \begin{equation*}
        \begin{pmatrix}
          2  &1  &1    &3  \\
          0  &1  &-2   &-7 \\
          0  &0  &19   &70
        \end{pmatrix}
      \end{equation*}
      gives another.  
    \end{answer}
  \recommended \item \label{exer:PossRedEchFrms} 
    List the reduced echelon forms possible for each size.
    \begin{exparts*}
      \partsitem \( \nbyn{2} \)
      \partsitem \( \nbym{2}{3} \)
      \partsitem \( \nbym{3}{2} \)
      \partsitem \( \nbyn{3} \)
    \end{exparts*}
    \begin{answer}
      In the cases listed below, we take $a,b\in\Re$.
      Thus, some canonical forms 
      listed below actually include infinitely many cases.
      In particular, they includes the cases $a=0$ and $b=0$.
      \begin{exparts}
        \partsitem 
          $\begin{pmatrix}
            0  &0  \\
            0  &0
          \end{pmatrix}$,
          $\begin{pmatrix}
            1  &a  \\
            0  &0
          \end{pmatrix}$, 
          $\begin{pmatrix}
            0  &1  \\
            0  &0
          \end{pmatrix}$, 
          $\begin{pmatrix}
            1  &0  \\
            0  &1
          \end{pmatrix}$
        \partsitem
          $\begin{pmatrix}
               0  &0  &0  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &a  &b  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               0  &1  &a  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               0  &0  &1  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &0  &a  \\
               0  &1  &b
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &a  &0  \\
               0  &0  &1
             \end{pmatrix}$,
          $\begin{pmatrix}
               0  &1  &0  \\
               0  &0  &1
             \end{pmatrix}$
        \partsitem
          $\begin{pmatrix}
               0  &0  \\
               0  &0  \\
               0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &a  \\
               0  &0  \\
               0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               0  &1  \\
               0  &0  \\
               0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &0  \\
               0  &1  \\
               0  &0
             \end{pmatrix}$
        \partsitem
          $\begin{pmatrix}
               0  &0  &0  \\
               0  &0  &0  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &a  &b  \\
               0  &0  &0  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               0  &1  &a  \\
               0  &0  &0  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               0  &0  &1  \\
               0  &0  &0  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &0  &a  \\
               0  &1  &b  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &a  &0  \\
               0  &0  &1  \\
               0  &0  &0
             \end{pmatrix}$,
          $\begin{pmatrix}
               1  &0  &0  \\
               0  &1  &0  \\
               0  &0  &1
             \end{pmatrix}$
      \end{exparts}  
    \end{answer}
  \recommended \item  
    What results from applying Gauss-Jordan reduction to a
    nonsingular matrix?
    \begin{answer}
      A nonsingular homogeneous linear system has a unique solution.
      So a nonsingular matrix must reduce to a (square) 
      matrix that is all \( 0 \)'s
      except for \( 1 \)'s down the upper-left to lower-right diagonal, e.g.,
      \begin{equation*}
         \begin{pmatrix}
           1  &0  \\
           0  &1  \\
         \end{pmatrix},
         \quad\text{or}\quad
         \begin{pmatrix}
           1  &0  &0  \\
           0  &1  &0  \\
           0  &0  &1
         \end{pmatrix},
         \quad\text{etc.}
      \end{equation*}  
    \end{answer}
 \item \cite{Cleary}
    Consider the following relationship on the set of $\nbyn{2}$ matrices:  
    we say that $A$ is \textit{sum-what like} $B$ if the sum of all of 
    the entries in $A$ is the same as the sum of all the entries in $B$.  
    For instance, the zero matrix would be sum-what like the matrix 
    whose first row had two sevens, and whose second row had two 
    negative sevens.
    Prove or disprove that this is an equivalence relation on the set 
    of $\nbyn{2}$ matrices.
    \begin{answer}
      It is an equivalence relation.
      To prove that we must check that the relation 
      is reflexive, symmetric, and transitive.

      Assume that all matrices are $\nbyn{2}$.
      For reflexive, we note that a matrix has the same sum of entries as
      itself.
      For symmetric, we assume $A$ has the same sum of entries as~$B$ 
      and obviously then $B$ has the same sum of entries as~$A$.
      Transitivity is no harder\Dash if $A$ has the same sum of entries
      as $B$ and $B$ has the same sum of entries as $C$ then clearly
      $A$ has the same as $C$.
    \end{answer}
 \item \cite{Cleary}
  Consider the set of students in a class.  
  Which of the following relationships are equivalence relations?  
  Explain each answer in at least a sentence.
  \begin{exparts}
    \item  Two students $x$ and $y$ are related 
      if $x$ has taken at least as many 
      math classes as $y$.
    \item Students $x$ and $y$ are related if $x$ and $y$ have names 
      that start with the same letter.
  \end{exparts}
  \begin{answer}
    To be an equivalence, each relation must be reflexive, symmetric, and
    trasitive.
    \begin{exparts}
      \item This relation 
        is not symmetric because if $x$ has taken $4$~classes and $y$
        has taken $3$ then $x$ is related to $y$ but $y$ is not related
        to $x$.
      \item This is reflexive because $x$'s name starts with the same
        letter as does $x$'s.
        It is symmetric because if $x$'s name starts with the same letter 
        as $y$'s then $y$'s starts with the same letter as does~$x$'s.
        And it is transitive because if $x$'s name starts with the same letter
        as does~$y$'s and $y$'s name starts with the same letter as 
        does $z$'s then $x$'s starts with the same letter as does $z$'s.
        So it is an equivalence.
    \end{exparts}
  \end{answer}
 \item \label{exer:INotJMakesRowOpsRev}
   The proof of \nearbylemma{le:RowOpsRev} contains a reference to the 
   $i\neq j$ condition on the row combination operation.
   \begin{exparts}
     \partsitem The definition of row operations has an $i\neq j$ condition on
        the swap operation $\rho_i\leftrightarrow\rho_j$. 
        Show that in 
        $A\grstep{\rho_i\leftrightarrow\rho_j}\;
          \grstep{\rho_i\leftrightarrow\rho_j}A$
        this condition is not needed.
     \partsitem Write down a $\nbyn{2}$ matrix with nonzero entries,
        and show that the $-1\cdot\rho_1+\rho_1$ operation is not
        reversed by $1\cdot\rho_1+\rho_1$.
     \partsitem Expand the proof of that lemma to make explicit exactly where 
        the $i\neq j$ condition on combining is used.
   \end{exparts}
   \begin{answer}
    \begin{exparts}
      \partsitem The $\rho_i\leftrightarrow\rho_i$ operation does not
        change $A$.
      \partsitem For instance,
        \begin{equation*}
          \begin{pmatrix}
            1  &2  \\
            3  &4  
          \end{pmatrix}
          \grstep{-\rho_1+\rho_1}
          \begin{pmatrix}
            0  &0  \\
            3  &4  
          \end{pmatrix}
          \grstep{\rho_1+\rho_1}
          \begin{pmatrix}
            0  &0  \\
            3  &4  
          \end{pmatrix}
        \end{equation*}
        leaves the matrix changed.
      \partsitem If $i\neq j$ then
        \begin{eqnarray*}
          \begin{pmatrix}
            \vdots                     \\
            a_{i,1}  &\cdots  &a_{i,n}  \\
            \vdots                     \\
            a_{j,1}  &\cdots  &a_{j,n}  \\
            \vdots                     
          \end{pmatrix}
          &\grstep{k\rho_i+\rho_j}
          &\begin{pmatrix}
            \vdots                                      \\
            a_{i,1}           &\cdots  &a_{i,n}          \\
            \vdots                                      \\
            ka_{i,1}+a_{j,1}  &\cdots  &ka_{i,n}+a_{j,n}  \\
            \vdots                     
          \end{pmatrix}                                        \\
          &\grstep{-k\rho_i+\rho_j}
          &\begin{pmatrix}
            \vdots                                      \\
            a_{i,1}           &\cdots  &a_{i,n}          \\
            \vdots                                      \\
            -ka_{i,1}+ka_{i,1}+a_{j,1}  &\cdots &-ka_{i,n}+ka_{i,n}+a_{j,n} \\
            \vdots                     
          \end{pmatrix}
        \end{eqnarray*}
        does indeed give $A$ back.
        (Of course, if $i=j$ then the third matrix would have entries of the 
        form $-k(ka_{i,j}+a_{i,j})+ka_{i,j}+a_{i,j}$.)
    \end{exparts}
   \end{answer}
\end{exercises}




















\subsection{Row Equivalence}
We will close this section and this chapter by proving 
that every matrix is row equivalent to one
and only one reduced echelon form matrix.
The ideas that appear here will reappear, and be further developed, in the
next chapter.

The underlying theme here is that one way to understand a
mathematical situation is by being able to classify the cases that can happen.
We have met this theme several times already.
We have classified solution sets of linear systems into the no-elements, 
one-element, and infinitely-many elements cases.
We have also classified linear systems with the same number of equations 
as unknowns into the nonsingular and singular cases.
We adopted these classifications because they give us a way to understand
the situations that we were investigating.
Here, where we are investigating row equivalence, we know that the set of all
matrices breaks into the row equivalence classes.
When we finish the proof here, we will have a way to understand each of those
classes\Dash its matrices can be thought of as derived by row operations from the
unique reduced echelon form matrix in that class.

To understand how row operations act to transform one matrix into another,
we consider the effect that they have on the parts of a matrix.
The crucial observation is that row operations combine the rows linearly.

% \begin{definition}
% A \definend{linear combination}\index{linear combination} of 
% \( x_1,\ldots,x_m \)
% is an expression of the form $c_1x_1+c_2x_2+\,\cdots\,+c_mx_m$
% where the \( c \)'s are scalars.
%\end{definition}

% \noindent (We have already used the phrase 
% `linear combination' in this book.
% The meaning is unchanged, but the next result's statement makes
% a more formal definition in order.)

\begin{lemma}[Linear Combination Lemma] \index{Linear Combination Lemma}
A linear combination of linear combinations is a linear combination.
\end{lemma}

\begin{proof}
Given the linear combinations 
$c_{1,1}x_1+\dots+c_{1,n}x_n$ through $c_{m,1}x_1+\dots+c_{m,n}x_n$,
consider a combination of those
\begin{equation*}
  d_1(c_{1,1}x_1+\dots+c_{1,n}x_n)\,+\dots+\,d_m(c_{m,1}x_1+\dots+c_{m,n}x_n)
\end{equation*}
where the $d$'s are scalars along with the $c$'s.
Distributing those $d$'s and regrouping gives
\begin{equation*}
  %&=d_1c_{1,1}x_1+\dots+d_1c_{1,n}x_n\,
  % +d_2c_{2,1}x_1+\dots+\,
  % d_mc_{1,1}x_1+\dots+d_mc_{1,n}x_n         \\
  =(d_1c_{1,1}+\dots+d_mc_{m,1})x_1\,+\dots+\,(d_1c_{1,n}+\dots+d_mc_{m,n})x_n
\end{equation*}
which is a linear combination of the $x$'s.
\end{proof}

In this subsection we will use the convention
that, where a matrix is named with an upper case roman letter,
the matching lower-case greek letter names the rows.
\begin{equation*}
  A=
    \begin{pmatrix}
%      \colorbox{lightergray}{\makebox[2.25cm]{$\alpha_1$}}   \\[.5ex]
%      \colorbox{lightergray}{\makebox[2.25cm]{$\alpha_2$}}  \\[.5ex]
%      \vdots                                                     \\[.5ex]
%      \colorbox{lightergray}{\makebox[2.25cm]{$\alpha_m$}}  
      \makebox[2.25cm]{$\cdots$\ $\alpha_1$\ $\cdots$}   \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\alpha_2$\ $\cdots$}   \\[.5ex]
      \vdots                                                     \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\alpha_m$\ $\cdots$}   
    \end{pmatrix}
  \qquad
  B=
    \begin{pmatrix}
%      \colorbox{lightergray}{\makebox[2.25cm]{$\smash[b]{\beta}_1$}}  \\[.5ex]
%      \colorbox{lightergray}{\makebox[2.25cm]{$\smash[b]{\beta}_2$}}  \\[.5ex]
%      \vdots                                                    \\[.5ex]
%      \colorbox{lightergray}{\makebox[2.25cm]{$\smash[b]{\beta}_m$}}  
      \makebox[2.25cm]{$\cdots$\ $\smash[b]{\beta}_1$\ $\cdots$}  \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\smash[b]{\beta}_2$\ $\cdots$}  \\[.5ex]
      \vdots                                                    \\[.5ex]
      \makebox[2.25cm]{$\cdots$\ $\smash[b]{\beta}_m$\ $\cdots$}  
    \end{pmatrix}
\end{equation*}

\begin{corollary} \label{cor:RowsOfEqMatsLinCombos}
Where one matrix reduces to another, each row of the second
is a linear combination of the rows of the first.
\end{corollary}

The proof below of this lemma
uses induction.
Before we proceed, here is an outline of the argument
(readers unfamiliar with induction may want to compare this argument with the
one used in the 
`$\text{General}=\text{Particular}+\text{Homogeneous}$' 
proof).\appendrefs{mathematical induction} %\spacefactor=1000 

First, for the base step of the argument, we
will verify that the proposition is true when reduction 
can be done in zero row operations.
Second, for the inductive step, we will 
argue that if being able to reduce the first matrix to the second in some
number $t\geq 0$ of operations implies that each row of the second is a linear
combination of the rows of the first, then being able to reduce the first to
the second in $t+1$ operations implies the same thing.

Together, this base step and induction step prove the result because  
by the inductive step the fact that it is true in the zero operations case
(that's shown in the base step)
implies that it is true in the one operation case, and then the inductive step
applied again gives that it is therefore true in the two operations case, etc.

\begin{proof}
We proceed by induction on the minimum number of row operations that take a
first matrix $A$ to a second one $B$.

In the base step, that
zero reduction operations suffice, the two matrices are equal and each 
row of $B$ is obviously a combination of
$A$'s rows: $\vec{\beta}_i
  =0\cdot\vec{\alpha}_1+\dots+1\cdot\vec{\alpha}_i+\dots+0\cdot\vec{\alpha}_m$.

For the inductive step, assume the inductive hypothesis:~with $t\geq 0$,
if a matrix can be derived from \( A \) in \( t \) or fewer operations 
then its rows are linear combinations of the $A$'s rows.
Consider a $B$ that takes $t+1$ operations.
Because there are more than zero operations, 
there must be a next-to-last matrix $G$  
so that $A\longrightarrow\cdots\longrightarrow G\longrightarrow B$.
This \( G \) is only $t$ operations away from \( A \) and so the inductive
hypothesis applies to it, that is, each row of \( G \)
is a linear combination of the rows of \( A \).

If the last operation, the one from \( G \) to \( B \), is a row swap then
the rows of $B$ are just the rows of $G$ reordered and thus each row of $B$
is also a linear combination of the rows of $A$.
The other two possibilities for this last operation, that it multiplies a 
row by a scalar and that it adds a multiple of one row to another, both result
in the rows of $B$ being linear combinations of the rows of $G$.
But therefore, by the Linear Combination Lemma, each row of $B$ is a linear
combination of the rows of $A$.


With that, we have both the base step and the inductive step, and 
so the proposition follows.
\end{proof}

\begin{example}
In the reduction
\begin{equation*}
    \begin{pmatrix}
       0  &2  \\
       1  &1
     \end{pmatrix}
    \grstep{\rho_1\leftrightarrow\rho_2}
    \begin{pmatrix}
       1  &1  \\
       0  &2
     \end{pmatrix}                  
    \grstep{(1/2)\rho_2}
    \begin{pmatrix}
       1  &1  \\
       0  &1
     \end{pmatrix}                   
    \grstep{-\rho_2+\rho_1}
    \begin{pmatrix}
       1  &0  \\
       0  &1
     \end{pmatrix}
\end{equation*}
call the matrices \( A \), \( D \), \( G \), and \( B \).
The methods of the proof show that there are three sets of linear
relationships. 
\begin{equation*}
  \begin{aligned}
     \delta_1 &=0\cdot\alpha_1+1\cdot\alpha_2         \\
     \delta_2 &=1\cdot\alpha_1+0\cdot\alpha_2
  \end{aligned}
  \qquad
  \begin{aligned}
     \gamma_1 &=0\cdot\alpha_1+1\cdot\alpha_2         \\
     \gamma_2 &=(1/2)\alpha_1+0\cdot\alpha_2
  \end{aligned}
  \qquad
  \begin{aligned}
     \beta_1 &=(-1/2)\alpha_1+1\cdot\alpha_2        \\
     \beta_2 &=(1/2)\alpha_1+0\cdot\alpha_2
  \end{aligned}
\end{equation*}
\end{example}

The prior result gives us the insight that Gauss' method works by taking
linear combinations of the rows.
But to what end; why do we go to echelon form as a particularly simple, or
basic, version of a linear system?
The answer, of course, is that echelon form is suitable for back substitution,
because we have isolated the variables.
For instance, in this matrix
\begin{equation*}
  R=\begin{pmatrix}
    2  &3  &7  &8  &0  &0  \\
    0  &0  &1  &5  &1  &1  \\
    0  &0  &0  &3  &3  &0  \\
    0  &0  &0  &0  &2  &1
  \end{pmatrix}
\end{equation*}
$x_1$ has been removed from $x_5$'s equation.
That is, Gauss' method has made $x_5$'s row independent of $x_1$'s row.

Independence of a collection of row vectors, or of any kind of vectors, 
will be precisely defined and explored in the next chapter.
But a first take on it is that we can show that, say, the third row above
is not comprised of the other rows, that
$\rho_3\neq c_1\rho_1+c_2\rho_2+c_4\rho_4$.
For, suppose that there are scalars $c_1$, $c_2$, and $c_4$ such that this
relationship holds.
\begin{align*}
  \rowvec{0  &0  &0  &3  &3  &0}
  &=c_1\rowvec{2 &3 &7 &8 &0 &0}             \\
  &\quad\hbox{}+c_2\rowvec{0 &0 &1 &5 &1 &1} \\
  &\quad\hbox{}+c_4\rowvec{0 &0 &0 &0 &2 &1}
\end{align*}
The first row's leading entry is in the first column and narrowing our
consideration of the above relationship to consideration only of the entries
from the first column $0=2c_1+0c_2+0c_4$ gives that $c_1=0$.
The second row's leading entry is in the third column and the equation of
entries in that column $0=7c_1+1c_2+0c_4$, along with the knowledge that
$c_1=0$, gives that $c_2=0$.
Now, to finish, the third row's leading entry is in the fourth column and the
equation of entries in that column $3=8c_1+5c_2+0c_4$, along with $c_1=0$ and
$c_2=0$, gives an impossibility.

The following result shows that this effect always holds.
It shows that what Gauss' linear elimination method eliminates is linear
relationships among the rows.

\begin{lemma}      \label{le:EchFormNoLinCombo}
In an echelon form matrix,
no nonzero row is a linear combination of the other rows.
\end{lemma}

\begin{proof}
Let $R$ be in echelon form.
Suppose, to obtain a contradiction, that some nonzero row is a linear
combination of the others.
\begin{equation*}
   \rho_i=c_1\rho_1+\ldots+c_{i-1}\rho_{i-1}+
               c_{i+1}\rho_{i+1}+\ldots+c_m\rho_m
\end{equation*}
We will first use induction to show that the coefficients 
$c_1$, \ldots, $c_{i-1}$ associated with rows above $\rho_i$ are all zero.
The contradiction will come from consideration of $\rho_i$ and the rows below
it.

The base step of the induction argument 
is to show that the first coefficient $c_1$ is zero.
Let the first row's leading entry be in column number \( \ell_1 \) 
and consider the equation of entries in that column.
\begin{equation*}
   \rho_{i,\ell_1}=c_1\rho_{1,\ell_1}+\ldots+c_{i-1}\rho_{i-1,\ell_1}
               +c_{i+1}\rho_{i+1,\ell_1}+\ldots+c_m\rho_{m,\ell_1}
\end{equation*}
The matrix is in echelon form so the entries 
$\rho_{2,\ell_1}$, \ldots, $\rho_{m,\ell_1}$, including
$\rho_{i,\ell_1}$, are all zero. 
\begin{equation*}
   0=c_1\rho_{1,\ell_1}+\dots+c_{i-1}\cdot 0
               +c_{i+1}\cdot 0+\dots+c_m\cdot 0
\end{equation*}
Because the entry $\rho_{1,\ell_1}$ is nonzero as it leads its row,
the coefficient $c_1$ must be zero.  

The inductive step is to show that 
for each row index $k$ between $1$ and $i-2$,
if the coefficient $c_1$ and the 
coefficients $c_2$, \ldots, $c_{k}$ are all zero 
then $c_{k+1}$ is also zero.
That argument,
and the contradiction that finishes this proof, is saved for 
\nearbyexercise{ex:EchFormNoLinCombo}.
\end{proof}

\begin{theorem}
\label{th:ReducedEchelonFormIsUnique}
Each matrix is row equivalent to a unique reduced echelon form matrix.
\end{theorem}

\begin{proof} \cite{Yuster}
Fix a number of rows \( m \).
We will proceed by induction on the number of columns \( n \).

The base case is that the matrix has \( n=1 \) column.
If this is the zero matrix then its unique echelon form is the zero matrix. 
If instead it has any nonzero entries then when the matrix is brought to 
reduced echelon form, it must have at least one nonzero entry, so it has a
\( 1 \), in the first row. 
In either case, its reduced echelon form is unique.

For the inductive step we assume that \( n>1 \) and that all \( m \)~row
matrices with fewer than \( n \) columns have a unique reduced echelon form.
Consider a \( \nbym{m}{n} \) matrix \( A \), and suppose that 
\( B \) and \( C \) are two reduced echelon form matrices derived from \( A \).
We will show that these two must be equal.

Let \( \hat{A} \) be the matrix consisting of the first \( n-1 \) columns of
\( A \).
Observe that any sequence of row operations that bring \( A \) to reduced 
echelon form will also bring \( \hat{A} \) to reduced echelon form.
By the inductive hypothesis the reduced echelon form of \( \hat{A} \)
is unique, so if \( B \) and \( C \) differ then the difference must 
occur in their \( n \)-th columns.

We finish by showing that the two cannot differ in only that column.
Consider a homogeneous system of equations for which \( A \) is the
matrix of coefficients.  
\begin{equation*}
  \begin{linsys}{4}
    a_{1,1}x_1  &+  &a_{1,2}x_2  &+  &\cdots  &+  &a_{1,n}x_n  &=  &0  \\
    a_{2,1}x_1  &+  &a_{2,2}x_2  &+  &\cdots  &+  &a_{2,n}x_n  &=  &0  \\
               &\vdots  \\
    a_{m,1}x_1  &+  &a_{m,2}x_2  &+  &\cdots  &+  &a_{m,n}x_n  &=  &0  
  \end{linsys}
  \tag{$*$}
\end{equation*}
By the first theorem of this chapter the set of solution to that system
is the same as the set of solution to this system
\begin{equation*}
  \begin{linsys}{4}
    b_{1,1}x_1  &+  &b_{1,2}x_2  &+  &\cdots  &+  &b_{1,n}x_n  &=  &0  \\
    b_{2,1}x_1  &+  &b_{2,2}x_2  &+  &\cdots  &+  &b_{2,n}x_n  &=  &0  \\
               &\vdots  \\
    b_{m,1}x_1  &+  &b_{m,2}x_2  &+  &\cdots  &+  &b_{m,n}x_n  &=  &0  
  \end{linsys}
  \tag{$**$}
\end{equation*}
and to this one.
\begin{equation*}
  \quad
  \begin{linsys}{4}
    c_{1,1}x_1  &+  &c_{1,2}x_2  &+  &\cdots  &+  &c_{1,n}x_n  &=  &0  \\
    c_{2,1}x_1  &+  &c_{2,2}x_2  &+  &\cdots  &+  &c_{2,n}x_n  &=  &0  \\
               &\vdots  \\
    c_{m,1}x_1  &+  &c_{m,2}x_2  &+  &\cdots  &+  &c_{m,n}x_n  &=  &0  
  \end{linsys}
  \tag{$\mathord{*}\mathord{*}\mathord{*}$}
\end{equation*}
Suppose that \( B  \) and \( C \) differ only in column~\( n \), in row~\( i \).
Subtract row~\( i \) of ($\mathord{*}\mathord{*}\mathord{*}$) from 
row~\( i \) of ($**$).
Since \( B \) and \( C \) agree on their first \( n-1\)~columns the 
result is the equation 
\( (b_{i,n}-c_{i,n})\cdot x_n=0 \).
Because \( b_{i,n}\neq c_{i,n} \) we know that $x_n=0$.
Thus in ($**$) and~($\mathord{*}\mathord{*}\mathord{*}$)
the \( n \)-th column has a leading \( 1 \), or else 
the variable \( x_n \) would be free.
That's a contradiction, because with the first \( n-1 \)~columns of
\( B \) and \( C \) equal, the leading \( 1 \)'s in the 
\( n \)-th column would have to be in the same row.
So \( B=C \).
\end{proof}

That result answers the two questions that we posed in the intrudcution
to this chapter: do any two echelon form versions of a linear system 
have the same number of free variables, and if so are they
exactly the same variables?
We get from any echelon form version to the reduced echelon form by pivoting up,
and so uniqueness of reduced echelon form implies that the same variables 
are free in all echelon form version of a system.
Thus both questions are answered ``yes.''
There is no linear system and no combination of row operations such
that, say, we could solve the system 
one way and get $y$ and $z$ free but solve it
another way and get $y$ and $w$ free, or solve it one way and get two free
variables while solving it another way yields three. 


% %================= existing text

% We can now prove that each matrix is row equivalent to one and only one
% reduced echelon form matrix.
% We will find it convenient to break the first half of the argument off as a
% preliminary lemma.
% For one thing, it holds for any echelon form whatever, not just
% reduced echelon form.

% \begin{lemma}  \label{le:EquivMatsSameForm}
% If two echelon form matrices are row equivalent then the leading entries in
% their first rows lie in the same column.
% The same is true of all the nonzero rows\Dash the leading entries in their 
% second rows lie in the same column, etc.
% \end{lemma}

% For the proof we rephrase the result in more technical terms. 
% Define the \definend{form}\index{form}\index{matrix:form}
% of an $\nbym{m}{n}$~matrix to be the sequence
% $\langle \ell_1,\ell_2,\ldots\,,\ell_m \rangle$
% where $\ell_i$ is the column number of the leading entry in row~$i$
% and $\ell_i=\infty$ if there is no leading entry in that row.
% The lemma says that if two echelon form matrices are row equivalent
% then their forms are equal sequences.

% \begin{proof}
% Let \( B \) and \( D \) be echelon form matrices that are row equivalent.
% Because they are row equivalent they must be the same size, say
% $\nbym{m}{n}$.
% Let the column number of the leading entry in row $i$ of $B$ be $\ell_i$ and
% let the column number of the leading entry in row $j$ of $D$ be $k_j$.
% We will show that $\ell_1=k_1$, that $\ell_2=k_2$, etc., by induction.

% This induction argument relies on the fact that the matrices are row
% equivalent, because the Linear Combination Lemma and its corollary therefore
% give that each row of \( B \) is a linear combination of the rows of \( D \)
% and vice versa:
% \begin{equation*}
%   \beta_i=s_{i,1}\delta_1+s_{i,2}\delta_2+\dots+s_{i,m}\delta_m
%   \quad\text{and}\quad
%   \delta_j=t_{j,1}\beta_1+t_{j,2}\beta_2+\dots+t_{j,m}\beta_m
% \end{equation*}
% where the $s$'s and $t$'s are scalars.

% The base step of the induction is to verify the lemma for the 
% first rows of the matrices, that is, to verify that $\ell_1=k_1$.
% If either row is a zero row then every entry in the matrix is a zero since it
% is in echelon form, and therefore both matrices consist solely of zero entries 
% (by \nearbycorollary{cor:RowsOfEqMatsLinCombos}), and so both
% $\ell_1$ and $k_1$ are $\infty$.
% For the case where neither $\beta_1$ nor $\delta_1$ is a zero row,  
% consider the $i=1$ instance of the linear relationship above.
% \begin{align*}
%   \beta_1 &=s_{1,1}\delta_1+s_{1,2}\delta_2+\dots+s_{1,m}\delta_m  \\
%   \rowvec{0 &\cdots &b_{1,\ell_1} &\cdots &}
%           &=s_{1,1}\rowvec{0 &\cdots &d_{1,k_1} &\cdots &}   \\
%           &\quad\hbox{}+s_{1,2}\rowvec{0 &\cdots &0         &\cdots &}   \\
%           &\quad \alignedvdots                                    \\
%           &\quad\hbox{}+s_{1,m}\rowvec{0 &\cdots &0         &\cdots &}  
% \end{align*}
% First, note that $\ell_1<k_1$ is impossible:~in the columns of $D$ to the left
% of column $k_1$ the entries are all zeroes 
% (as $d_{1,k_1}$ leads the first row) and
% so if $\ell_1<k_1$ then the equation of entries from column~$\ell_1$ would be 
% $b_{1,\ell_1}=s_{1,1}\cdot 0+\dots+s_{1,m}\cdot 0$,
% but $b_{1,\ell_1}$ isn't zero since it leads its row and so this is
% an impossibility.
% Next, a symmetric argument
% shows that $k_1<\ell_1$ also is impossible.
% Thus the $\ell_1=k_1$ base case holds.

% The inductive step is to show that 
% if $\ell_1=k_1$, and
% $\ell_2=k_2$, \ldots, and $\ell_r=k_r$, then also $\ell_{r+1}=k_{r+1}$
% (for $r$ in the interval $1\,..\,m-1$).
% This argument is saved for \nearbyexercise{ex:RowEqEchFormAreSameShape}. 
% \end{proof}

% That lemma answers two of the questions that we have posed: (i)~any
% two echelon form versions of a matrix have the same free variables,
% and consequently, and (ii)~any two echelon form versions have the same number 
% of free variables.
% There is no linear system and no combination of row operations such
% that, say, we could solve the system 
% one way and get $y$ and $z$ free but solve it
% another way and get $y$ and $w$ free, or solve it one way and get two free
% variables while solving it another way yields three. 

% We finish now by specializing to the case of reduced echelon form matrices.

% \begin{theorem}
% \label{th:ReducedEchelonFormIsUnique}
% Each matrix is row equivalent to a unique reduced echelon form matrix.
% \end{theorem}

% \begin{proof}
% Clearly any matrix is row equivalent to at least 
% one reduced echelon form matrix, via Gauss-Jordan reduction.
% For the other half, that any matrix is equivalent to at most one 
% reduced echelon form matrix, we will show that
% if a matrix Gauss-Jordan reduces to each of two others then those
% two are equal.

% Suppose that a matrix is row equivalent to two reduced echelon form matrices
% \( B \) and \( D \), which are therefore row equivalent to each other.
% The Linear Combination Lemma and its corollary allow us to
% write the rows of one, say \( B \), as a linear combination 
% of the rows of the other
% $\beta_i=c_{i,1}\delta_1+\cdots+c_{i,m}\delta_m$.
% The preliminary result, \nearbylemma{le:EquivMatsSameForm}, says that
% in the two matrices, the same collection of rows are nonzero.
% Thus, if $\beta_1$ through $\beta_r$ are the nonzero rows of $B$ then 
% the nonzero rows of $D$ are $\delta_1$ through $\delta_r$.
% Zero rows don't contribute to the sum so we can rewrite the relationship
% to include just the nonzero rows.
% \begin{equation*}
%    \beta_i =c_{i,1}\delta_1+\dots+c_{i,r}\delta_r        
% \tag*{($*$)}\end{equation*}

% The preliminary result also says that for  
% each row \( j \) between $1$ and $r$, the leading entries 
% of the $j$-th row of $B$ and $D$ appear in the same column, 
% denoted \( \ell_j \).
% Rewriting the above relationship to focus on the entries in the
% $\ell_j$-th column 
% \begin{align*}
%    \rowvec{ &\cdots &b_{i,\ell_j} &\cdots &}
%        &=c_{i,1}\rowvec{ &\cdots &d_{1,\ell_j} &\cdots &} \\
%        &\quad\hbox{}+c_{i,2}\rowvec{ &\cdots 
%             &d_{2,\ell_j} &\cdots &}                             \\
%        &\quad\alignedvdots                                              \\
%        &\quad\hbox{}+c_{i,r}\rowvec{ &\cdots 
%             &d_{r,\ell_j} &\cdots &}
% \end{align*}
% gives this set of equations for $i=1$ up to $i=r$.
% \begin{align*}
%    b_{1,\ell_j} &=c_{1,1}d_{1,\ell_j}
%                      +\cdots+c_{1,j}d_{j,\ell_j}+\cdots
%                      +c_{1,r}d_{r,\ell_j}                 \\
%                 &\alignedvdots                            \\
%    b_{j,\ell_j} &=c_{j,1}d_{1,\ell_j}
%                      +\cdots+c_{j,j}d_{j,\ell_j}+\cdots
%                      +c_{j,r}d_{r,\ell_j}                 \\
%                 &\alignedvdots                            \\
%    b_{r,\ell_j} &=c_{r,1}d_{1,\ell_j}
%                      +\cdots+c_{r,j}d_{j,\ell_j}+\cdots
%                      +c_{r,r}d_{r,\ell_j}
% \end{align*}
% Since \(D\) is in reduced echelon form,
% all of the \( d \)'s in column $\ell_j$ are zero except 
% for \( d_{j,\ell_j} \), which is $1$.
% Thus each equation above simplifies to
% $b_{i,\ell_j}=c_{i,j}d_{j,\ell_j}=c_{i,j}\cdot 1$.
% But $B$ is also in reduced echelon form and so all of the $b$'s 
% in column $\ell_j$ are zero
% except for $b_{j,\ell_j}$, which is $1$.
% Therefore, each $c_{i,j}$ is zero, except that 
% \( c_{1,1}=1 \), and $c_{2,2}=1$, \ldots, and $c_{r,r}=1$.
 
% We have shown that the only nonzero coefficient
% in the linear combination labelled ($*$) is \( c_{j,j}  \), which is \( 1 \).
% Therefore $\beta_j=\delta_j$.
% Because this holds for all nonzero rows, $B=D$. 
% \end{proof}

We end with a recap.
In Gauss' method we start with a matrix and then
derive a sequence of other matrices.
We defined two matrices to be related if one can be derived from the other.
That relation is an equivalence relation, %\appendrefs{equivalence relation} 
called row equivalence, and
so partitions the set of all matrices into row equivalence classes.
\begin{center}
  \includegraphics{ch1.30}
\end{center}
(There are infinitely many matrices in the pictured class, but we've only
got room to show two.)
We have proved there is one and only one reduced echelon form matrix in
each row equivalence class.
So the reduced echelon form is a
{\em canonical form}\appendrefs{canonical representatives}%
\index{canonical form!for row equivalence}%
\index{representative!for row equivalence classes}%
for row equivalence:
the reduced echelon form matrices are
representatives of the classes.
\begin{center}
  \includegraphics{ch1.31}
\end{center}
We can answer questions about the classes by translating them
into questions about the representatives.

\begin{example}  \label{ex:MatsNotRowEq}
We can decide if matrices are interreducible
by seeing if Gauss-Jordan reduction produces the same
reduced echelon form result.
Thus, these are not row equivalent
\begin{equation*}
  \begin{pmatrix}
    1  &-3  \\
   -2  &6
  \end{pmatrix}
  \qquad
  \begin{pmatrix}
    1  &-3  \\
   -2  &5
  \end{pmatrix}
\end{equation*}
because their reduced echelon forms are not equal.
\begin{equation*}
  \begin{pmatrix}
    1  &-3  \\
    0  &0
  \end{pmatrix}
  \qquad
  \begin{pmatrix}
    1  &0   \\
    0  &1
  \end{pmatrix}
\end{equation*}
\end{example}

\begin{example}
Any nonsingular \( \nbyn{3} \) matrix Gauss-Jordan reduces to this.
\begin{equation*}
    \begin{pmatrix}
      1  &0  &0 \\
      0  &1  &0 \\
      0  &0  &1
    \end{pmatrix}
\end{equation*}
\end{example}

\begin{example} \label{ex:RowEqClassTwoTwoMats}
We can describe the classes by listing all possible
reduced echelon form matrices.
Any $\nbyn{2}$ matrix lies in one of these:~the class of matrices
row equivalent to this,
\begin{equation*}
  \begin{pmatrix}
     0  &0  \\
     0  &0
  \end{pmatrix}
\end{equation*}
the infinitely many classes of matrices row equivalent to one of this type
\begin{equation*}
  \begin{pmatrix}
     1  &a  \\
     0  &0
  \end{pmatrix}
\end{equation*}
where \( a\in\Re \) (including $a=0$),
the class of matrices row equivalent to this,
\begin{equation*}
  \begin{pmatrix}
     0  &1  \\
     0  &0
  \end{pmatrix}
\end{equation*}
and the class of matrices row equivalent to this
\begin{equation*}
  \begin{pmatrix}
     1  &0  \\
     0  &1
  \end{pmatrix}
\end{equation*}
(this is the class of nonsingular $\nbyn{2}$ matrices).
\end{example}



\begin{exercises}
  \recommended \item 
    Decide if the matrices are row equivalent.
    \begin{exparts*}
       \partsitem \(
           \begin{pmatrix}
             1  &2  \\
             4  &8
           \end{pmatrix}, 
           \begin{pmatrix}
             0  &1  \\
             1  &2
           \end{pmatrix} \)
       \partsitem \(
           \begin{pmatrix}
             1  &0  &2  \\
             3  &-1 &1  \\
             5  &-1 &5
           \end{pmatrix},  
           \begin{pmatrix}
             1  &0  &2  \\
             0  &2  &10 \\
             2  &0  &4
           \end{pmatrix} \)
       \partsitem \(
           \begin{pmatrix}
             2  &1  &-1 \\
             1  &1  &0  \\
             4  &3  &-1
           \end{pmatrix},  
           \begin{pmatrix}
             1  &0  &2  \\
             0  &2  &10 \\
           \end{pmatrix} \)
       \partsitem \(
           \begin{pmatrix}
             1  &1  &1  \\
            -1  &2  &2
           \end{pmatrix},  
           \begin{pmatrix}
             0  &3  &-1 \\
             2  &2  &5
           \end{pmatrix} \)
       \partsitem \(
           \begin{pmatrix}
             1  &1  &1  \\
             0  &0  &3
           \end{pmatrix},  
           \begin{pmatrix}
             0  &1  &2  \\
             1  &-1 &1
           \end{pmatrix} \)
    \end{exparts*}
    \begin{answer}
      Bring each to reduced echelon form and compare.
      \begin{exparts}
        \partsitem The first gives
          \begin{equation*}
            \grstep{-4\rho_1+\rho_2}
            \begin{pmatrix}
              1  &2  \\
              0  &0
            \end{pmatrix}
          \end{equation*}
          while the second gives
          \begin{equation*}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \begin{pmatrix}
              1  &2  \\
              0  &1
            \end{pmatrix}
            \grstep{-2\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  \\
              0  &1
            \end{pmatrix}
          \end{equation*}
          The two reduced echelon form matrices are not identical, and so the
          original matrices are not row equivalent.
        \partsitem The first is this.
          \begin{equation*}
            \grstep[-5\rho_1+\rho_3]{-3\rho_1+\rho_2}
            \begin{pmatrix}
              1  &0  &2  \\
              0  &-1 &-5 \\
              0  &-1 &-5
            \end{pmatrix}
            \grstep{-\rho_2+\rho_3}
            \begin{pmatrix}
              1  &0  &2  \\
              0  &-1 &-5 \\
              0  &0  &0
            \end{pmatrix}
            \grstep{-\rho_2}
            \begin{pmatrix}
              1  &0  &2  \\
              0  &1  &5  \\
              0  &0  &0
            \end{pmatrix}
          \end{equation*}
          The second is this.
          \begin{equation*}
            \grstep{-2\rho_1+\rho_3}
            \begin{pmatrix}
              1  &0  &2  \\
              0  &2  &10 \\
              0  &0  &0
            \end{pmatrix}
            \grstep{(1/2)\rho_2}
            \begin{pmatrix}
              1  &0  &2  \\
              0  &1  &5  \\
              0  &0  &0
            \end{pmatrix}
          \end{equation*}
          These two are row equivalent.
        \partsitem These two are not row equivalent because they have different
          sizes.
        \partsitem The first,
          \begin{equation*}
            \grstep{\rho_1+\rho_2}
            \begin{pmatrix}
              1  &1  &1  \\
              0  &3  &3
            \end{pmatrix}
            \grstep{(1/3)\rho_2}
            \begin{pmatrix}
              1  &1  &1  \\
              0  &1  &1
            \end{pmatrix}
            \grstep{-\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  &0  \\
              0  &1  &1
            \end{pmatrix}
          \end{equation*}
          and the second.
          \begin{equation*}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \begin{pmatrix}
              2  &2  &5  \\
              0  &3  &-1
            \end{pmatrix}
            \grstep[(1/3)\rho_2]{(1/2)\rho_1}
            \begin{pmatrix}
              1  &1  &5/2 \\
              0  &1  &-1/3
            \end{pmatrix}
            \grstep{-\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  &17/6 \\
              0  &1  &-1/3
            \end{pmatrix}
          \end{equation*}
          These are not row equivalent.
        \partsitem Here the first is
          \begin{equation*}
            \grstep{(1/3)\rho_2}
            \begin{pmatrix}
              1  &1  &1  \\
              0  &0  &1
            \end{pmatrix}
            \grstep{-\rho_2+\rho_1}
            \begin{pmatrix}
              1  &1  &0  \\
              0  &0  &1
            \end{pmatrix}
          \end{equation*}
          while this is the second.
          \begin{equation*}
            \grstep{\rho_1\leftrightarrow\rho_2}
            \begin{pmatrix}
              1  &-1 &1  \\
              0  &1  &2
            \end{pmatrix}
            \grstep{\rho_2+\rho_1}
            \begin{pmatrix}
              1  &0  &3  \\
              0  &1  &2
            \end{pmatrix}
          \end{equation*}
          These are not row equivalent.
       \end{exparts}  
     \end{answer}
  \item 
     Describe the matrices in each of the classes represented in
     \nearbyexample{ex:RowEqClassTwoTwoMats}.
     \begin{answer}
       First, the only matrix row equivalent to the matrix of all
       \( 0 \)'s is itself (since row operations have no effect).

       Second, the matrices that reduce to 
       \begin{equation*}
         \begin{pmatrix}
           1  &a  \\
           0  &0
         \end{pmatrix}
       \end{equation*}
       have the form
       \begin{equation*}
         \begin{pmatrix}
           b  &ba \\
           c  &ca
         \end{pmatrix}
       \end{equation*}
       (where \( a,b,c\in\Re \), and \(b\) and \(c\) are not both zero).  

       Next, the matrices that reduce to 
       \begin{equation*}
         \begin{pmatrix}
           0  &1  \\
           0  &0
         \end{pmatrix}
       \end{equation*}
       have the form
       \begin{equation*}
         \begin{pmatrix}
           0  &a \\
           0  &b
         \end{pmatrix}
       \end{equation*}
       (where \( a,b\in\Re \), and not both are zero).  

       Finally, the matrices that reduce to 
       \begin{equation*}
         \begin{pmatrix}
           1  &0  \\
           0  &1
         \end{pmatrix}
       \end{equation*}
       are the nonsingular matrices.
       That's because a linear system for which this is the matrix of
       coefficients will have a unique solution, and that is the definition
       of nonsingular.
       (Another way to say the same thing is to say that they fall into none
       of the above classes.)
     \end{answer}
  \item 
    Describe all matrices in the row equivalence class of
    these.
    \begin{exparts*}
       \partsitem  \(
           \begin{pmatrix}
             1  &0  \\
             0  &0
           \end{pmatrix}  \)
       \partsitem  \(
           \begin{pmatrix}
             1  &2      \\
             2  &4
           \end{pmatrix} \)
       \partsitem \(
           \begin{pmatrix}
             1  &1      \\
             1  &3
           \end{pmatrix} \)
    \end{exparts*}
    \begin{answer}
      \begin{exparts}
        \partsitem They have the form
          \begin{equation*}
            \begin{pmatrix}
              a  &0  \\
              b  &0
            \end{pmatrix}
          \end{equation*}
          where \( a,b\in\Re \).
        \partsitem They have this form (for \( a,b\in\Re \)).
          \begin{equation*}
            \begin{pmatrix}
             1a  &2a \\
             1b  &2b
            \end{pmatrix}
          \end{equation*}
        \partsitem They have the form
          \begin{equation*}
            \begin{pmatrix}
              a  &b  \\
              c  &d
            \end{pmatrix}
          \end{equation*}
          (for \( a,b,c,d\in\Re \)) where \( ad-bc\neq 0 \).
          (This is the formula that determines when a \( \nbyn{2} \) matrix
          is nonsingular.)
      \end{exparts}  
    \end{answer}
  \item 
    How many row equivalence classes are there?
    \begin{answer}
       Infinitely many.
       For instance, in 
       \begin{equation*}
         \begin{pmatrix}
           1  &k  \\
           0  &0
         \end{pmatrix}
       \end{equation*}
       each $k\in\Re$ gives a different class.  
    \end{answer}
  \item 
    Can row equivalence classes contain different-sized matrices?
    \begin{answer}
      No.
      Row operations do not change the size of a matrix.  
    \end{answer}
  \item 
    How big are the row equivalence classes?
    \begin{exparts} 
      \partsitem Show that for any matrix of all zeros, the class is finite.
      \partsitem Do any other classes contain only finitely many members?
    \end{exparts}
    \begin{answer}
     \begin{exparts}
      \partsitem A row operation on a matrix of zeros has no effect.
        Thus each such matrix is alone in its row equivalence class.  
      \partsitem No.
        Any nonzero entry can be rescaled.
     \end{exparts}
    \end{answer}
  \recommended \item 
    Give two reduced echelon form matrices that have their leading
    entries in the same columns,
    but that are not row equivalent.
    \begin{answer}
      Here are two.
      \begin{equation*}
        \begin{pmatrix}
          1  &1  &0  \\
          0  &0  &1
        \end{pmatrix}
        \quad\text{and}\quad
        \begin{pmatrix}
          1  &0  &0  \\
          0  &0  &1
        \end{pmatrix}
      \end{equation*}  
     \end{answer}
  \recommended \item 
    Show that any two \( \nbyn{n} \) nonsingular matrices are
    row equivalent.
    Are any two singular matrices row equivalent?
    \begin{answer}
      Any two \( \nbyn{n} \) nonsingular matrices have
      the same reduced echelon
      form, namely the matrix with all \( 0 \)'s except for \( 1 \)'s down
      the diagonal.
      \begin{equation*}
        \begin{pmatrix}
          1  &0  &       &0  \\
          0  &1  &       &0  \\
             &   &\ddots &   \\
          0  &0  &       &1
        \end{pmatrix}
      \end{equation*}

      Two same-sized singular matrices need not be row equivalent.
      For example, these two \( \nbyn{2} \) singular matrices
      are not row equivalent.
      \begin{equation*}
        \begin{pmatrix}
          1  &1  \\
          0  &0
        \end{pmatrix}
        \quad\text{and}\quad
        \begin{pmatrix}
          1  &0  \\
          0  &0
        \end{pmatrix}
      \end{equation*}  
    \end{answer}
  \recommended \item 
    Describe all of the row equivalence classes containing these.
    \begin{exparts*}
      \partsitem \( \nbyn{2} \)~matrices
      \partsitem \( \nbym{2}{3} \)~matrices
      \partsitem \( \nbym{3}{2} \)~matrices
      \partsitem \( \nbyn{3} \)~matrices
    \end{exparts*}
    \begin{answer}
      Since there is one and only one reduced echelon form matrix in each
      class, we can just list the possible reduced echelon form matrices.

      For that list, see the answer for \nearbyexercise{exer:PossRedEchFrms}. 
    \end{answer}
  \item  
     \begin{exparts}
          \partsitem Show that a vector $\vec{\beta}_0$ is a linear combination
            of members of the set $\set{\vec{\beta}_1,\ldots,\vec{\beta}_n}$
            if and only if there is a linear relationship 
            $\zero=c_0\vec{\beta}_0+\cdots+c_n\vec{\beta}_n$
            where $c_0$ is not zero.
            (\textit{Hint.}   Watch out for the $\vec{\beta}_0=\zero$ case.)
         \partsitem Use that to simplify the proof of 
            \nearbylemma{le:EchFormNoLinCombo}.   
       \end{exparts}
       \begin{answer}
          \begin{exparts}
           \partsitem If there is a linear relationship where $c_0$ is not zero
             then we can subtract $c_0\vec{\beta}_0$ from both sides and divide
             by $-c_0$ to get $\vec{\beta}_0$ as a linear
             combination of the others.
             (Remark:  
             if there are no other vectors in the set\Dash if the 
             relationship is, say, 
             $\zero=3\cdot\zero$\Dash then the statement is still true because
             the zero vector is by definition the sum of the empty set 
             of vectors.)

             Conversely, if $\vec{\beta}_0$ is a combination of the others 
             $\vec{\beta}_0=c_1\vec{\beta}_1+\dots+c_n\vec{\beta}_n$
             then subtracting
             $\vec{\beta}_0$ from both sides gives a relationship where 
             at least one
             of the coefficients is nonzero; namely,
             the $-1$ in front of $\vec{\beta}_0$.
           \partsitem The first row is not a linear combination of the
             others for
             the reason given in the proof:~in the equation of components from
             the column containing the leading entry of the first row, the
             only nonzero entry is the leading entry from the first row, so
             its coefficient must be zero.
             Thus, from the prior part of this exercise, the first row is in
             no linear relationship with the other rows.

             Thus, when considering whether the second row can be in a linear 
             relationship
             with the other rows, we can leave the first row out.
             But now the argument just applied to the first row will apply
             to the second row.
             (That is, we are arguing here by induction.)             
         \end{exparts}
      \end{answer}
  \recommended \item \label{ex:EchFormNoLinCombo} 
    Finish the proof of \nearbylemma{le:EchFormNoLinCombo}.
    \begin{exparts}
      \partsitem First illustrate the inductive step by showing 
         that $c_2=0$.
      \partsitem Do the full inductive step: where \( 1\leq n<i-1 \),
        assume that \( c_k=0 \)  for $1<k< n$
        and deduce that
        \( c_{n+1}=0 \) also.
      \partsitem Find the contradiction.
    \end{exparts}
    \begin{answer}
      \begin{exparts} 
        \partsitem 
          In the equation 
          \begin{equation*}
            \rho_i=c_1\rho_1+c_2\rho_2+\ldots+c_{i-1}\rho_{i-1}+
                     c_{i+1}\rho_{i+1}+\ldots+c_m\rho_m
          \end{equation*}
          we already know that $c_1=0$.
          Let $\ell_2$ be the column number of the leading entry of the
          second row. 
          Consider the prior equation on entries in that column.
          \begin{equation*}
             \rho_{i,\ell_1}=c_2\rho_{2,\ell_2}+\ldots+c_{i-1}\rho_{i-1,\ell_2}
                             +c_{i+1}\rho_{i+1,\ell_2}+\ldots+c_m\rho_{m,\ell_2}
          \end{equation*}
          Because $\ell_2$ is the column of the leading entry in the second
          row, $\rho_{i,\ell_2}=0$ for $i>2$.  
          Thus the equation reduces to  
          \begin{equation*}
             0=c_2\rho_{2,\ell_2}+0+\ldots+0
          \end{equation*}
          and since $\rho_{2,\ell_2}$ is not $0$ we have that $c_2=0$.
        \partsitem 
          In the equation 
          \begin{equation*}
            \rho_i=c_1\rho_1+c_2\rho_2+\ldots+c_{i-1}\rho_{i-1}+
                     c_{i+1}\rho_{i+1}+\ldots+c_m\rho_m
          \end{equation*}
          we already know that $0=c_1=c_2=\dots =c_n$.
          Let $\ell_{n+1}$ be the column number of the leading entry of
          row~$n+1$. 
          Consider the above equation on entries in that column.
          \begin{equation*}
             \rho_{i,\ell_{n+1}}=c_{n+1}\rho_{n+1,\ell_{n+1}}+\ldots
                             +c_{i-1}\rho_{i-1,\ell_{n+1}}
                             +c_{i+1}\rho_{i+1,\ell_{n+1}}
                             +\dots+c_m\rho_{m,\ell_{n+1}}
          \end{equation*}
          Because $\ell_{n+1}$ is the column of the leading entry in the
          row $n+1$, we have that $\rho_{j,\ell_{n+1}}=0$ for $j>{n+1}$.  
          Thus the equation reduces to  
          \begin{equation*}
             0=c_{n+1}\rho_{n+1,\ell_{n+1}}+0+\ldots+0
          \end{equation*}
          and since $\rho_{n+1,\ell_{n+1}}$ is not $0$ we have that $c_{n+1}=0$.
        \partsitem 
          From the prior item in this exercise we know that in the equation 
          \begin{equation*}
            \rho_i=c_1\rho_1+c_2\rho_2+\ldots+c_{i-1}\rho_{i-1}+
                     c_{i+1}\rho_{i+1}+\ldots+c_m\rho_m
          \end{equation*}
          we already know that $0=c_1=c_2=\dots =c_{i-1}$.

          Let $\ell_{i}$ be the column number of the leading entry of
          row~$i$. 
          Rewrite the above equation on entries in that column.
          \begin{equation*}
             \rho_{i,\ell_{i}}=c_{i+1}\rho_{i+1,\ell_{i}}
                             +\dots+c_m\rho_{m,\ell_{i}}
          \end{equation*}
          Because $\ell_{i}$ is the column of the leading entry in the
          row $i$, we have that $\rho_{j,\ell_{i}}=0$ for $j>i$.
          That makes the right side of the equation sum to $0$, but the
          left side is not $0$ since it is the leading entry of the row. 
          That's the contradiction. 
      \end{exparts}  
    \end{answer}
  % \item \label{ex:RowEqEchFormAreSameShape}
  %   Finish the induction argument in \nearbylemma{le:EquivMatsSameForm}.
  %   \begin{exparts}
  %     \partsitem State the inductive hypothesis, 
  %        Also state what must be shown to follow from that hypothesis.
  %     \partsitem Check that the inductive hypothesis implies that
  %        in the relationship 
  %        $\beta_{r+1}=s_{r+1,1}\delta_1+s_{r+2,2}\delta_2
  %            +\dots+s_{r+1,m}\delta_m$ 
  %        the coefficients $s_{r+1,1},\,\ldots\,,s_{r+1,r}$
  %        are each zero.
  %     \partsitem Finish the inductive step by arguing, as in the base
  %        case, that $\ell_{r+1}<k_{r+1}$ and $k_{r+1}<\ell_{r+1}$
  %        are impossible.
  %   \end{exparts}
  %   \begin{answer}
  %     \begin{exparts}
  %       \partsitem The inductive step is to show that if 
  %         the statement holds on rows $1$ through~$r$ then it also holds on 
  %         row~$r+1$.       
  %         That is, we assume that $\ell_1=k_1$, and
  %         $\ell_2=k_2$, \ldots, and $\ell_r=k_r$, 
  %         and we will show that $\ell_{r+1}=k_{r+1}$ also holds
  %         (for $r$ in $1\;..\;m-1$).
  %       \partsitem
  %         \nearbylemma{cor:RowsOfEqMatsLinCombos} gives the
  %         relationship  
  %         $\beta_{r+1}=s_{r+1,1}\delta_1+s_{r+2,2}\delta_2
  %            +\dots+s_{r+1,m}\delta_m$ 
  %         between rows.

  %         Inside of those row vectors, consider the relationship between
  %         the entries in the column $\ell_1=k_1$.
  %         Because by the induction hypothesis this is a row greater than the 
  %         first $r+1>1$, the row $\beta_{r+1}$ has a zero in entry $\ell_1$
  %         (the matrix $B$ is in echelon form). 
  %         But the row $\delta_1$
  %         has a nonzero entry in column $k_1$; by definition of 
  %         $k_1$ it is the leading entry in the first row of $D$.
  %         Thus, in that column, the above relationship among rows resolves
  %         to this equation among numbers: $0=s_{r+1,1}\cdot d_{1,k_1}$,
  %         with $d_{1,k_1}\neq 0$.
  %         Therefore $s_{r+1,1}=0$.

  %         With $s_{r+1,1}=0$, a similar argument shows that 
  %         $s_{r+1,2}=0$.
  %         With those two, another turn gives that $s_{r+1,3}=0$.
  %         That is, inside of the larger induction argument used to 
  %         prove the entire lemma, here is an subargument by induction
  %         that shows $s_{r+1,j}=0$ for all $j$ in $1\,..\,r$.
  %         (We won't write out the details since it is just like
  %         the induction done in \nearbyexercise{ex:EchFormNoLinCombo}.)     
  %       \partsitem 
  %         Note that the prior item of this exercise shows that the relationship
  %         between rows
  %         $\beta_{r+1}=s_{r+1,1}\delta_1+s_{r+2,2}\delta_2
  %            +\dots+s_{r+1,m}\delta_m$
  %         reduces to  
  %         $\beta_{r+1}=s_{r+1,r+1}\delta_{r+1}+\dots+s_{r+1,m}\delta_m$.
  %         Consider the column $\ell_{r+1}$ entries in this equation.
  %         By definition of $k_{r+1}$ as the column number of the leading
  %         entry of $\delta_{r+1}$, the entries in this column of the other rows 
  %         $\delta_{r+2}\;\;\delta_{m}$ are zeros.
  %         Now if $\ell_{r+1}<k_{r+1}$
  %         then the equation of entries from 
  %         column~$\ell_{k+1}$ would be 
  %         $b_{r+1,\ell_{r+1}}=s_{r+1,1}\cdot 0+\dots+s_{r+1,m}\cdot 0$,
  %         which is impossible as $b_{r+1,\ell_{r+1}}$ isn't zero as it leads
  %         its row.

  %         A symmetric argument
  %         shows that $k_{r+1}<\ell_{r+1}$ also is impossible.
  %     \end{exparts}
  %   \end{answer}
  % \item 
  %   Why, in the proof of \nearbytheorem{th:ReducedEchelonFormIsUnique},
  %   do we bother to restrict to the nonzero rows?
  %   Why not just stick to the relationship that we began with,
  %   $\beta_i=c_{i,1}\delta_1+\dots+c_{i,m}\delta_m$, with $m$ instead of $r$,
  %   and argue using it that the only nonzero coefficient
  %   is \( c_{i,i}  \), which is \( 1 \)?
  %   \begin{answer}
  %      The zero rows could have nonzero coefficients, and
  %      so the statement would not be true.
  %   \end{answer}
  \recommended \item 
   Three truck drivers went into a roadside cafe.
   One truck driver purchased four sandwiches, a cup of coffee, and ten 
   doughnuts for \$$8.45$.
   Another driver purchased three sandwiches, a cup of coffee, and seven
   doughnuts for \$$6.30$.
   What did the third truck driver pay for a sandwich, a cup of coffee, and 
   a doughnut?
   \cite{Trono}
   \begin{answer}
     We know that $4s+c+10d=8.45$ and that $3s+c+7d=6.30$, and we'd like to
     know what $s+c+d$ is.
     Fortunately, $s+c+d$ is a linear combination of $4s+c+10d$ and $3s+c+7d$.
     Calling the unknown price $p$, we have this reduction.
     \begin{equation*}
       \begin{amatrix}{3}
         4  &1  &10  &8.45 \\
         3  &1  &7   &6.30 \\
         1  &1  &1   &p
       \end{amatrix}
       \grstep[-(1/4)\rho_1+\rho_3]{-(3/4)\rho_1+\rho_2}
       \begin{amatrix}{3}
         4  &1    &10     &8.45      \\
         0  &1/4  &-1/2   &-0.037\,5 \\
         0  &3/4  &-3/2   &p-2.112\,5
       \end{amatrix}
       \grstep{-3\rho_2+\rho_3}
       \begin{amatrix}{3}
         4  &1    &10     &8.45      \\
         0  &1/4  &-1/2   &-0.037\,5 \\
         0  &0    &0      &p-2.00
       \end{amatrix}
     \end{equation*}
     The price paid is \$$2.00$.
   \end{answer}
  % \item
  %  The fact that Gaussian reduction disallows multiplication of
  %  a row by zero is needed for the proof of uniqueness of reduced echelon form,
  %  or else every matrix would
  %  be row equivalent to a matrix of all zeros.
  %  Where is it used?
  %  \begin{answer}
  %    If multiplication of a row by zero were allowed then
  %    \nearbylemma{le:EquivMatsSameForm}
  %    would not hold.
  %    That is, where
  %    \begin{equation*}
  %      \begin{pmatrix}
  %        1  &3  \\
  %        2  &1
  %      \end{pmatrix}
  %      \grstep{0\rho_2}
  %      \begin{pmatrix}
  %        1  &3  \\
  %        0  &0
  %      \end{pmatrix}
  %    \end{equation*}
  %    all the rows of the second matrix can be expressed as linear combinations
  %    of the rows of the first, but the converse does not hold.
  %    The second row of the first matrix is not a linear combination of the
  %    rows of the second matrix.  
  %  \end{answer}
  \recommended \item 
   The Linear Combination Lemma says which equations can be gotten from
   Gaussian reduction from a given linear system.
   \begin{enumerate}
     \item Produce an equation not implied by this system.
       \begin{equation*}
         \begin{linsys}{2}
           3x  &+  &4y  &=  &8 \\
           2x  &+  & y  &=  &3 
         \end{linsys}
       \end{equation*}
     \item Can any equation be derived from an inconsistent system?
   \end{enumerate}
   \begin{answer}
     \begin{enumerate}
        \item An easy answer is this:
          \begin{equation*}
            0=3.
          \end{equation*}
          For a less wise-guy-ish answer, solve the system:
          \begin{equation*}
            \begin{amatrix}{2}
              3  &-1  &8  \\
              2  &1   &3
            \end{amatrix}
            \grstep{-(2/3)\rho_1+\rho_2}
            \begin{amatrix}{2}
              3  &-1  &8    \\
              0  &5/3 &-7/3
            \end{amatrix}
          \end{equation*}
          gives \( y=-7/5 \) and \( x=11/5 \).
          Now any equation not satisfied by \( (-7/5,11/5) \) will do,
          e.g., \( 5x+5y=3 \).
        \item Every equation can be derived from an inconsistent system.
          For instance, here is how to derive ``\( 3x+2y=4 \)'' from
          ``\( 0=5 \)''.
          First,
          \begin{equation*}
            0=5
            \grstep{(3/5)\rho_1}
            0=3
            \grstep{x\rho_1}
            0=3x
          \end{equation*}
          (validity of the \( x=0 \) case is separate but clear).
          Similarly, \( 0=2y \).
          Ditto for \( 0=4 \).
          But now, \( 0+0=0 \) gives \( 3x+2y=4 \).
     \end{enumerate}  
    \end{answer}
  \item 
    Extend the definition of row equivalence to linear systems.
    Under your definition, do equivalent systems have the same solution set?
    \cite{HoffmanKunze}
    \begin{answer}
      Define linear systems to be equivalent if their augmented
      matrices are row equivalent.
      The proof that equivalent systems have the same solution set is easy.  
    \end{answer}
  \recommended \item 
    In this matrix
    \begin{equation*}
      \begin{pmatrix}
        1  &2  &3  \\
        3  &0  &3  \\
        1  &4  &5
      \end{pmatrix}
    \end{equation*}
    the first and second columns add to the third.
    \begin{exparts}
      \partsitem Show that remains true under any row operation.
      \partsitem Make a conjecture.
      \partsitem Prove that it holds.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem The three possible row swaps are easy, 
          as are the three possible rescalings.
          One of the six possible row combinations is \( k\rho_1+\rho_2 \):
          \begin{equation*}
            \begin{pmatrix}
              1           &2           &3  \\
              k\cdot 1+3  &k\cdot 2+0  &k\cdot 3+3  \\
              1           &4           &5
            \end{pmatrix}
          \end{equation*}
          and again the first and second columns add to the third.
          The other five combinations are similar.
        \partsitem The obvious conjecture is that row operations do not change
          linear relationships among columns.
        \partsitem A case-by-case 
          proof follows the sketch given in the first item.
      \end{exparts}  
   \end{answer}
\end{exercises}
