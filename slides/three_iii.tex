% see: https://groups.google.com/forum/?fromgroups#!topic/comp.text.tex/s6z9Ult_zds
\makeatletter\let\ifGm@compatii\relax\makeatother 
\documentclass[10pt,t]{beamer}
\usefonttheme{professionalfonts}
\usefonttheme{serif}
\PassOptionsToPackage{pdfpagemode=FullScreen}{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color}
% \DeclareGraphicsRule{*}{mps}{*}{}
\usepackage{../linalgjh}
\usepackage{present}
\usepackage{xr}\externaldocument{../map3} % read refs from .aux file
\usepackage{xr}\externaldocument{../map1} % read refs from .aux file
\usepackage{xr}\externaldocument{../vs3} % read refs from .aux file
\usepackage{catchfilebetweentags}
\usepackage{etoolbox} % from http://tex.stackexchange.com/questions/40699/input-only-part-of-a-file-using-catchfilebetweentags-package
\makeatletter
\patchcmd{\CatchFBT@Fin@l}{\endlinechar\m@ne}{}
  {}{\typeout{Unsuccessful patch!}}
\makeatother

\mode<presentation>
{
  \usetheme{boxes}
  \setbeamercovered{invisible}
  \setbeamertemplate{navigation symbols}{} 
}
\addheadbox{filler}{\ }  % create extra space at top of slide 
\hypersetup{colorlinks=true,linkcolor=blue} 

\title[Computing Linear Maps] % (optional, use only with long paper titles)
{Three.III Computing Linear Maps}

\author{\textit{Linear Algebra} \\ {\small Jim Hef{}feron}}
\institute{
  \texttt{http://joshua.smcvt.edu/linearalgebra}
}
\date{}


\subject{Computing Linear Maps}
% This is only inserted into the PDF information catalog. Can be left
% out. 

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

% =============================================
% \begin{frame}{Reduced Echelon Form} 
% \end{frame}



% ..... Three.III.1 .....
\section{Representing Linear Maps with Matrices}
%..........
\begin{frame}{\parbox[t]{\paperwidth}{Linear maps are determined by the action on a basis}}
Fix a domain space~$V$ 
with basis~$\sequence{\vec{\beta}_1,\ldots,\vec{\beta}_n}$, 
and a codomain space~$W$.
We've seen that to specify the action of a  
homomorphism $\map{h}{V}{W}$ on all domain vectors, we need only
specify its action
on the basis elements.
\begin{equation*}
  h(\vec{v})=h(c_1\cdot\vec{\beta}_1+\cdots+c_n\cdot\vec{\beta}_n)
            =c_1\cdot h(\vec{\beta}_1)+\cdots+c_n\cdot h(\vec{\beta}_n)
  \tag{$*$}
\end{equation*}
We've called this extending the action linearly from the basis to the entire 
domain. 
We now introduce a scheme for these calculations.

\pause\medskip
\ex
Let the domain be $V=\polyspace_2$ and the codomain be $W=\Re^2$,
with these bases.
\begin{equation*}
  B_{V}=\sequence{1,1+x,1+x+x^2}
  \qquad
  B_{W}
  =\sequence{\colvec{2 \\ 0}, \colvec{-1 \\ 1}}
\end{equation*}
Suppose that $\map{h}{\polyspace_2}{\Re^2}$ has this 
action on the domain basis.
\begin{equation*}
  h(1)=\colvec{0 \\ 1}
  \quad
  h(1+x)=\colvec{3 \\ 2}
  \quad
  h(1+x+x^2)=\colvec{-2 \\ -1}
\end{equation*}
\end{frame}
\begin{frame}
\noindent With this effect on the domain's basis,
\begin{equation*}
  h(\vec{\beta}_1)=\colvec{0 \\ 1}
  \quad
  h(\vec{\beta}_2)=\colvec{3 \\ 2}
  \quad
  h(\vec{\beta}_3)=\colvec{-2 \\ -1}
\end{equation*}
next find the
representation of the three outputs 
with respect to the codomain's basis $B_{W}$.
\begin{align*}
  &\rep{h(\vec{\beta}_1)}{B_{W}}
  =\colvec{1/2 \\ 1}
  \qquad\text{since }
  \colvec{0 \\ 1}=(1/2)\cdot\colvec{2 \\ 0}+1\cdot\colvec{-1 \\ 1}     \\
  &\rep{h(\vec{\beta}_2)}{B_{W}}
  =\colvec{5/2 \\ 2}
  \qquad\text{since }
  \colvec{3 \\ 2}=(5/2)\cdot\colvec{2 \\ 0}+2\cdot\colvec{-1 \\ 1}     \\
  &\rep{h(\vec{\beta}_3)}{B_{W}}
  =\colvec{-3/2 \\ -1}
  \qquad\text{since }
  \colvec{-2 \\ -1}=(-3/2)\cdot\colvec{2 \\ 0}-1\cdot\colvec{-1 \\ 1}   
\end{align*}
\pause
Summarize by writing those side-by-side.
\begin{equation*}
  \begin{mat}
    1/2 &5/2 &-3/2 \\
    1   &2   &-1
  \end{mat}
\end{equation*}
This is the matrix representation of~$h$ (with respect to $B_V,B_W$).
\end{frame}
\begin{frame}
The point of arranging those numbers in that way is that
if we start by representing a domain vector with respect to the domain's basis 
$\vec{v}=c_1\cdot\vec{\beta}_1+c_2\cdot\vec{\beta}_2+c_3\cdot\vec{\beta}_3$
% \begin{equation*}
%   \rep{\vec{v}}{B_{V}}=\colvec{c_1 \\ c_2 \\ c_3}
% \end{equation*}
and apply equation~($*$)
\begin{align*}
   h(c_1\vec{\beta}_1+c_2\vec{\beta}_2+c_3\vec{\beta}_3)  
   &=c_1\cdot h(\vec{\beta}_1)+c_2\cdot h(\vec{\beta}_2)+c_3\cdot h(\vec{\beta}_3) \\
    &=c_1\cdot\bigl((1/2)\cdot\colvec{2 \\ 0}+1\cdot\colvec{-1 \\ 1}\bigr)  \\
      &\quad +c_2\cdot\bigl((5/2)\cdot\colvec{2 \\ 0}+2\cdot\colvec{-1 \\ 1}\bigr)  \\
      &\quad +c_3\cdot\bigl((-3/2)\cdot\colvec{2 \\ 0}-1\cdot\colvec{-1 \\ 1}\bigr)
\end{align*}
then regrouping
\begin{equation*}
  =((1/2)c_1+(5/2)c_2-(3/2)c_3)\cdot\colvec{2 \\ 0}
  +(1c_1+2c_2-1c_3)\cdot\colvec{-1 \\ 1}         
\end{equation*}
expresses the image of $h(\vec{v})$ with respect to the codomain's 
basis.
\begin{equation*}
  \rep{h(\vec{v})}{B_{W}}
  =\colvec{(1/2)c_1+(5/2)c_2-(3/2)c_3 \\ 1c_1+2c_2-1c_3}
\end{equation*}
\end{frame}
\begin{frame}
\noindent
In summary, to represent application of the linear map
\begin{equation*}
  h(\vec{v})
\end{equation*}
we write the representation of the map next to the representation of
the input
\begin{equation*}
  \begin{mat}
    1/2 &5/2 &-3/2 \\
    1   &2   &-1
  \end{mat}_{B_V,B_W}
  \colvec{c_1 \\ c_2 \\ c_3}_{B_V}
\end{equation*}
and compute the representation of the output.
\begin{equation*}
  \rep{h(\vec{v})}{B_{W}}=\colvec{(1/2)c_1+(5/2)c_2-(3/2)c_3 \\ 1c_1+2c_2-1c_3}_{B_W}
\end{equation*}

\pause
The way the numbers combine to do this computation is:
take
the dot product of the rows of the representing matrix
with the single column 
representing the input,
to get the single column representing the output.
\end{frame}




%..........
\begin{frame}{Matrix representation of a linear map}
\df[def:MatRepMap]
\ExecuteMetaData[../map3.tex]{df:MatRepMap}

\pause
\medskip
(We often omit the matrix's subscript $B,D$.)
\end{frame}
\begin{frame}
\ex
Consider projection $\map{\pi}{\Re^2}{\Re^2}$ onto the $x$-axis.
\begin{equation*}
  \colvec{a \\ b}\mapsunder{\pi}\colvec{a \\ 0}
\end{equation*}
Let the domain and codomain bases be 
\begin{equation*}
  B=\sequence{\colvec{1 \\ 1}, \colvec{-1 \\ 1}}
  \qquad
  D=\sequence{\colvec{0 \\ 1}, \colvec{2 \\ 2}}
\end{equation*}
then by the computations 
\begin{align*}
  \colvec{1 \\ 1}\mapsunder{\pi}\colvec{1 \\ 0}
  &\quad\text{so}\quad
  \rep{\pi(\vec{\beta}_1)}{D}=\colvec{-1 \\ 1/2}     \\
  \colvec{-1 \\ 1}\mapsunder{\pi}\colvec{-1 \\ 0}
  &\quad\text{so}\quad
  \rep{\pi(\vec{\beta}_2)}{D}=\colvec{1 \\ -1/2}
\end{align*}
this is the matrix representation of $\pi$.
\begin{equation*}
  \rep{\pi}{B,D}=
  \begin{mat}
    -1  &1 \\
   1/2  &-1/2
  \end{mat}
\end{equation*}
\end{frame}
% \begin{frame}
% \ex
% Again consider projection onto the $x$-axis
% \begin{equation*}
%   \colvec{a \\ b}\mapsunder{\pi}\colvec{a \\ 0}
% \end{equation*}
% but this time take the input and output bases to be the standard.
% \begin{equation*}
%   B=D=\stdbasis_2
%   =\sequence{\colvec{1 \\ 0}, \colvec{0 \\ 1}}
% \end{equation*}
% We have
% \begin{gather*}
%   \colvec{1 \\ 0}\mapsunder{\pi}\colvec{1 \\ 0}
%   \quad\text{ so }
%   \rep{\pi(\vec{\beta}_1)}{D}=\colvec{1 \\ 0}               \\
%   \colvec{0 \\ 1}\mapsunder{\pi}\colvec{0 \\ 0}
%   \quad\text{ so }
%   \rep{\pi(\vec{\beta}_2)}{D}=\colvec{0 \\ 0}
% \end{gather*}
% and this is $\rep{\pi}{\stdbasis_2,\stdbasis_2}$.
% \begin{equation*}
%   \begin{mat}
%     1  &0  \\
%     0  &0
%   \end{mat}
% \end{equation*}
% \end{frame}

\begin{frame}
\ex Consider the domain~$\Re^2$ and the codomain~$\Re$.
Recall that with respect to the standard basis, a vector represents itself.
\begin{equation*}
  \rep{\colvec{-2 \\ 2}}{\stdbasis_2}=\colvec{-2 \\ 2}_{\stdbasis_2}
\end{equation*}
To represent $\map{h}{\Re^2}{\Re}$ 
\begin{equation*}
  \colvec{a \\ b}\mapsunder{h} 2a+3b
\end{equation*}
with respect to 
$\stdbasis_2$ and~$\stdbasis_1$,
first find the effect of $h$ on the domain's basis.
\begin{equation*}
  \colvec{1 \\ 0}\mapsto 2
  \qquad
  \colvec{0 \\ 1}\mapsto 3
\end{equation*}
Represent those with respect to the codomain's basis.
\begin{equation*}
  \rep{h(\vec{e}_1)}{\stdbasis_1}=\colvec{2}
  \qquad
  \rep{h(\vec{e}_2)}{\stdbasis_1}=\colvec{3}
\end{equation*}
This is $\nbym{1}{2}$ matrix representation.
\begin{equation*}
  H=\rep{h}{\stdbasis_2,\stdbasis_1}
  =
  \begin{mat}
    2 &3
  \end{mat}
\end{equation*}
\end{frame}




%..........
\begin{frame}\vspace*{-.5ex}
\th[th:MatMultRepsFuncAppl]
\ExecuteMetaData[../map3.tex]{th:MatMultRepsFuncAppl}
\end{frame}
\begin{frame}
\pf
This formalizes the example that began this subsection.
See \nearbyexercise{exer:MatVecMultRepLinMap}.
\qed

\pause
\medskip
\df[def:MatrixVecProd]
\ExecuteMetaData[../map3.tex]{df:MatrixVecProd}

\pause
\ex
We can perform the operation without any reference to spaces and bases.
\begin{equation*}
  \begin{mat}[r]
    3  &1  &2  \\
    0  &-2 &5
  \end{mat}
  \colvec[r]{4  \\ -1 \\ -3}
  =\colvec{3\cdot 4+1\cdot(-1)+2\cdot(-3) \\ 0\cdot 4-2\cdot(-1)+5\cdot(-3)}
  =\colvec[r]{5 \\ -13}
\end{equation*}
\end{frame}
\begin{frame}
\ex
Recall the matrix
representing 
projection $\map{\pi}{\Re^2}{\Re^2}$ onto the $x$-axis
\begin{equation*}  
  \rep{\pi}{B,D}=
  \begin{mat}
    -1  &1 \\
   1/2  &-1/2
  \end{mat}
\end{equation*}
% \begin{equation*}
%   \colvec{a \\ b}\mapsunder{\pi}\colvec{a \\ 0}
% \end{equation*}
with respect to these. 
\begin{equation*}
  B=\sequence{\colvec{1 \\ 1}, \colvec{-1 \\ 1}}\quad
  D=\sequence{\colvec{0 \\ 1}, \colvec{2 \\ 2}}
\end{equation*}
The domain vector 
\begin{equation*}
  \vec{v}=\colvec{-1 \\ 5}
\end{equation*} 
has this representation with respect to the domain basis.
\begin{equation*}
  \rep{\vec{v}}{B}=\colvec{2 \\ 3}
  \qquad
  \text{since 
  $2\cdot\colvec{1 \\ 1}+3\cdot\colvec{-1 \\ 1}=\colvec{-1 \\ 5}$}
\end{equation*}
\end{frame}
\begin{frame}
\noindent The matrix-vector product
gives the representation
\begin{equation*}
  \begin{mat}
    -1  &1 \\
    1/2  &-1/2
  \end{mat}
  \colvec{2 \\ 3}
  =\colvec{1 \\ -1/2}
  =\rep{\,\pi(\vec{v})\,}{D}
\end{equation*}
As a check,
\begin{equation*}
  1\cdot\colvec{0 \\ 1}-(1/2)\cdot\colvec{2 \\ 2}
  =\colvec{-1 \\ 0}
\end{equation*}
so it correctly computes the action of the projection map on the domain vector.
\begin{equation*}
  \pi(\,\colvec{-1 \\ 5}\,)=\colvec{-1 \\ 0}
\end{equation*}
\end{frame}

\begin{frame}
\ex Recall also that the map $\map{h}{\Re^2}{\Re}$ with this action
\begin{equation*}
  \colvec{a \\ b}\mapsunder{h} 2a+3b
\end{equation*}
is represented 
with respect to the standard bases $\stdbasis_2,\stdbasis_1$ by a
$\nbym{1}{2}$ matrix.
\begin{equation*}
  \rep{h}{\stdbasis_2,\stdbasis_1}
  =
  \begin{mat}
    2 &3
  \end{mat}
\end{equation*}
The domain vector
\begin{equation*}
  \vec{v}=\colvec{-2 \\ 2}
  \qquad
  \rep{\vec{v}}{\stdbasis_2}
  =\colvec{-2 \\ 2}
\end{equation*}
has this image.
\begin{equation*}
  \rep{h(\vec{v})}{\stdbasis_1}
  =
  \begin{mat}
    2 &3 
  \end{mat}
  \colvec{-2 \\ 2}
  =
  \colvec{2}_{\stdbasis_1}
\end{equation*}
Since this is a representation 
with respect to the standard basis $\stdbasis_1$,
meaning that vectors represent themselves, 
the image is $h(\vec{v})=2$.
\end{frame}






% ..... Three.III.2 .....
\section{Any Matrix Represents a Linear Map}
%..........
\begin{frame}
The prior subsection shows how to start with a linear map and produce its matrix
representation.
What about the converse?
\ex
Fix a matrix
\begin{equation*}
  H=\begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
\end{equation*}
and also fix a domain and codomain, with bases.
\begin{equation*}
  \stdbasis_2\subset\Re^2
  \quad
  \sequence{1-x,1+x}\subset\polyspace_1
\end{equation*}
Is there a linear map between the spaces associated with the matrix?

\pause
Consider $\map{h}{\Re^2}{\polyspace_1}$ defined by:
for any domain vector $\vec{v}$, represent it with respect to the domain basis
\begin{equation*}
  \vec{v}=c_1\vec{e}_1+c_2\vec{e}_2
  \qquad 
  \rep{\vec{v}}{E_2}=\colvec{c_1 \\ c_2}
\end{equation*}
multiply that representation by~$H$
\begin{equation*}
  \begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \colvec{c_1 \\ c_2}
  =\colvec{c_1+2c_2 \\ 3c_1+4c_2}
\end{equation*}
and then call $h(\vec{v})$ the codomain vector represented by
the result.
\begin{equation*}
  h(\vec{v})=(c_1+2c_2)\cdot (1-x)+(3c_1+4c_2)\cdot(1+x)
\end{equation*}
\end{frame}

\begin{frame}
Note first that $h$ is a function, that is, it is well-defined\Dash 
for a given input $\vec{v}$, the output $h(\vec{v})$ exists and is unique.
This is because
the representation of a vector with respect to a basis can be done in
one and only one way. 

\pause
We will now verify that $h$ is a linear function.
Fix domain vectors $\vec{u},\vec{v}\in\Re^2$ and represent them with 
respect to the domain basis. 
Multiply $c\cdot\rep{\vec{u}}{B}+d\cdot\rep{\vec{w}}{D}$ by $H$.
\begin{align*}
  \begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \left(c\cdot\colvec{u_1 \\ u_2}+d\cdot\colvec{v_1 \\ v_2}\right)
  &=
  \begin{mat}
    1 &2 \\
    3 &4
  \end{mat}                              
  \colvec{cu_1+dv_1 \\ cu_2+dv_2}    \\
  &=
  \colvec{1(cu_1+dv_1)+2(cu_2+dv_2) \\ 3(cu_1+dv_1)+4(cu_2+dv_2)}   \\ 
  &=
  \colvec{1cu_1+2cu_2 \\ 3cu_1+4cu_2}  
  +
  \colvec{1dv_1+2dv_2 \\ 3dv_1+4dv_2}     \\              
  &=
  c\cdot\begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \colvec{u_1 \\ u_2}
  +
  d\cdot\begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \colvec{v_1 \\ v_2}
\end{align*}
By the definition of $h$,
the result is $c\cdot\rep{h(\vec{u})}{D}+d\cdot\rep{h(\vec{v})}{D}$.
\end{frame}

\begin{frame}
\th[th:MatIsLinMap]
\ExecuteMetaData[../map3.tex]{th:MatIsLinMap}
\iftoggle{showallproofs}{
  \pause
  \pf
  \ExecuteMetaData[../map3.tex]{pf:MatIsLinMap}
  \qed
}{
 
  \bigskip
  The book has the proof.
}
\end{frame}




%..........
\begin{frame}
We will connect some properties of linear maps
with properties of the associated matrices.

First an easy one: 
observe that $\map{h}{V}{W}$ is the zero map $\vec{v}\mapsto\zero$
if and only
if it is represented, with respect to any bases, by the zero matrix.

\pause
For one direction, assume that $h$ is the zero map.
Then for any bases $B,D$ we have $h(\vec{\beta}_i)=\zero_W$, which is 
represented with respect to $D$ by the column vector of zeroes.
Thus $h$ is represented by the zero matrix.

\pause
For the other direction 
assume that there are bases $B,D$ such that $\rep{h}{B,D}$ is the zero
matrix.
For each $\vec{\beta}_i$ we have that $\rep{h(\vec{\beta}_1)}{D}$ is a
vector of zeros, and so $h(\vec{\beta}_i)$ is $\zero_W$.
Extending linearly gives that $h$ maps each $\vec{v}\in V$ to $\zero_W$,
and $h$ is the zero map.  

\pause
\ex The zero map $\map{z}{\Re^2}{\Re^3}$ is represented, 
with respect to any pair of bases,
by the $\nbym{2}{3}$
zero matrix.
\begin{equation*}
  Z=
  \begin{mat}
    0 &0 \\
    0 &0 \\
    0 &0
  \end{mat}
\end{equation*}

% \pause
% \medskip
% One thing this example does not illustrate is that typically a linear map
% will have many different matrices representing it, with respect to 
% the many different pairs of bases~$B,D$.
% A matrix property that derives from the map will be shared across
% all these representing matrices. 
\end{frame}




%..........
\begin{frame}
\th[th:RankMatEqRankMap]
\ExecuteMetaData[../map3.tex]{th:RankMatEqRankMap}
\iftoggle{showallproofs}{
  \pause
  \pf
  \ExecuteMetaData[../map3.tex]{pf:RankMatEqRankMap0}

  \pause
  \ExecuteMetaData[../map3.tex]{pf:RankMatEqRankMap1}
}{

  \bigskip
  The book has the proof.
}
\end{frame}
\iftoggle{showallproofs}{
  \begin{frame}
  \ExecuteMetaData[../map3.tex]{pf:RankMatEqRankMap2}
  \qed
  \end{frame}
}{}



%..........
\begin{frame}
\ex 
Consider the linear transformation $\map{t}{\Re^2}{\Re^2}$
given by
\begin{equation*}
  \colvec{a \\ b}\mapsto\colvec{2a-b \\ 2a-b}
\end{equation*}
Its range is the line~$x=y$ and so the rank of the map
is~$1$.

We will see two matrices representing this map,
the first with respect the standard bases $\stdbasis_2,\stdbasis_2$
and the second with respect to these.
\begin{equation*}
  B=\sequence{\colvec{1 \\ 1}, \colvec{-1 \\ 1}},
  \quad
  D=\sequence{\colvec{1/2 \\ 0}, \colvec{0 \\ 1/3}}
\end{equation*}
The standard basis case is easy.  This is the other calculation.
\begin{equation*}
  \rep{\colvec{1 \\ 1}}{D}=\colvec{2 \\ 3}
  \quad
  \rep{\colvec{-3 \\ -3}}{D}=\colvec{-6 \\ -9}
\end{equation*}
\pause
The two representing matrices are each of rank~$1$.
\begin{equation*}
  \rep{t}{\stdbasis_2,\stdbasis_2}
  =
  \begin{mat}
    2  &-1  \\
    2  &-1  
  \end{mat}
  \qquad
  \rep{t}{B,D}
  =
  \begin{mat}
    2  &-6  \\
    3  &-9  
  \end{mat}
\end{equation*}
\end{frame}




%..........
\begin{frame}
\co[cor:MatDescsMap]
\ExecuteMetaData[../map3.tex]{co:MatDescsMap}
\iftoggle{showallproofs}{
  \pause
  \pf
  \ExecuteMetaData[../map3.tex]{pf:MatDescsMap0}

  \pause
  \ExecuteMetaData[../map3.tex]{pf:MatDescsMap1}
  \qed
}{

  \bigskip
  The book has the proof.
}
\end{frame}

\begin{frame}
\ex
Any transformation rotating vectors 
counterclockwise by $\Theta$~radians $\map{t_\Theta}{\Re^2}{\Re^2}$ 
is represented with respect to the
standard bases by this matrix.
\begin{equation*}
  \rep{t_\Theta}{\stdbasis_2,\stdbasis_2}
  =
  \begin{mat}[r]
    \cos\Theta  &-\sin\Theta  \\
    \sin\Theta  &\cos\Theta
  \end{mat}
\end{equation*}
The $\Theta=\pi/4$ instance is
\begin{equation*}
  \rep{t_{\pi/4}}{\stdbasis_2,\stdbasis_2}
  =
  \begin{mat}[r]
    \sqrt{2}/2  &-\sqrt{2}/2  \\
    \sqrt{2}/2  &\sqrt{2}/2
  \end{mat}
  =(\sqrt{2}/2)\cdot
  \begin{mat}
    1  &-1  \\
    1  &1
  \end{mat}
\end{equation*}
and the rank of this matrix is two, reflecting
that the map $t_{\pi/4}$ is one-to-one and onto. 
\end{frame}




%..........
\begin{frame}
\df[df:NonsingularMap]
\ExecuteMetaData[../map3.tex]{df:NonsingularMap}
% \end{frame}




% %..........
% \begin{frame}
\pause
\lm[le:NonsingMatIffNonsingMap]
\ExecuteMetaData[../map3.tex]{le:NonsingMatIffNonsingMap}
\pause
\pf
\ExecuteMetaData[../map3.tex]{pf:NonsingMatIffNonsingMap0}

\pause
\ExecuteMetaData[../map3.tex]{pf:NonsingMatIffNonsingMap1}
\qed
\end{frame}




%..........
\begin{frame}
\ex
This matrix
\begin{equation*}
  \begin{mat}
    0  &3  \\
   -1  &2
  \end{mat}
\end{equation*}
is nonsingular since by inspection its two rows form a linearly independent
set.
So any map, with any domain and codomain, and represented by this matrix  
with respect to any pair of bases,
is an isomorphism.

\pause
\ex
Gauss's method shows that this matrix
\begin{equation*}
  \begin{mat}
    2  &1  &-2  \\
    3  &2  &1   \\
   -1  &0  &5
  \end{mat}
\end{equation*}
is singular so any map that it represents will be a homomorphism that
is not an isomorphism.
\end{frame}




%...........................
% \begin{frame}
% \ExecuteMetaData[../gr3.tex]{GaussJordanReduction}
% \df[def:RedEchForm]
% 
% \end{frame}
\end{document}
