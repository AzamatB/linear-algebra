% see: https://groups.google.com/forum/?fromgroups#!topic/comp.text.tex/s6z9Ult_zds
\makeatletter\let\ifGm@compatii\relax\makeatother 
\documentclass[10pt,t]{beamer}
\usefonttheme{professionalfonts}
\usefonttheme{serif}
\PassOptionsToPackage{pdfpagemode=FullScreen}{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color}
% \DeclareGraphicsRule{*}{mps}{*}{}
\usepackage{../linalgjh}
\usepackage{present}
\usepackage{xr}\externaldocument{../map3} % read refs from .aux file
\usepackage{xr}\externaldocument{../map1} % read refs from .aux file
\usepackage{xr}\externaldocument{../vs3} % read refs from .aux file
\usepackage{catchfilebetweentags}
\usepackage{etoolbox} % from http://tex.stackexchange.com/questions/40699/input-only-part-of-a-file-using-catchfilebetweentags-package
\makeatletter
\patchcmd{\CatchFBT@Fin@l}{\endlinechar\m@ne}{}
  {}{\typeout{Unsuccessful patch!}}
\makeatother

\mode<presentation>
{
  \usetheme{boxes}
  \setbeamercovered{invisible}
  \setbeamertemplate{navigation symbols}{} 
}
\addheadbox{filler}{\ }  % create extra space at top of slide 
\hypersetup{colorlinks=true,linkcolor=blue} 

\title[Computing Linear Maps] % (optional, use only with long paper titles)
{Three.III Computing Linear Maps}

\author{\textit{Linear Algebra} \\ {\small Jim Hef{}feron}}
\institute{
  \texttt{http://joshua.smcvt.edu/linearalgebra}
}
\date{}


\subject{Computing Linear Maps}
% This is only inserted into the PDF information catalog. Can be left
% out. 

\begin{document}
\begin{frame}
  \titlepage
\end{frame}

% =============================================
% \begin{frame}{Reduced Echelon Form} 
% \end{frame}



% ..... Three.III.1 .....
\section{Representing Linear Maps with Matrices}
%..........
\begin{frame}{\parbox[t]{\paperwidth}{Linear maps are determined by the action on a basis}}
Consider
a domain space~$V$, a codomain space~$W$,
and a basis~$B_{V}=\sequence{\vec{\beta}_1,\ldots,\vec{\beta}_n}$ for
the domain.
We've seen that we can specify a  
homomorphism $\map{h}{V}{W}$ by naming the 
action of $h$ on the basis elements
$h(\vec{\beta}_1)$, \ldots, $h(\vec{\beta}_n)$, 
which then extending linearly to 
the action of $h$ on any $\vec{v}\in V$.
\begin{equation*}
  h(\vec{v})=h(c_1\cdot\vec{\beta}_1+\cdots+c_n\cdot\vec{\beta}_n)
            =c_1\cdot h(\vec{\beta}_1)+\cdots+c_n\cdot h(\vec{\beta}_n)
  \tag{$*$}
\end{equation*}
We will see a scheme to do these computations.

\pause\medskip
\ex
Consider the domain $V=\polyspace_2$ with the basis
$B_{V}=\sequence{1,1+x,1+x+x^2}$ and
the codomain $\Re^2$ with this basis.
\begin{equation*}
  B_{W}
  =\sequence{\colvec{2 \\ 0}, \colvec{-1 \\ 1}}
\end{equation*}
\end{frame}
\begin{frame}
Let $\map{h}{\polyspace_2}{\Re^2}$ have this 
action on the domain basis.
\begin{equation*}
  h(1)=\colvec{0 \\ 1}
  \quad
  h(1+x)=\colvec{3 \\ 2}
  \quad
  h(1+x+x^2)=\colvec{-2 \\ -1}
\end{equation*}
\pause
Represent the action of $h$ on $B_{V}$,
with respect to the codomain's basis $B_{W}$.
\begin{align*}
  &\rep{h(\vec{\beta}_1)}{B_{W}}
  =\colvec{1/2 \\ 1}
  \quad\text{since }
  \colvec{0 \\ 1}=(1/2)\cdot\colvec{2 \\ 0}+1\cdot\colvec{-1 \\ 1}     \\
  &\rep{h(\vec{\beta}_2)}{B_{W}}
  =\colvec{5/2 \\ 2}
  \quad\text{since }
  \colvec{3 \\ 2}=(5/2)\cdot\colvec{2 \\ 0}+2\cdot\colvec{-1 \\ 1}     \\
  &\rep{h(\vec{\beta}_3)}{B_{W}}
  =\colvec{-3/2 \\ -1}
  \quad\text{since }
  \colvec{-2 \\ -1}=(-3/2)\cdot\colvec{2 \\ 0}-1\cdot\colvec{-1 \\ 1}   
\end{align*}
\pause
Summarize by writing those side-by-side, in order, 
in a matrix.
\begin{equation*}
  \begin{mat}
    1/2 &5/2 &-3/2 \\
    1   &2   &-1
  \end{mat}
\end{equation*}
\end{frame}
\begin{frame}
Now, to an arbitrary domain vector
$\vec{v}=c_1\cdot\vec{\beta}_1+c_2\cdot\vec{\beta}_2+c_3\cdot\vec{\beta}_3$
% \begin{equation*}
%   \rep{\vec{v}}{B_{V}}=\colvec{c_1 \\ c_2 \\ c_3}
% \end{equation*}
apply equation~($*$).
\begin{align*}
  h(\vec{v})
            &=c_1\cdot h(\vec{\beta}_1)+\cdots+c_n\cdot h(\vec{\beta}_n) \\
    &=c_1\cdot(\,(1/2)\cdot\colvec{2 \\ 0}+1\cdot\colvec{-1 \\ 1}\,)  \\
      &\quad +c_2\cdot(\,(5/2)\cdot\colvec{2 \\ 0}+2\cdot\colvec{-1 \\ 1}\,)  \\
      &\quad +c_3\cdot(\,(-3/2)\cdot\colvec{2 \\ 0}-1\cdot\colvec{-1 \\ 1}\,)
\end{align*}
Regroup.
\begin{gather*}
  =((1/2)c_1+(5/2)c_2-(3/2)c_3)\cdot\colvec{2 \\ 0}
  +(1c_1+2c_2-1c_3)\cdot\colvec{-1 \\ 1}          \\
  \rep{h(\vec{v})}{B_{W}}
  =\colvec{(1/2)c_1+(5/2)c_2-(3/2)c_3 \\ 1c_1+2c_2-1c_3}
\end{gather*}
\end{frame}
\begin{frame}
\noindent
So, the action of the linear map summarized by the matrix
\begin{equation*}
  \begin{mat}
    1/2 &5/2 &-3/2 \\
    1   &2   &-1
  \end{mat}
\end{equation*}
on the domain element represented in this way
\begin{equation*}
  \rep{\vec{v}}{B_{V}}=\colvec{c_1 \\ c_2 \\ c_3}_{B_V}
\end{equation*}
is to send it to the codomain element represented in this way. 
\begin{equation*}
  \rep{h(\vec{v})}{B_{W}}=\colvec{(1/2)c_1+(5/2)c_2-(3/2)c_3 \\ 1c_1+2c_2-1c_3}_{B_W}
\end{equation*}
In short, we
get the representation of the output by taking 
the dot product of each row of the matrix representation
with the single column 
representing the input.
\end{frame}




%..........
\begin{frame}{Matrix representation of a linear map}
\df[def:MatRepMap]
\ExecuteMetaData[../map3.tex]{df:MatRepMap}

\pause
\medskip
(We often omit the matrix's subscript $B,D$.)
\end{frame}
\begin{frame}
\ex
Consider projection $\map{\pi}{\Re^2}{\Re^2}$ onto the $x$-axis.
\begin{equation*}
  \colvec{a \\ b}\mapsunder{\pi}\colvec{a \\ 0}
\end{equation*}
Let the domain and codomain bases be 
\begin{equation*}
  B=\sequence{\colvec{1 \\ 1}, \colvec{-1 \\ 1}}
  \qquad
  D=\sequence{\colvec{0 \\ 1}, \colvec{2 \\ 2}}
\end{equation*}
then by the computations 
\begin{align*}
  \colvec{1 \\ 1}\mapsunder{\pi}\colvec{1 \\ 0}
  &\quad\text{so}\quad
  \rep{\pi(\vec{\beta}_1)}{D}=\colvec{-1 \\ 1/2}     \\
  \colvec{-1 \\ 1}\mapsunder{\pi}\colvec{-1 \\ 0}
  &\quad\text{so}\quad
  \rep{\pi(\vec{\beta}_2)}{D}=\colvec{1 \\ -1/2}
\end{align*}
this is the matrix representation of $\pi$.
\begin{equation*}
  \rep{\pi}{B,D}=
  \begin{mat}
    -1  &1 \\
   1/2  &-1/2
  \end{mat}
\end{equation*}
\end{frame}
\begin{frame}
\ex
Again consider projection onto the $x$-axis
\begin{equation*}
  \colvec{a \\ b}\mapsunder{\pi}\colvec{a \\ 0}
\end{equation*}
but this time take the input and output bases to be the standard.
\begin{equation*}
  B=D=\stdbasis_2
  =\sequence{\colvec{1 \\ 0}, \colvec{0 \\ 1}}
\end{equation*}
We have
\begin{gather*}
  \colvec{1 \\ 0}\mapsunder{\pi}\colvec{1 \\ 0}
  \quad\text{ so }
  \rep{\pi(\vec{\beta}_1)}{D}=\colvec{1 \\ 0}               \\
  \colvec{0 \\ 1}\mapsunder{\pi}\colvec{0 \\ 0}
  \quad\text{ so }
  \rep{\pi(\vec{\beta}_2)}{D}=\colvec{0 \\ 0}
\end{gather*}
and this is $\rep{\pi}{\stdbasis_2,\stdbasis_2}$.
\begin{equation*}
  \begin{mat}
    1  &0  \\
    0  &0
  \end{mat}
\end{equation*}
\end{frame}

\begin{frame}
\ex Consider this map $\map{h}{\Re^2}{\Re}$.
\begin{equation*}
  \colvec{a \\ b}\mapsunder{h} 2a+3b
\end{equation*}
We will represent the map with respect to the bases 
$\stdbasis_2$ and~$\stdbasis_1$.
First find the effect of $h$ on the domain's basis.
\begin{equation*}
  \colvec{1 \\ 0}\mapsto 2
  \qquad
  \colvec{0 \\ 1}\mapsto 3
\end{equation*}
Represent those with respect to the codomain's basis.
\begin{equation*}
  \rep{h(\vec{e}_1)}{\stdbasis_1}=\colvec{2}
  \qquad
  \rep{h(\vec{e}_2)}{\stdbasis_1}=\colvec{3}
\end{equation*}
This is $\nbym{1}{2}$ matrix representation.
\begin{equation*}
  H=\rep{h}{\stdbasis_2,\stdbasis_1}
  =
  \begin{mat}
    2 &3
  \end{mat}
\end{equation*}
\end{frame}




%..........
\begin{frame}\vspace*{-.5ex}
\th[th:MatMultRepsFuncAppl]
\ExecuteMetaData[../map3.tex]{th:MatMultRepsFuncAppl}
\end{frame}
\begin{frame}
\pf
This formalizes the example that began this subsection.
See \nearbyexercise{exer:MatVecMultRepLinMap}.
\qed

\pause
\medskip
\df[def:MatrixVecProd]
\ExecuteMetaData[../map3.tex]{df:MatrixVecProd}

\pause
\ex
We can perform the operation without any reference to spaces and bases.
\begin{equation*}
  \begin{mat}[r]
    3  &1  &2  \\
    0  &-2 &5
  \end{mat}
  \colvec[r]{4  \\ -1 \\ -3}
  =\colvec{3\cdot 4+1\cdot(-1)+2\cdot(-3) \\ 0\cdot 4-2\cdot(-1)+5\cdot(-3)}
  =\colvec[r]{5 \\ -13}
\end{equation*}
\end{frame}
\begin{frame}
\ex
Recall the two matrices 
\begin{equation*}  
  \rep{\pi}{B,D}=
  \begin{mat}
    -1  &1 \\
   1/2  &-1/2
  \end{mat}
  \qquad
  \rep{\pi}{\stdbasis_2,\stdbasis_2}
  \begin{mat}
    1  &0  \\
    0  &0
  \end{mat}
\end{equation*}
representing 
projection $\map{\pi}{\Re^2}{\Re^2}$ onto the $x$-axis
% \begin{equation*}
%   \colvec{a \\ b}\mapsunder{\pi}\colvec{a \\ 0}
% \end{equation*}
with respect to 
\begin{equation*}
  B=\sequence{\colvec{1 \\ 1}, \colvec{-1 \\ 1}},\,
  D=\sequence{\colvec{0 \\ 1}, \colvec{2 \\ 2}}
\end{equation*}
and also with respect to the standard bases $\stdbasis_{2},\stdbasis_{2}$.
\pause
This domain vector 
\begin{equation*}
  \vec{v}=\colvec{-1 \\ 5}
\end{equation*} 
has these representations with respect to the two domain bases.
\begin{equation*}
  \rep{\vec{v}}{B}=\colvec{2 \\ 3}
  \qquad
  \rep{\vec{v}}{\stdbasis_{2}}
  =\colvec{-1 \\ 5}
\end{equation*}
\end{frame}
\begin{frame}
\noindent The matrix-vector products
\begin{equation*}
  \begin{mat}
    -1  &1 \\
    1/2  &-1/2
  \end{mat}
  \colvec{2 \\ 3}
  =\colvec{1 \\ -1/2}
  \qquad
  \begin{mat}
    1  &0  \\
    0  &0
  \end{mat}
  \colvec{-1 \\ 5}
  =
  \colvec{-1 \\ 0}
\end{equation*}
give the two representations
$\rep{\pi(\vec{v})}{D}$ and $\rep{\pi(\vec{v})}{\stdbasis_{2}}$.
In both cases 
\begin{equation*}
  1\cdot\colvec{0 \\ 1}-(1/2)\cdot\colvec{2 \\ 2}
  =\colvec{-1 \\ 0}
  \qquad
  -1\cdot\vec{e}_1+0\cdot\vec{e}_2
  =\colvec{-1 \\ 0}
\end{equation*}
they compute this projection.
\begin{equation*}
  \pi(\,\colvec{-1 \\ 5}\,)=\colvec{-1 \\ 0}
\end{equation*}
\end{frame}

\begin{frame}
\ex Recall also that the map $\map{h}{\Re^2}{\Re}$ with this action
\begin{equation*}
  \colvec{a \\ b}\mapsunder{h} 2a+3b
\end{equation*}
is represented 
with respect to the standard bases $\stdbasis_2,\stdbasis_1$ by a
$\nbym{1}{2}$ matrix.
\begin{equation*}
  \rep{h}{\stdbasis_2,\stdbasis_1}
  =
  \begin{mat}
    2 &3
  \end{mat}
\end{equation*}
The domain vector
\begin{equation*}
  \vec{v}=\colvec{-2 \\ 2}
  \qquad
  \rep{\vec{v}}{\stdbasis_2}
  =\colvec{-2 \\ 2}
\end{equation*}
has this image.
\begin{equation*}
  \rep{h(\vec{v})}{\stdbasis_1}
  =
  \begin{mat}
    2 &3 
  \end{mat}
  \colvec{-2 \\ 2}
  =
  \colvec{2}_{\stdbasis_1}
\end{equation*}
Since this is a representation 
with respect to the standard basis $\stdbasis_1$,
meaning that vectors represent themselves, 
the image is $h(\vec{v})=2$.
\end{frame}






% ..... Three.III.2 .....
\section{Any Matrix Represents a Linear Map}
%..........
\begin{frame}
The prior subsection shows how to start with a linear map and produce its matrix
representation.
What about the converse?
\ex
Fix a matrix
\begin{equation*}
  H=\begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
\end{equation*}
and also fix a domain and codomain, with bases.
\begin{equation*}
  \stdbasis_2\subset\Re^2
  \quad
  \sequence{1-x,1+x}\subset\polyspace_1
\end{equation*}
Is there a linear map between the spaces associated with the matrix?

\pause
Consider $\map{h}{\Re^2}{\polyspace_1}$ defined by:
for any domain vector $\vec{v}$, represent it with respect to the domain basis,
multiply that representation by~$H$, 
and then $h(\vec{v})$ is the codomain vector represented by
the result.
We will verify that $h$ is a linear function.

\pause
Note first that $h$ is a function.
This is because
the representation of a vector with respect to a basis can be done in
one and only one way, so the outcome of the operation in the prior paragraph
is well-defined\Dash 
for a given input, the output from $h$ exists and is unique. 
\end{frame}
\begin{frame}
Next we show that $h$ is linear.
Fix domain vectors $\vec{u},\vec{v}\in\Re^2$ and represent them with 
respect to the domain basis. 
Multiply $c\cdot\rep{\vec{u}}{B}+d\cdot\rep{\vec{w}}{D}$ by $H$.
\begin{align*}
  \begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \left(c\cdot\colvec{u_1 \\ u_2}+d\cdot\colvec{v_1 \\ v_2}\right)
  &=
  \begin{mat}
    1 &2 \\
    3 &4
  \end{mat}                              
  \colvec{cu_1+dv_1 \\ cu_2+dv_2}    \\
  &=
  \colvec{1(cu_1+dv_1)+2(cu_2+dv_2) \\ 3(cu_1+dv_1)+4(cu_2+dv_2)}   \\ 
  &=
  \colvec{1cu_1+2cu_2 \\ 3cu_1+4cu_2}  
  +
  \colvec{1dv_1+2dv_2 \\ 3dv_1+4dv_2}     \\              
  &=
  c\cdot\begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \colvec{u_1 \\ u_2}
  +
  d\cdot\begin{mat}
    1 &2 \\
    3 &4
  \end{mat}
  \colvec{v_1 \\ v_2}
\end{align*}
By the definition of $h$,
the result is $c\cdot\rep{h(\vec{u})}{D}+d\cdot\rep{h(\vec{v})}{D}$.
\end{frame}

\begin{frame}
The next result verifies that the above 
calculation works for all matrices, spaces
(of suitable dimension), and bases.

\th[th:MatIsLinMap]
\ExecuteMetaData[../map3.tex]{th:MatIsLinMap}
\iftoggle{showallproofs}{
  \pause
  \pf
  \ExecuteMetaData[../map3.tex]{pf:MatIsLinMap}
  \qed
}{
 
  \bigskip
  The book has the proof.
}
\end{frame}




%..........
\begin{frame}
We will connect some properties of linear maps
with properties of the associated matrices.

First an easy one: 
observe that $\map{h}{V}{W}$ is the zero map $\vec{v}\mapsto\zero$
if and only
if it is represented, with respect to any bases, by the zero matrix.

\pause
For one direction, assume that $h$ is the zero map.
Then for any bases $B,D$ we have $h(\vec{\beta}_i)=\zero_W$, which is 
represented with respect to $D$ by the column vector of zeroes.
Thus $h$ is represented by the zero matrix.

\pause
For the other direction 
assume that there are bases $B,D$ such that $\rep{h}{B,D}$ is the zero
matrix.
For each $\vec{\beta}_i$ we have that $\rep{h(\vec{\beta}_1)}{D}$ is a
vector of zeros, and so $h(\vec{\beta}_i)$ is $\zero_W$.
Extending linearly gives that $h$ maps each $\vec{v}\in V$ to $\zero_W$,
and $h$ is the zero map.  

\pause
\ex The zero map $\map{z}{\Re^2}{\Re^3}$ is represented, 
with respect to any pair of bases,
by the $\nbym{2}{3}$
zero matrix.
\begin{equation*}
  Z=
  \begin{mat}
    0 &0 \\
    0 &0 \\
    0 &0
  \end{mat}
\end{equation*}

% \pause
% \medskip
% One thing this example does not illustrate is that typically a linear map
% will have many different matrices representing it, with respect to 
% the many different pairs of bases~$B,D$.
% A matrix property that derives from the map will be shared across
% all these representing matrices. 
\end{frame}




%..........
\begin{frame}
\th[th:RankMatEqRankMap]
\ExecuteMetaData[../map3.tex]{th:RankMatEqRankMap}
\iftoggle{showallproofs}{
  \pause
  \pf
  \ExecuteMetaData[../map3.tex]{pf:RankMatEqRankMap0}

  \pause
  \ExecuteMetaData[../map3.tex]{pf:RankMatEqRankMap1}
}{

  \bigskip
  The book has the proof.
}
\end{frame}
\iftoggle{showallproofs}{
  \begin{frame}
  \ExecuteMetaData[../map3.tex]{pf:RankMatEqRankMap2}
  \qed
  \end{frame}
}{}



%..........
\begin{frame}
\ex 
Consider the linear transformation $\map{t}{\Re^2}{\Re^2}$
given by
\begin{equation*}
  \colvec{a \\ b}\mapsto\colvec{2a-b \\ 2a-b}
\end{equation*}
Its range is the line~$x=y$ and so the rank of the map
is~$1$.

We will see two matrices representing this map,
the first with respect the standard bases $\stdbasis_2,\stdbasis_2$
and the second with respect to these.
\begin{equation*}
  B=\sequence{\colvec{1 \\ 1}, \colvec{-1 \\ 1}},
  \quad
  D=\sequence{\colvec{1/2 \\ 0}, \colvec{0 \\ 1/3}}
\end{equation*}
The standard basis case is easy.  This is the other calculation.
\begin{equation*}
  \rep{\colvec{1 \\ 1}}{D}=\colvec{2 \\ 3}
  \quad
  \rep{\colvec{-3 \\ -3}}{D}=\colvec{-6 \\ -9}
\end{equation*}
\pause
The two representing matrices are each of rank~$1$.
\begin{equation*}
  \rep{t}{\stdbasis_2,\stdbasis_2}
  =
  \begin{mat}
    2  &-1  \\
    2  &-1  
  \end{mat}
  \qquad
  \rep{t}{}
  =
  \begin{mat}
    2  &-6  \\
    3  &-9  
  \end{mat}
\end{equation*}
\end{frame}




%..........
\begin{frame}
\co[cor:MatDescsMap]
\ExecuteMetaData[../map3.tex]{co:MatDescsMap}
\iftoggle{showallproofs}{
  \pause
  \pf
  \ExecuteMetaData[../map3.tex]{pf:MatDescsMap0}

  \pause
  \ExecuteMetaData[../map3.tex]{pf:MatDescsMap1}
  \qed
}{

  \bigskip
  The book has the proof.
}
\end{frame}

\begin{frame}
\ex
The transformation $\map{t_\Theta}{\Re^2}{\Re^2}$ rotating vectors 
counterclockwise by $\Theta$~radians is represented with respect to the
standard bases by this matrix.
\begin{equation*}
  \rep{t_\Theta}{\stdbasis_2,\stdbasis_2}
  =
  \begin{mat}[r]
    \cos\Theta  &-\sin\Theta  \\
    \sin\Theta  &\cos\Theta
  \end{mat}
\end{equation*}
The $\Theta=\pi/4$ instance is
\begin{equation*}
  \begin{mat}[r]
    \sqrt{2}/2  &-\sqrt{2}/2  \\
    \sqrt{2}/2  &\sqrt{2}/2
  \end{mat}
  =(\sqrt{2}/2)\cdot
  \begin{mat}
    1  &-1  \\
    1  &1
  \end{mat}
\end{equation*}
and the rank of this matrix is two, reflecting
that the map $t_{\pi/4}$ is one-to-one and onto. 
\end{frame}




%..........
\begin{frame}
\df[df:NonsingularMap]
\ExecuteMetaData[../map3.tex]{df:NonsingularMap}
% \end{frame}




% %..........
% \begin{frame}
\pause
\lm[le:NonsingMatIffNonsingMap]
\ExecuteMetaData[../map3.tex]{le:NonsingMatIffNonsingMap}
\pause
\pf
\ExecuteMetaData[../map3.tex]{pf:NonsingMatIffNonsingMap0}

\pause
\ExecuteMetaData[../map3.tex]{pf:NonsingMatIffNonsingMap1}
\qed
\end{frame}




%..........
\begin{frame}
\ex
This matrix
\begin{equation*}
  \begin{mat}
    0  &3  \\
   -1  &2
  \end{mat}
\end{equation*}
is nonsingular since by inspection its two rows form a linearly independent
set.
So any map, with any domain and codomain, and represented by this matrix  
with respect to any pair of bases,
is an isomorphism.

\pause
\ex
Gauss's method shows that this matrix
\begin{equation*}
  \begin{mat}
    2  &1  &-2  \\
    3  &2  &1   \\
   -1  &0  &5
  \end{mat}
\end{equation*}
is singular so any map that it represents will be a homomorphism that
is not an isomorphism.
\end{frame}




%...........................
% \begin{frame}
% \ExecuteMetaData[../gr3.tex]{GaussJordanReduction}
% \df[def:RedEchForm]
% 
% \end{frame}
\end{document}
