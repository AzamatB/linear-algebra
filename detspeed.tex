% Chapter 4, Topic _Linear Algebra_ Jim Hefferon
%  http://joshua.smcvt.edu/linalg.html
%  2001-Jun-12
\topic{Speed of Calculating Determinants}
The permutation expansion formula for computing determinants is useful 
for proving theorems, but the method of using row operations
is a much better for finding the determinants of a large matrix.
We can make this statement 
precise by considering, as computer algorithm designers do,
the number of arithmetic operations that each method uses.

We measure the speed of an algorithm by finding
how the time taken by the computer grows as the size of
its input data set grows. 
For instance, if
we increase the size of the input data by a
factor of ten
% , from a $1000$~row matrix to a $10,000$~row matrix 
% or from $10,000$ to $100,000$?
does the time taken by the computer grow by a factor of ten,
or by a factor of a hundred, or by a factor of a thousand?
That is, 
is the time proportional to the size of the data set, 
or to the square of that size, or to the cube of that size, etc.? 

Recall 
the permutation expansion formula\index{determinant!permutation expansion}%
\index{permutation expansion}
for determinants.
\begin{equation*}
   \begin{vmat}
      t_{1,1}  &t_{1,2}  &\ldots  &t_{1,n}  \\
      t_{2,1}  &t_{2,2}  &\ldots  &t_{2,n}  \\
               &\vdots                      \\
      t_{n,1}  &t_{n,2}  &\ldots  &t_{n,n}
   \end{vmat}
   =\!\!\sum_{\text{permutations\ }\phi}\!\!\!\!
     t_{1,\phi(1)}t_{2,\phi(2)}\cdots t_{n,\phi(n)}
                                 \deter{P_{\phi}}   
   % &=
   % \begin{aligned}[t]
   %    &t_{1,\phi_1(1)}\cdot t_{2,\phi_1(2)}\cdots
   %         t_{n,\phi_1(n)}\deter{P_{\phi_1}}       \\  %[.25ex]
   %    &\hbox{}\quad\hbox{}
   %      +t_{1,\phi_2(1)}\cdot t_{2,\phi_2(2)}\cdots
   %         t_{n,\phi_2(n)}\deter{P_{\phi_2}}       \\
   %    &\hbox{}\quad\hbox{}\vdotswithin{+}             \\
   %    &\hbox{}\quad\hbox{}
   %      +t_{1,\phi_k(1)}\cdot t_{2,\phi_k(2)}\cdots
   %         t_{n,\phi_k(n)}\deter{P_{\phi_k}} 
   % \end{aligned}
\end{equation*}
There are  
$n!=n\cdot(n-1)\cdot(n-2)\cdots 2\cdot 1$ different \( n \)-permutations.
This factorial function grows quickly; for instance  
when $n$ is only $10$ then the expansion above has  
$10!=3,628,800$ terms, each with $n$~multiplications. 
Doing $n!$ many operations is doing more than $n^2$ many operations 
(roughly:
multiplying the first
two factors in $n!$ gives $n\cdot(n-1)$, which for large $n$
is approximately $n^2$ and then multiplying in more factors will make the 
factorial even larger).
Similarly, the factorial function grows faster than the cube 
or the fourth power or
any polynomial function.
So a computer program that uses the permutation
expansion formula, and thus performs a number of operations
that is greater than or equal to the factorial of the number of rows,
would be very slow.
It would take a time longer than the square of the number of rows,
longer than the cube, etc.

In contrast, the time taken by the row reduction method does not grow so fast.
The fragment of row-reduction code shown below is
in the computer language \textsc{FORTRAN}, which is widely used for numeric code.
The matrix is in the $\nbyn{\text{\lstinline[style=inline]!N!}}$ 
array \lstinline[style=inline]!A!. 
The program's outer loop runs through
each \lstinline[style=inline]!ROW! between \lstinline[style=inline]!1! and 
\lstinline[style=inline]!N-1! 
and does the entry-by-entry combination 
% \begin{equation*}
\(  -\text{\lstinline[style=inline]!PIVINV!}\cdot \rho_{\,\text{\lstinline!ROW!}}
      +\rho_{\,\text{\lstinline!I!}} \)
% \end{equation*}
with the lower rows.
% (there are no rows below
% the \lstinline[style=inline]!N!-th 
% so it is not included in the loop).
\begin{lstlisting}
DO 10 ROW=1, N-1
  PIVINV=1.0/A(ROW,ROW)
  DO 20 I=ROW+1, N
    DO 30 J=I, N
      A(I,J)=A(I,J)-PIVINV*A(ROW,J)
    30 CONTINUE
  20 CONTINUE
10 CONTINUE
\end{lstlisting} 
(This code is naive; for example it does not handle the case that 
the \lstinline[style=inline]!A(ROW,ROW)! is zero.
Analysis of a finished version that includes all of the tests
and subcases is messier but gives the same conclusion.)
For each \lstinline[style=inline]!ROW!, 
the nested \lstinline[style=inline]!I! and \lstinline[style=inline]!J! 
loops perform the combination with the lower rows by
doing arithmetic on the entries
in \lstinline[style=inline]!A! that are below and to the right of 
\lstinline[style=inline]!A(ROW,ROW)!.
There are 
$(\text{\lstinline[style=inline]!N!}-\text{\lstinline[style=inline]!ROW!})^2$ 
such entries. 
On average, \lstinline[style=inline]!ROW! will be 
$\text{\lstinline[style=inline]!N!}/2$.
Therefore, this program will perform the arithmetic 
about $(\text{\lstinline[style=inline]!N!}/2)^2$ times,
that is, 
this program will run in a time proportional to the square of the number
of equations.
Taking into account the outer loop, we 
estimate that the running time of the algorithm
is proportional to the cube of the number of equations.

Finding the fastest algorithm to compute the determinant 
is a topic of current research.
So far, people have found algorithms that run in time between the 
square and cube of \lstinline[style=inline]!N!.

% Speed estimates like these help us to understand how quickly or
% slowly an algorithm will run.
% Algorithms that run in time proportional to the size of the 
% data set are fast, algorithms that run in time proportional to the
% square of the size of the data set are less fast, but typically quite
% usable, and algorithms that run in time proportional to the cube of
% the size of the data set are still reasonable in speed for
% not-too-big input data. 
% However, algorithms that run in time that is the factorial
% of the size of the data set are not practical for input of any
% appreciable size.

% There are other methods besides the two discussed here
% that we can  also used for computation of determinants.
% Those lie outside of our scope.
The contrast between these two methods for computing determinants 
makes the point that although in principle they give the same answer, 
in practice we want the one that is fast.




\begin{exercises}
  \item[\textit{Most of these presume access to a computer.}]
  \item 
    Computer systems generate random numbers
    (of course, these are only pseudo-random, in that they come from
    an algorithm, but they 
    pass a number of reasonable statistical tests for randomness).
    \begin{exparts}
      \partsitem Fill a $\nbyn{5}$ array with random numbers (say, in the
        range $[0..1)$).
        See if it is singular.
        Repeat that experiment a few times.
        Are singular matrices frequent or rare (in this sense)?
      \partsitem Time your computer algebra system at finding the
        determinant of ten $\nbyn{5}$ arrays of random numbers.
        Find the average time per array.
        Repeat the prior item for $\nbyn{15}$ arrays,
        $\nbyn{25}$ arrays, $\nbyn{35}$ arrays, etc.
        You may find that you need to get above a certain size
        to get a timing that you can use. 
        (Notice that, when an array is singular, we can sometimes decide that
        quickly,
        for instance if the first row equals the second.
        In the light of your answer to the first part, do you expect that 
        singular systems play a large role in your average?)
      \partsitem Graph the input size versus the average time.
    \end{exparts}
    \begin{answer}
      \begin{exparts}
        \partsitem Under Octave, \texttt{rank(rand(5))} finds the
          rank of a $\nbyn{5}$ matrix whose entries are (uniformly
          distributed) in the interval $[0..1)$.
          This loop which runs the test $5000$ times
\begin{lstlisting}
octave:1> for i=1:5000
> if rank(rand(5))<5 printf("That's one."); endif
> endfor
\end{lstlisting}  
          produces (after a few seconds) returns the prompt, with no output.

          The Octave script
\begin{lstlisting}
function elapsed_time = detspeed (size)
  a=rand(size);
  tic();
  for i=1:10
     det(a);
  endfor
  elapsed_time=toc();
endfunction
\end{lstlisting}  
          lead to this session (obviously, your times will vary). 
\begin{lstlisting}
octave:1> detspeed(5)
ans = 0.019505
octave:2> detspeed(15)
ans = 0.0054691
octave:3> detspeed(25)
ans = 0.0097431
octave:4> detspeed(35)
ans = 0.017398
\end{lstlisting}  
          \partsitem Here is the data (rounded a bit), and the graph.
            \begin{center}
              \begin{tabular}{r|ccccccccc}
                 \textit{matrix rows} 
                    &$15$ &$25$ &$35$ &$45$ &$55$ &$65$ &$75$ &$85$ &$95$ \\
                 \hline
                 \textit{time per ten}
                    &$0.0034$                     
                    &$0.0098$
                    &$0.0675$
                    &$0.0285$ 
                    &$0.0443$
                    &$0.0663$ 
                    &$0.1428$  
                    &$0.2282$ 
                    &$0.1686$              
              \end{tabular}
            \end{center}
          (This data is from an average of twenty runs of the above script,
          because of the possibility that the randomly chosen matrix
          happens to take an unusually long or short time.
          Even so, the timing cannot be relied on too heavily; this is
          just an experiment.)
          \begin{center}
            \includegraphics{ch4.28}
          \end{center}
      \end{exparts}
    \end{answer}
  \item 
    Compute the determinant of each of these by hand using the 
    two methods discussed above.
    \begin{exparts*}
      \partsitem $\begin{vmat}[r]
                    2  &1  \\
                    5  &-3
                  \end{vmat}$
      \partsitem $\begin{vmat}[r]
                    3  &1  &1  \\
                   -1  &0  &5  \\
                   -1  &2  &-2 
                  \end{vmat}$
      \partsitem $\begin{vmat}[r]
                    2  &1  &0  &0  \\
                    1  &3  &2  &0  \\
                    0  &-1 &-2 &1  \\
                    0  &0  &-2 &1
                  \end{vmat}$
    \end{exparts*}
    Count the number of multiplications and divisions used in each case,
    for each of the methods.
    (On a computer, multiplications and divisions take much 
     longer than additions and subtractions, so algorithm 
     designers worry about them more.)
     \begin{answer}
       The number of operations depends on exactly how we do the operations.
       \begin{exparts}
         \partsitem The determinant is $-11$.
           To row reduce takes a single row combination 
           with two multiplications
           ($-5/2$ times $2$ plus $5$, and $-5/2$ times $1$ plus $-3$)
           and the product down the diagonal takes one more multiplication.
           The permutation expansion takes two multiplications ($2$ times
           $-3$ and $5$ times $1$).
         \partsitem The determinant is $-39$.
           Counting the operations is routine.
         \partsitem The determinant is $4$.
       \end{exparts}
     \end{answer}
  \item 
    What $\nbyn{10}$ array can you invent that takes your computer
    system the longest to reduce?
    The shortest?
    \begin{answer}
      One way to get started is to compare these under Octave:
      \texttt{det(rand(10));}, versus
      \texttt{det(hilb(10));}, versus
      \texttt{det(eye(10));}, versus
      \texttt{det(zeroes(10));}. 
      You can time them as in \texttt{tic(); det(rand(10)); toc()}.
    \end{answer}
%   \item 
%     Write the rest of the FORTRAN program to do a
%     straightforward implementation of calculating determinants via
%     Gauss' method.
%     (Don't test for a zero leading entry.)
%     Compare the speed of your code to that used in your computer algebra
%     system.
%     \begin{answer}
%       This is a simple one.
% \begin{lstlisting}
% DO 5 ROW=1, N
%    PIVINV=1.0/A(ROW,ROW)
%    DO 10 I=ROW+1, N
%       DO 20 J=I, N
%          A(I,J)=A(I,J)-PIVINV*A(ROW,J)
%       20 CONTINUE
%    10 CONTINUE
% 5 CONTINUE
% \end{lstlisting}     
%     \end{answer}
  \item The \textsc{FORTRAN} language specification requires that arrays be
    stored ``by column,'' that is, the entire first column is stored
    contiguously, then the second column, etc.
    Does the code fragment given take advantage of this,
    or can it be rewritten to make it faster, by taking advantage of
    the fact that computer fetches are faster from contiguous locations?
    \begin{answer}
      Yes, because the $J$ is in the innermost loop.
    \end{answer}
\end{exercises}
\endinput
